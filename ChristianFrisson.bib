
@patent{SonixCyclePatentWO2017,
  title = {Audio Search User Interface},
  shorttitle = {{{SonixCyclePatentWO2020}}},
  author = {Urbain, Gabriel and Moinet, Alexis and Frisson, Christian},
  date = {2017-09-21},
  location = {{WO}},
  url = {https://patents.google.com/patent/WO2017158159A1/},
  urldate = {2020-12-18},
  holder = {{Universite De Mons}},
  keywords = {audio, descriptor, file, index, perceptual},
  langid = {english},
  number = {WO2017158159A1},
  type = {patent}
}

@unpublished{PolyMTLINF8088Winter2021EDI,
  title = {Visualisation d'information ↔ {{Equité}}, Diversité et Inclusion},
  shorttitle = {{{PolyMTLINF8088Winter2021EDI}}},
  author = {{Christian Frisson}},
  date = {2021-02-25},
  url = {https://frisson.re/2021-PolyMTL-INF8808-EDI/},
  abstract = {Presentation (February 25th, 50 min) on Equity, Diversity and Inclusion ↔ (in/with) InfoVis.},
  editora = {Hurtut, Thomas},
  editoratype = {collaborator},
  eventtitle = {{{INF8808 Winter}} 2021 {{Data Visualization}}},
  type = {Guest Lecture},
  venue = {{Polytechnique Montréal}}
}

@thesis{MathiasKirkegaardMAMcGill2021,
  title = {Integrating 1-{{DoF Force Feedback Interactions In Self}}-{{Contained DMIs}} (Submitted)},
  shorttitle = {{{MathiasKirkegaardMAMcGill2021}}},
  author = {Kirkegaard, Mathias},
  date = {2021},
  institution = {{McGill University, Montreal, Canada}},
  editora = {Wanderley, Marcelo M. and Frisson, Christian and Pietrzak, Thomas},
  editoratype = {collaborator},
  keywords = {digital musical instruments, force-feedback, haptics},
  type = {MA}
}

@thesis{MathiasBredholtMAMcGill2021,
  title = {Live-Looping of Distributed Gesture-to-Sound Mappings (Submitted)},
  shorttitle = {{{MathiasBredholtMAMcGill2021}}},
  author = {Bredholt, Mathias},
  date = {2021},
  institution = {{McGill University, Montreal, Canada}},
  editora = {Wanderley, Marcelo M. and Meneses, Eduardo and Frisson, Christian},
  editoratype = {collaborator},
  keywords = {digital musical instruments},
  type = {MA}
}

@inproceedings{TorqueTunerNIME2020,
  title = {{{TorqueTuner}}: {{A}} Self Contained Module for Designing Rotary Haptic Force Feedback for Digital Musical Instruments},
  shorttitle = {{{TorqueTuner}}},
  booktitle = {20th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Kirkegaard, Mathias and Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M.},
  date = {2020-07-21/2020-07-25},
  location = {{Birmihgham, UK}},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper52.pdf},
  keywords = {3d printing, digital musical instruments, force feedback, haptics, mapping, sound synthesis},
  series = {{{NIME}}'20}
}

@inproceedings{CIRMMTVisHCII2020,
  title = {A {{Visualization Tool}} for the {{CIRMMT Distinguished Lecture Series}}},
  shorttitle = {{{CIRMMTVis}}},
  booktitle = {22nd {{International Conference}} on {{Human}}-{{Computer Interaction}}},
  author = {Wanderley, Marcelo M. and Bredholt, Mathias and Frisson, Christian},
  date = {2020},
  publisher = {{Springer}},
  location = {{Copenhagen, Denmark}},
  doi = {10.1007/978-3-030-50020-7_10},
  keywords = {data visualization, information visualization, multimedia browsers, music, science and technology, video lectures},
  series = {{{HCII}}'20}
}

@inproceedings{RepHapHAPTICS2020WIP,
  title = {{{RepHap}}: Towards an Open Source Platform for Benchmarking Haptic Devices Leveraging the {{Robot Operating System}} Ecosystem},
  shorttitle = {{{RepHap}}},
  booktitle = {Haptics {{Symposium}}, {{Works}} in {{Progress}}},
  author = {Frisson, Christian and Delbos, Benjamin and Désourdy, Félix and Ding, Steve and Wanderley, Marcelo M. and Lévesque, Vincent and Gallacher, Colin},
  date = {2020},
  publisher = {{IEEE}},
  location = {{Washington, DC, USA}},
  keywords = {3d printing, force feedback, haptics, robotics},
  series = {{{HAPTICS}}'20 {{WIP}}}
}

@inproceedings{PrintgetsHAID2020,
  title = {Printgets: An {{Open}}-{{Source Toolbox}} for {{Designing Vibrotactile Widgets}} with {{Industrial}}-{{Grade Printed Actuators}} and {{Sensors}}},
  shorttitle = {Printgets},
  booktitle = {10th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  author = {Frisson, Christian and Decaudin, Julien and Sanz-Lopez, Mario and Pietrzak, Thomas},
  date = {2020-08-21},
  url = {https://hal.archives-ouvertes.fr/hal-02901202},
  urldate = {2020-09-09},
  abstract = {New technologies for printing sensors and actuators combine the flexibility of interface layouts of touchscreens with localized vibrotactile feedback, but their fabrication still requires industrial-grade facilities. Until these technologies become easily replicable, interaction designers need material for ideation. We propose an open-source hardware and software toolbox providing maker-grade tools for iterative design of vibrotactile widgets with industrial-grade printed sensors and actuators. Our hardware toolbox provides a mechanical structure to clamp and stretch printed sheets, and electronic boards to drive sensors and actuators. Our software toolbox expands the design space of haptic interaction techniques by reusing the wide palette of available audio processing algorithms to generate real-time vibrotactile signals. We validate our toolbox with the implementation of three exemplar interface elements with tactile feedback: buttons, sliders, touchpads.},
  eventtitle = {{{HAID}} 2020 - {{International Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  keywords = {3d printing, haptics, piezoelectric ink, printed actuators, printed electronics, printed sensors, PureData, vibrotactile},
  langid = {english},
  series = {{{HAID}}'20}
}

@article{DataChangesTVCG2020,
  title = {Data {{Changes Everything}}: {{Challenges}} and {{Opportunities}} in {{Data Visualization Design Handoff}}},
  shorttitle = {{{DataChanges}}},
  author = {Walny, Jagoda and Frisson, Christian and West, Mieka and Kosminsky, Doris and Knudsen, Søren and Carpendale, Sheelagh and Willett, Wesley},
  date = {2020-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {26},
  pages = {12--22},
  publisher = {{IEEE}},
  issn = {2160-9306},
  doi = {10.1109/TVCG.2019.2934538},
  abstract = {Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.},
  keywords = {Collaboration, data mapping, Data visualization, design handoff, design process, Design tools, Information visualization, Software, Task analysis, Tools},
  number = {1},
  series = {{{TVCG}}'20}
}

@unpublished{ComproVisCIRMMT2020,
  title = {Overview of Gestural Interaction with Sound Information, towards Force-Feedback},
  shorttitle = {{{ComproVisCIRMMT2020}}},
  author = {Frisson, Christian},
  date = {2020-02},
  url = {https://frisson.re/ComproVisCIRMMT2020/},
  eventtitle = {{{CIRMMT Workshop}}: {{Composing}} \& Improvising with Information},
  keywords = {audio, digital musical instruments, force feedback, haptics, multimedia browsers, music information retrieval, PureData, sound},
  series = {{{CIRMMT ComproVis}}'20},
  type = {Workshop Presentation},
  venue = {{Montreal, Qc, Canada}}
}

@unpublished{MediaMappingScriptingIEA2020,
  title = {Navigation Dans Des Collections Multimédia: De La Cartographie Vers l'écriture.},
  shorttitle = {{{MediaMappingScriptingIEA2020}}},
  author = {Frisson, Christian},
  date = {2020-11-19},
  url = {https://frisson.re/MediaMappingScriptingIEA2020},
  abstract = {Nous partageons nos perspectives sur la navigation dans des collections de contenu multimédia, d'abord par la cartographie (ou mise en correspondance) entre caractéristiques des collections et des interactions; puis par le parcours de cheminements (ou séquences) que l'on pourrait pré-écrire.},
  eventtitle = {{{IEA Workshop}}: {{Écriture}} Du Temps et de l'interaction ({{CNRS}}-{{Inria LaBRI}} + {{McGill IDMIL}})},
  series = {{{IEA}}'20},
  type = {Workshop Presentation},
  venue = {{Montreal, Qc, Canada + Bordeaux, France}}
}

@unpublished{McGillMUMT620Fall2020InfoVis,
  title = {Gestural {{Control}} of {{Music}} (with {{Information Visualization}})},
  shorttitle = {{{McGillMUMT620Fall2020InfoVis}}},
  author = {{Christian Frisson}},
  date = {2020-10-01},
  url = {https://idmil.gitlab.io/course-materials/mumt620/2020-fall-guest-lecture-infovis-frisson/},
  abstract = {Presentation (50 min) of course complements in information visualization.},
  editora = {Wanderley, Marcelo M},
  editoratype = {collaborator},
  eventtitle = {{{MUMT620 Fall}} 2020 {{Gestural Control}} of {{Sound Synthesis}}},
  type = {Guest Lecture},
  venue = {{McGill University}}
}

@unpublished{McGillMUMT620Fall2020Haptics,
  title = {Gestural {{Control}} of {{Music}} (with {{Force Feedback Haptics}})},
  shorttitle = {{{McGillMUMT620Fall2020Haptics}}},
  author = {{Christian Frisson}},
  date = {2020-09-17},
  url = {https://idmil.gitlab.io/course-materials/mumt620/2020-fall-guest-lecture-force-feedback-frisson/#/start},
  abstract = {Presentation (50 min) of course complements in haptics.},
  editora = {Wanderley, Marcelo M},
  editoratype = {collaborator},
  eventtitle = {{{MUMT620 Fall}} 2020 {{Gestural Control}} of {{Sound Synthesis}}},
  type = {Guest Lecture},
  venue = {{McGill University}}
}

@software{HCI-MTL-Group,
  title = {{{HCI Montreal Group Map}}},
  shorttitle = {{{HCI}}-{{MTL}}-{{Group}}},
  author = {Frisson, Christian},
  date = {2020},
  url = {https://hcimtl.github.io/group/},
  keywords = {information visualization},
  type = {website}
}

@inproceedings{FreesoundTrackerHAID2019,
  title = {Haptic Techniques for Browsing Sound Maps Organized by Similarity},
  shorttitle = {{{FreesoundTracker}}},
  booktitle = {9th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  author = {Frisson, Christian and Gallacher, Colin and Wanderley, Marcelo M.},
  date = {2019-03},
  pages = {1},
  location = {{Lille, France}},
  url = {https://hal.inria.fr/hal-02050235},
  keywords = {force feedback, haptics, multimedia browsers, sound effects, t-SNE, web audio},
  series = {{{HAID}}'19}
}

@inproceedings{MediaHapticsIEEEICRA2019SoftHapticInteraction,
  title = {Towards {{Opensource Authoring Toolkits}} for {{Designing Soft}}+{{Stiff Haptic Interactions}}},
  shorttitle = {{{MediaHaptics}}},
  booktitle = {Workshop: {{Modeling}}, {{Design}} and {{Application}}},
  author = {Frisson, Christian and Gallacher, Colin and Wanderley, Marcelo M.},
  date = {2019-05},
  location = {{Montreal, Qc, Canada}},
  url = {https://frisson.re/MediaHapticsICRA2019SHI/},
  keywords = {force feedback, haptics},
  series = {{{IEEE ICRA}}'19 {{Soft Haptic Interaction}}}
}

@unpublished{HapticAudioFeedbacksIAM2019,
  title = {An Overview on {{Haptic}} \& {{Audio}} Research and Applications},
  shorttitle = {{{HapticAudioFeedbacksIAM2019}}},
  author = {Frisson, Christian},
  date = {2019-06},
  url = {https://frisson.re/HapticAudioFeedbacksIAM2019},
  eventtitle = {9th {{Interactive Audio Montreal}} Edition: {{Haptics}} in {{Interaction}} and {{Game Design}}},
  keywords = {audio, audio synthesis, digital musical instruments, force feedback, haptics, multimedia browsers},
  series = {{{CIRMMT IAM}}'19},
  type = {Workshop Presentation},
  venue = {{Montreal, Qc, Canada}}
}

@unpublished{OpenForceFeedbackHAID2019,
  title = {Open Technologies for Force-Feedback in Artistic Creation},
  shorttitle = {{{OpenForceFeedbackHAID2019}}},
  author = {Sinclair, Stephen and Leonard, James and Villeneuve, Jérôme and Frisson, Christian},
  date = {2019-03},
  url = {https://frisson.re/OpenForceFeedbackHAID2019/},
  eventtitle = {9th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  keywords = {digital musical instruments, force feedback, haptics, multimedia browsers},
  series = {{{HAID}}'19},
  type = {Workshop Presentation},
  venue = {{Lille, France}}
}

@unpublished{McGillMUMT620Winter2019InfoVis,
  title = {Information {{Visualisation}}},
  shorttitle = {{{McGillMUMT620Winter2019InfoVis}}},
  author = {{Christian Frisson}},
  date = {2019},
  abstract = {Presentation (2 x 50 min) of course complements and evaluation of student projects in information visualization.},
  editora = {Wanderley, Marcelo M},
  editoratype = {collaborator},
  eventtitle = {{{MUMT620 Winter}} 2019 {{Gestural Control}} of {{Sound Synthesis}}},
  type = {Guest Lecture},
  venue = {{McGill University}}
}

@unpublished{DataVisDesignHandoffPolyMTL2020,
  title = {Transfert de Conceptions de Visualisations de Données},
  shorttitle = {{{DataVisDesignHandoffPolyMTL2020}}},
  author = {{Christian Frisson}},
  date = {2019-04-14},
  url = {https://frisson.re/DataVisDesignHandoff-PolyMTL-INF8808-2020/},
  abstract = {Presentation (April 14th, 50 min) of Data Visualization Design Handoff (IEEE InfoVis’19 best paper) and complements.},
  editora = {Hurtut, Thomas},
  editoratype = {collaborator},
  eventtitle = {{{INF8808 Winter}} 2020 {{Data Visualization}}},
  type = {Guest Lecture},
  venue = {{Polytechnique Montréal}}
}

@unpublished{HaptificationPolyMTL2020,
  title = {Haptification},
  shorttitle = {{{HaptificationPolyMTL2020}}},
  author = {{Christian Frisson}},
  date = {2019-06-04},
  url = {https://frisson.re/HaptificationPolyMTL2020},
  abstract = {Presentation (June 4th, 50 min) on haptic interaction for information visualization.},
  editora = {Hurtut, Thomas},
  editoratype = {collaborator},
  eventtitle = {{{INF8808E Summer}} 2020 {{Data Visualization}}},
  type = {Guest Lecture},
  venue = {{Polytechnique Montréal}}
}

@software{CIRMMTSpeakerSeriesVis,
  title = {{{CIRMMT Distinguished Speaker Series Visualizations}}},
  shorttitle = {{{CIRMMTSpeakerSeriesVis}}},
  author = {Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M.},
  date = {2019/2020},
  url = {https://idmil.gitlab.io/CIRMMT_visualizations/},
  keywords = {data visualization, information visualization, multimedia browsers, music, science and technology}
}

@software{RepHap,
  title = {{{RepHap}}: An Open Source Platform for Benchmarking Haptic Devices Leveraging the {{Robot Operating System}} Ecosystem},
  shorttitle = {{{RepHap}}},
  author = {Frisson, Christian},
  date = {2019/2020},
  url = {https://github.com/RepHap},
  keywords = {3d printing, force feedback, haptics, robotics}
}

@inproceedings{EnergyVisCHI2018EA,
  title = {Democratizing {{Open Energy Data}} for {{Public Discourse}} Using {{Visualization}}},
  shorttitle = {{{EnergyVis}}},
  booktitle = {35th {{Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Knudsen, Søren and Vermeulen, Jo and Kosminsky, Doris and Walny, Jagoda and West, Mieka and Frisson, Christian and Adriel Aseniero, Bon and MacDonald Vermeulen, Lindsay and Perin, Charles and Quach, Lien and Buk, Peter and Tabuli, Katrina and Chopra, Shreya and Willett, Wesley and Carpendale, Sheelagh},
  date = {2018},
  publisher = {{ACM}},
  location = {{Montreal, Canada}},
  doi = {10.1145/3170427.3186539},
  abstract = {For this demo, we will show two interactive visualizations: Energy Futures and Pipeline Incidents. We designed and developed these visualizations as part of an open data initiative that aims to create interactive data visualizations to help make Canada's energy data publicly accessible, transparent, and understandable. This work was conducted in collaboration with the National Energy Board of Canada (NEB) and a visualization software development company, VizworX.},
  isbn = {978-1-4503-5621-3},
  keywords = {citizen engagement, data democratization, data visualization, information visualization, open data},
  series = {{{CHI}}'18 {{EA}}}
}

@inproceedings{DataMappingVIS2018InfoVisPosters,
  title = {Discovering the {{Data Mapping}} of an {{Unfamiliar Visualization}}},
  shorttitle = {{{DataMapping}}},
  booktitle = {23rd {{Conference}} on {{Information Visualization}}},
  author = {Hynes, Lisa and Huynh, Tina and Storteboom, Sarah and Walny, Jagoda and Frisson, Christian and Kosminsky, Doris and West, Mieka and Carpendale, Sheelagh and Willett, Wesley},
  date = {2018},
  publisher = {{IEEE}},
  location = {{Berlin, Germany}},
  keywords = {data mapping, information visualization, mapping, open data},
  series = {{{VIS}}'18 {{InfoVis Posters}}}
}

@inproceedings{HapticProxyIEEEVIS2018Dataphys,
  title = {Haptics as a Sustainable Proxy for Exploring Design Variables for Data Physicalization},
  shorttitle = {{{HapticProxy}}},
  booktitle = {Workshop: {{Toward}} a {{Design Language}} for {{Data Physicalization}}},
  author = {Frisson, Christian and Wanderley, Marcelo M. and Willett, Wesley and Carpendale, Sheelagh},
  date = {2018},
  location = {{Berlin, Germany}},
  url = {https://frisson.re/Vis18DataPhys},
  keywords = {data physicalization, force feedback, haptics, information visualization},
  series = {{{IEEE VIS}}'18 {{Dataphys}}}
}

@inproceedings{VibroTactileWidgetsUIST2017Adjunct,
  title = {Designing {{Vibrotactile Widgets}} with {{Printed Actuators}} and {{Sensors}}},
  shorttitle = {{{VibroTactileWidgets}}},
  booktitle = {30th {{Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Frisson, Christian and Decaudin, Julien and Pietrzak, Thomas and Ng, Alexander and Poncet, Pauline and Casset, Fabrice and Latour, Antoine and Brewster, Stephen A.},
  date = {2017},
  pages = {11--13},
  publisher = {{ACM}},
  location = {{Quebec, Qc, Canada}},
  doi = {10.1145/3131785.3131800},
  abstract = {Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.},
  isbn = {978-1-4503-5419-6},
  keywords = {3d printing, haptics, piezoelectric ink, printed actuators, printed electronics, printed sensors, PureData, thin-film actuators, vibrotactile, widgets},
  series = {{{UIST}}'17 {{Adjunct}}}
}

@report{PrintgetsHAPPINESS2017,
  title = {Guidelines for {{Next Generation Haptic Demonstrator}}},
  shorttitle = {{{PrintgetsHAPPINESS2017}}},
  author = {Frisson, Christian and Decaudin, Julien and Lopez, Mario Sanz and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Latour, Antoine},
  date = {2017-12},
  institution = {{HAPPINESS EU H2020 645145}},
  url = {https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b760277f&appId=PPGMS},
  keywords = {PureData},
  number = {D.17},
  series = {Deliverables for {{EU H2020}} Project 645145}
}

@report{VibroTactileWidgetsHAPPINESS2017,
  title = {Interaction Techniques Leveraging Haptic Feedback on New Interactive Surfaces (Final)},
  shorttitle = {{{VibroTactileWidgetsHAPPINESS2017}}},
  author = {Frisson, Christian and Decaudin, Julien and Lopez, Mario Sanz and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Gaffary, Yoren and Lecuyer, Anatole and Latour, Antoine},
  date = {2017-12},
  institution = {{HAPPINESS EU H2020 645145}},
  keywords = {PureData},
  number = {D2.4},
  series = {Deliverables for {{EU H2020}} Project 645145}
}

@unpublished{InspectorWidgetFOSDEM2017,
  title = {Developers Looking for Designers? {{Pitch}} Your Project: {{InspectorWidget}}},
  shorttitle = {{{InspectorWidgetFOSDEM2017}}},
  author = {Frisson, Christian},
  date = {2017-02},
  url = {https://archive.fosdem.org/2017/schedule/event/osd_pitch_your_project/},
  eventtitle = {17th {{Free}} and {{Open Source Developers}}’ {{European Meeting}}, {{Open Source Design}} Devroom},
  series = {{{FOSDEM}}'17},
  type = {Workshop Presentation},
  venue = {{Brussels, Belgium}}
}

@software{libhappiness,
  title = {Libhappiness: Library for Driving Piezo Actuators and Sensing Capacitive Touch},
  shorttitle = {Libhappiness},
  author = {Decaudin, Julien and Frisson, Christian and Pietrzak, Thomas},
  date = {2017/2020},
  url = {https://gitlab.inria.fr/Loki/happiness/libhappiness},
  keywords = {haptics, PureData, vibrotactile}
}

@software{DataEmpowermentSpeakerSeries,
  title = {Data {{Empowerment Speaker Series Website}}},
  shorttitle = {{{DataEmpowermentSpeakerSeries}}},
  author = {Frisson, Christian and West, Mieka and Pusch, Ricky},
  date = {2017/2018},
  url = {http://speakerseries.ilab.cpsc.ucalgary.ca/},
  keywords = {information visualization}
}

@software{EnergyVisConditions,
  title = {Conditions on {{CER}}-Regulated Energy Projects {{Information Visualization}}},
  shorttitle = {{{EnergyVisConditions}}},
  date = {2017/2020},
  url = {https://apps2.cer-rec.gc.ca/conditions/},
  keywords = {information visualization},
  organization = {{Canada Energy Regulator}}
}

@thesis{OnurFerhatPhDUAB2017,
  title = {A {{Gaze Estimation Method}} and {{System}} for {{Natural Light Cameras}}.},
  shorttitle = {{{OnurFerhatPhDUAB2017}}},
  author = {Ferhat, Onur},
  date = {2017},
  institution = {{Universitat Autònoma Barcelona, Spain}},
  editora = {Vilariño, Fernando and Villanueva, Arantzazu and Karatzas, Dimosthenis and Parraga, Carlos Alejandro and Gomez Bigorda, Luis and Super, Hans},
  editoratype = {collaborator},
  keywords = {computer vision, eye tracking},
  type = {PhD}
}

@inproceedings{SonixCycleAM2016,
  title = {A {{Semantic}} and {{Content}}-{{Based Search User Interface}} for {{Browsing Large Collections}} of {{Foley Sounds}}},
  shorttitle = {{{SonixCycle}}},
  booktitle = {11th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Urbain, Gabriel and Frisson, Christian and Moinet, Alexis and Dutoit, Thierry},
  date = {2016},
  pages = {272--277},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2986416.2986436},
  url = {http://doi.acm.org/10.1145/2986416.2986436},
  urldate = {2016-11-12},
  abstract = {Sound designers select the sounds they use among massive collections of recordings. They usually rely on text-based queries to narrow down a subset from these collections when looking for specific content. However, when it comes to unknown collections, this approach can fail to precisely retrieve files according to their content. We investigate an audio search engine that associates content-based features and semantic meta-data using Apache Solr deployed in a fully integrated server architecture. In order to facilitate the task of browsing the sounds, we also propose a search user interface in which the user can perform both text-based queries and visual browsing in a window where sounds are organized according to their audio features. A preliminary evaluation of the performances helped to optimize the parameters of the system.},
  isbn = {978-1-4503-4822-5},
  keywords = {information visualization, multimedia browsers, multimedia information retrieval, search user interfaces, sound effects},
  series = {{{AM}} '16}
}

@inproceedings{StereoHapticsSIGGRAPH2016,
  title = {Stereohaptics: {{A Haptic Interaction Toolkit}} for {{Tangible Virtual Experiences}}},
  shorttitle = {{{StereoHaptics}}},
  booktitle = {{{ACM SIGGRAPH}} 2016 {{Studio}}},
  author = {Israr, Ali and Zhao, Siyan and McIntosh, Kyna and Schwemler, Zachary and Fritz, Adam and Mars, John and Bedford, Job and Frisson, Christian and Huerta, Ivan and Kosek, Maggie and Koniaris, Babis and Mitchell, Kenny},
  date = {2016},
  publisher = {{ACM}},
  location = {{Anaheim, California, USA}},
  doi = {10.1145/2929484.2970273},
  abstract = {With a recent rise in the availability of affordable head mounted gear sets, various sensory stimulations (e.g., visual, auditory and haptics) are integrated to provide seamlessly embodied virtual experience in areas such as education, entertainment, therapy and social interactions. Currently, there is an abundance of available toolkits and application programming interfaces (APIs) for generating the visual and audio content. However, such richness in hardware technologies and software tools is missing in designing haptic experiences. Current solutions to integrate haptic effects are limited due to: i) a user's rigid adaptation to new hardware and software technologies, ii) limited scalability of the existing tools to incorporate haptic hardware and applications, iii) inflexible authoring capabilities, iv) missing infrastructure for storing, playback and distribution, and v) and unreliable hardware for long term usage.},
  isbn = {978-1-4503-4373-2},
  keywords = {audio signal processing, Haptics, PureData, vibrotactile},
  series = {{{SIGGRAPH}}'16},
  type = {Tutorial}
}

@inproceedings{ComixTripDIS2016Companion,
  title = {{{ComixTrip}}: {{Reading Comic Books}} with {{Text Sequenced}} through {{Gaze Tracking}}},
  shorttitle = {{{ComixTrip}}},
  booktitle = {2016 {{ACM Conference Companion Publication}} on {{Designing Interactive Systems}}},
  author = {Rochette, Alexis and Goossens, Cédric and Giot, Rudi and Frisson, Christian},
  date = {2016},
  publisher = {{ACM}},
  location = {{Brisbane, Australia}},
  doi = {10.1145/2908805.2909405},
  abstract = {Paper-based comic books are rendered on a static medium, where time can alternatively be sequenced through space. People usually prefer to read comic books at their pace. When comic books are digital, their medium becomes dynamic and interactive, how can the readers' experience be redesigned? We present ComixTrip, a system with which people can read digital comic books with sequential media cues responsively adapted to their reading pace. Our system relies on low-cost eye tracking both for measuring how people read; and for sequencing speech balloons semi-automatically: once a balloon is read, the next one is displayed. We ran a study to analyse how people read text paragraphs with diverse spatial layouts by tracking their gaze. Our preliminary results show that we may accurately track when people have read balloons in comic books. Our system needs to be improved regarding inter- and intra-person reading speed variations.},
  isbn = {978-1-4503-4315-2},
  keywords = {comic book, eye tracking, multimedia browsers, reading time},
  series = {{{DIS}}'16 {{Companion}}}
}

@inproceedings{InspectorWidgetCHI2016EA,
  title = {{{InspectorWidget}}: {{A System}} to {{Analyze Users Behaviors}} in {{Their Applications}}},
  shorttitle = {{{InspectorWidget}}},
  booktitle = {33rd {{Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Frisson, Christian and Malacria, Sylvain and Bailly, Gilles and Dutoit, Thierry},
  date = {2016},
  publisher = {{ACM}},
  location = {{San Jose, California, USA}},
  doi = {10.1145/2851581.2892388},
  abstract = {We propose InspectorWidget, an opensource application to track and analyze users' behaviors in interactive software. The key contributions of our application are: 1) it works with closed applications that do not provide source code nor scripting capabilities; 2) it covers the whole pipeline of software analysis from logging input events to visual statistics through browsing and programmable annotation; 3) it allows post-recording logging; and 4) it does not require programming skills. To achieve this, InspectorWidget combines low-level event logging (e.g. mouse and keyboard events) and high-level screen features (e.g. interface widgets) captured though computer vision techniques. InspectorWidget benefits end users, usability experts and HCI researchers.},
  isbn = {978-1-4503-4082-3},
  keywords = {accessibility api, automatic annotation, computer vision, logging, multimedia browsers, usability},
  series = {{{CHI}}'16 {{EA}}}
}

@inproceedings{InfoPhysTEI2016,
  title = {{{InfoPhys}}: {{Direct Manipulation}} of {{Information Visualisation}} through a {{Force}}-{{Feedback Pointing Device}}},
  shorttitle = {{{InfoPhys}}},
  booktitle = {10th {{Conference}} on {{Tangible}}, {{Embedded}} \& {{Embodied Interaction}}},
  author = {Frisson, Christian and Dumas, Bruno},
  date = {2016},
  publisher = {{ACM}},
  location = {{Eindhoven, Netherlands}},
  doi = {10.1145/2839462.2856545},
  abstract = {Information visualisation is the transformation of abstract data into visual, interactive representations. In this paper we present InfoPhys, a device that enables the direct, tangible manipulation of visualisations. InfoPhys makes use of a force-feedback pointing device to simulate haptic feedback while the user explores visualisations projected on top of the device. We present a use case illustrating the trends in ten years of TEI proceedings and how InfoPhys allows users to feel and manipulate these trends. The technical and software aspects of our prototype are presented, and promising improvements and future work opened by InfoPhys are then discussed.},
  isbn = {978-1-4503-3582-9},
  keywords = {direct manipulation, force-feedback user interfaces, haptics, information visualization, multimedia browsers, physical visualisation, tangible user interfaces},
  series = {{{TEI}}'16}
}

@inproceedings{WebAudioHapticsWAC2016,
  title = {{{WebAudioHaptics}}: {{Tutorial}} on {{Haptics}} with {{Web Audio}}},
  shorttitle = {{{WebAudioHaptics}}},
  booktitle = {2nd {{Web Audio Conference}}},
  author = {Frisson, Christian and Pietrzak, Thomas and Zhao, Siyan and Israr, Ali},
  date = {2016},
  location = {{Atlanta, Georgia, USA}},
  url = {https://WebAudioHaptics.github.io},
  abstract = {The Web Audio Haptics WAC Tutorial 2016 will explore how to create meaningful haptic content that engages different areas of the body using off-the-shelf hardware and open source software running on a web browser using Web Audio technologies. Participants will 1) learn the basic theories of tactile illusions; 2) get an overview on actuators and sensors; 3) explore tactile illusions using web-based audio tools and a box connecting actuators and sensors to their computer audio I/O; and 4) ideate use cases in groups. Tutorial material will remain available from: https://github.com/WebAudioHaptics},
  keywords = {audio signal processing, Haptics, PureData, vibrotactile, Web Audio},
  series = {{{WAC}}'16}
}

@report{VibroTactileWidgetsHAPPINESS2016,
  title = {Interaction Techniques Leveraging Haptic Feedback on New Interactive Surfaces (Intermediate)},
  shorttitle = {{{VibroTactileWidgetsHAPPINESS2016}}},
  author = {Frisson, Christian and Decaudin, Julien and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Gaffary, Yoren and Lecuyer, Anatole and Latour, Antoine},
  date = {2016-12},
  institution = {{HAPPINESS EU H2020 645145}},
  keywords = {PureData},
  number = {D2.3},
  series = {Deliverables for {{EU H2020}} Project 645145}
}

@unpublished{MechaMediaDuinoFFM2016,
  title = {Force-{{Feedback}} ({{Rotary}}) {{Audio Browsing}}},
  shorttitle = {{{MechaMediaDuinoFFM2016}}},
  author = {Frisson, Christian},
  date = {2016},
  url = {http://www.cirmmt.org/activities/workshops/research/ffedback_music},
  abstract = {A subset of not so new interfaces for musical expression have been traditionally employed in an artistic and scientific field related to and generative of computer music: physical/tangible controls for media browsing. Cyclic representations of time might have been the motivation for the use of rotary control for temporal media (audio and video). Rotary controls have been widely used by experts in audio edition and video montage even before their systems were computerized, with passive proprioceptive and kinesthetic feedback (on hands) limited by the physical controls during their design and fabrication. Why are there no cost-effective commercial devices for force-feedback rotary control widely available now for digital systems, with user-definable mappings, besides the upcoming Microsoft Surface Dial? Can we just make one from off-the-shelf and repurposed components? This talk will start with a short overview of past personal projects on tangible-to-force-feedback media browsing.  The core of the talk is to provide a log reporting hands-on attempts in replicating interaction techniques for force-feedback audio browsing from seminal papers, towards a "hello world" tutorial, using a recent low-cost opensource and openhardware servo motor project (MechaDuino) and a fork of a visual programming environment dedicated for audio/control dataflow (PurrData out of PureData) that had already been used for prototyping force-feedback and music applications.},
  eventtitle = {Force-{{Feedback}} \& {{Music Symposium}}},
  keywords = {audio browsing, force feedback, force-feedback user interfaces, haptics, jog wheel, media browsing, rotary, tangible user interfaces},
  series = {{{FF}}\&{{M}}'16},
  type = {Invited Talk},
  venue = {{Montreal, QC, Canada}}
}

@software{InspectorWidget,
  title = {{{InspectorWidget}}: An Opensource Suite to Track and Analyze Users Behaviors in Their Applications},
  shorttitle = {{{InspectorWidget}}},
  author = {Frisson, Christian and Malacria, Sylvain},
  date = {2016/2018},
  url = {https://github.com/InspectorWidget},
  keywords = {multimedia browsers}
}

@thesis{ChristianFrissonPhDUMONS2015,
  title = {Designing Interaction for Browsing Media Collections (by Similarity)},
  shorttitle = {{{ChristianFrissonPhDUMONS2015}}},
  author = {Frisson, Christian},
  date = {2015-02-17},
  institution = {{University of Mons, numediart Institute, Belgium}},
  url = {https://tel.archives-ouvertes.fr/tel-01570858},
  abstract = {Sound designers source sounds in massive and heavily tagged collections. When searching for media content, once queries are filtered by keywords, hundreds of items are left to be reviewed. How can we present these results efficiently? This doctoral work aims at improving the usability of browsers of media collections by blending techniques from multimedia information retrieval (MIR) and human-computer interaction (HCI). We produced an in-depth state-of-the-art on media browsers. We overviewed HCI and MIR techniques that support our work: organization by content-based similarity (MIR), information visualization and gestural interaction (HCI). We developed the MediaCycle framework for organization by content-based similarity and the DeviceCycle toolbox for rapid prototyping of gestural interaction, both facilitated the design of several media browsers. We evaluated the usability of some of our media browsers. Our main contribution is AudioMetro, an interactive visualization of sound collections. Sounds are represented by content-based glyphs, mapping perceptual sharpness (audio) to brightness and contour (visual). These glyphs are positioned in a starfield display using Student t-distributed Stochastic Neighbor Embedding (tSNE) for dimension reduction, then a proximity grid optimized for preserving direct neighbors. Known-item search evaluation shows that our technique significantly outperforms a grid of sounds represented by dots and ordered by filename.},
  keywords = {browsing, force-feedback, Glyph Design, haptic, haptics, Human computer interaction, Information visualization, multimedia browsers, multimedia information retrieval, music information retrieval, PureData, search user interfaces, similarity, tangible user interfaces},
  langid = {english},
  type = {PhD}
}

@inproceedings{AudioMetroAM2014,
  title = {{{AudioMetro}}: {{Directing Search}} for {{Sound Designers Through Content}}-Based {{Cues}}},
  shorttitle = {{{AudioMetro}}},
  booktitle = {9th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Frisson, Christian and Dupont, Stéphane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
  date = {2014-10-01/2014-10-03},
  publisher = {{ACM}},
  location = {{Aalborg, Denmark}},
  doi = {10.1145/2636879.2636880},
  abstract = {Sound designers source sounds in massive collections, heavily tagged by themselves and sound librarians. For each query, once successive keywords attained a limit to filter down the results, hundreds of sounds are left to be reviewed. AudioMetro combines a new content-based information visualization technique with instant audio feedback to facilitate this part of their workflow. We show through user evaluations by known-item search in collections of textural sounds that a default grid layout ordered by filename unexpectedly outperforms content-based similarity layouts resulting from a recent dimension reduction technique (Student-t Stochastic Neighbor Embedding), even when complemented with content-based glyphs that emphasize local neighborhoods and cue perceptual features. We propose a solution borrowed from image browsing: a proximity grid, whose density we optimize for nearest neighborhood preservation among the closest cells. Not only does it remove overlap but we show through a subsequent user evaluation that it also helps to direct the search. We based our experiments on an open dataset (the OLPC sound library) for replicability.},
  isbn = {978-1-4503-3032-9},
  keywords = {content-based similarity, dimension reduction, known-item search, media browsers, multimedia browsers, music information retrieval, proximity grids, sound effects, Student-t Stochastic Neighbor Embedding, t-SNE, visual variables},
  series = {{{AM}}'14}
}

@inproceedings{TangibleHaystackTEI2014,
  title = {Tangible {{Needle}}, {{Digital Haystack}}: {{Tangible Interfaces}} for {{Reusing Media Content Organized}} by {{Similarity}}},
  shorttitle = {{{TangibleHaystack}}},
  booktitle = {8th {{Conference}} on {{Tangible}}, {{Embedded}} \& {{Embodied Interaction}}},
  author = {Frisson, Christian and Rocca, François and Dupont, Stéphane and Dutoit, Thierry and Grobet, Damien and Giot, Rudi and El Brouzi, Mohammed and Bouaziz, Samir and Yvart, Willy and Merviel, Sylvie},
  date = {2014},
  publisher = {{ACM}},
  location = {{Munich, Germany}},
  doi = {10.1145/2540930.2540983},
  abstract = {This paper presents the design process of a desk-set tangible user interface for the navigation and manipulation of media content organized by content-based similarity with off-the-shelf/flea market devices. For intra-media navigation, a refurbished portable vinyl player has its inside mechanics replaced by a webcam monitoring circular gray code analyzed through computer vision for position/speed tracking. For inter-media navigation, a 3D force-feedback controller is mounted in upright position on a truss with cell clamps, repurposed as trackpad. For media recomposition, motorized faders recall the effect presets of the closest/last selected media item.},
  isbn = {978-1-4503-2635-3},
  keywords = {content-based similarity, contextual inquiry, force feedback, force-feedback user interfaces, haptics, known-item search, multimedia browsers, tangible interfaces, tangible user interfaces},
  series = {{{TEI}}'14}
}

@inproceedings{MetropolitanViewsMMM2014Mediadrom,
  title = {Scenarizing {{Metropolitan Views}}: {{FlanoGraphing}} the {{Urban Spaces}}},
  shorttitle = {Metropolitan {{Views}}},
  booktitle = {20th {{International Conference}} on {{MultiMedia Modeling}}, {{Mediadrom}} Special Session},
  author = {Jacobs, Bénédicte and Jacobs, Laure-Anne and Frisson, Christian and Yvart, Willy and Dutoit, Thierry and Leleu-Merviel, Sylvie},
  date = {2014-01-07/2014-01-10},
  publisher = {{Springer}},
  location = {{Dublin, Ireland}},
  doi = {10.1007/978-3-319-04117-9_2},
  abstract = {The recent decade has seen a rapid evolution in the field of digital media. Mobile devices are now being integrated into every aspect of urban life. GPS, sensor technologies and augmented reality have transformed the new generation of mobile devices from a communication and information platform into a navigational tool, fostering new ways of perceiving reality and image building. Touch sensor technology has changed the screen into a joint input and display device. In this paper we present the FlanoGraph, an application for smartphones and tablets designed to take benefit of the changes induced by mobile devices. We first briefly outline the conceptual background, evoking the work of some researchers in the fields of 'Non Representational Theory', mobile media, and computational data processing. We then present and describe the FlanoGraph through a set of use cases. Finally, we conclude discussing some techniques necessary for the development of the application.},
  isbn = {978-3-319-04116-2},
  keywords = {abstracting technologies, data visualization, database management, FlanoGraph, gestural interaction, GPS, information retrieval, information visualization, navigation, sensing technologies, summarizing technologies, timeline, user interface design},
  series = {{{MMM}}'14 {{Mediadrom}}}
}

@inproceedings{CADastreExquisseMMM2014Mediadrom,
  title = {Scenarizing {{CADastre Exquisse}}: {{A Crossover Between Snoezeling}} in {{Hospitals}}/{{Domes}}, and {{Authoring}}/{{Experiencing Soundful Comic Strips}}},
  shorttitle = {{{CADastre Exquisse}}},
  booktitle = {20th {{International Conference}} on {{MultiMedia Modeling}}, {{Mediadrom}} Special Session},
  author = {Sabato, Cédric and Giraudet, Aurélien and Delattre, Virginie and Desnos, Yves and Frisson, Christian and Giot, Rudi and Yvart, Willy and Rocca, François and Dupont, Stéphane and Bemden, Guy Vandem and Leleu-Merviel, Sylvie and Dutoit, Thierry},
  date = {2014-01-07/2014-01-10},
  publisher = {{Springer}},
  location = {{Dublin, Ireland}},
  doi = {10.1007/978-3-319-04117-9_3},
  abstract = {This paper aims at providing scenarios for the design of authoring and experiencing environments for interactive soundful comic strips. One setting would be a virtual immersive environment made of a dome including spherical projection, surround sound, where visitors comfortably lying down on an interactive mattress can appreciate exquisite corpses floating on the ceiling of the dome, animated, with sound, dependent of the overall behavior of the visitors. On tabletops, creators can generate comic-strip-like creatures by collage or sketching, and associate audiovisual behaviors and soundscapes to these. This creation system will be used in hospitals towards a living lab comforting patients in accepting their health trip. Both settings are inspired by snoezelen methods. These crossover scenarios associate a project by L'Art-Chétype retained to be featured for Mons 2015 EU Capital of Culture and other partners aiming at designing an environment for experiencing/authoring interactive comic-strips augmented with sound.},
  isbn = {978-3-319-04116-2},
  keywords = {collaborative media authoring, interactive comic strips, snoezelen, virtual environments},
  series = {{{MMM}}'14 {{Mediadrom}}}
}

@inproceedings{AudioMetroISMIR2014,
  title = {A Proximity Grid Optimization Method to Improve Audio Search for Sound Design},
  shorttitle = {{{AudioMetro}}},
  booktitle = {15th {{International Symposium}} on {{Music Information Retrieval}}},
  author = {Frisson, Christian and Dupont, Stéphane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
  date = {2014-10-27/2014-10-31},
  location = {{Taipei, Taiwan}},
  doi = {10.5281/zenodo.1417245},
  abstract = {Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques.},
  keywords = {content-based similarity, dimension reduction, known-item search, Media browsers, multimedia browsers, music information retrieval, sound effects, Student-t Stochastic Neighbor Embedding, t-SNE, visual variables},
  series = {{{ISMIR}}'14}
}

@inproceedings{BaltazarsMMM2014Mediadrom,
  title = {An Interactive Device for Exploring Thematically Sorted Art-Works},
  shorttitle = {Baltazars},
  booktitle = {20th {{International Conference}} on {{MultiMedia Modeling}}, {{Mediadrom}} Special Session},
  author = {Baltazar, Aurélie and Baltazar, Pia and Frisson, Christian},
  date = {2014-01-07/2014-01-10},
  publisher = {{Springer}},
  location = {{Dublin, Ireland}},
  doi = {10.1007/978-3-319-04117-9_4},
  abstract = {This Mediadrom artful post-TV scenario consists in sketching the user interface of an interactive media content browsing system for exploring thematically sorted artworks, from the art field of plastic theater, merging art pieces at the intersection of the visual and the performing arts. Combining a touchscreen and an hypermedia browser of image and video content with expert annotations, this system can be installed in venues such as museum and media libraries, and performance spaces as satellite installation to plastic theater performances.},
  keywords = {hypermedia browser, interactive installation, multimedia annotation, multimedia browsers, plastic theater},
  series = {{{MMM}}'14 {{Mediadrom}}}
}

@inproceedings{AuracleeNTERFACE2014,
  title = {Auracle: How Are Salient Cues Situated in Audiovisual Content?},
  shorttitle = {Auracle},
  booktitle = {10th {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Frisson, Christian and Riche, Nicolas and Coutrot, Antoine and Delestage, Charles-Alexandre and Dupont, Stéphane and Ferhat, Onur and Guyader, Nathalie and Mahmoudi, Sidi Ahmed and Mancas, Matei and Mital, Parag K. and Echániz, Alicia Prieto and Rocca, François and Rochette, Alexis and Yvart, Willy},
  date = {2014-06-08/2014-07-05},
  location = {{Bilbao, Spain}},
  keywords = {eye tracking, multimedia browsers},
  series = {{{eNTERFACE}}'14}
}

@article{VideoBrowserShowdownIJMIR2014,
  title = {The {{Video Browser Showdown}}: A Live Evaluation of Interactive Video Search Tools},
  shorttitle = {{{VideoBrowserShowdown}}},
  author = {Schoeffmann, Klaus and Ahlström, David and Bailer, Werner and Cobârzan, Claudiu and Hopfgartner, Frank and McGuinness, Kevin and Gurrin, Cathal and Frisson, Christian and Le, Duy-Dinh and Fabro, Manfred and Bai, Hongliang and Weiss, Wolfgang},
  date = {2014},
  journaltitle = {International Journal of Multimedia Information Retrieval},
  volume = {3},
  pages = {1--15},
  doi = {10.1007/s13735-013-0050-8},
  abstract = {The Video Browser Showdown evaluates the performance of exploratory video search tools on a common data set in a common environment and in presence of the audience. The main goal of this competition is to enable researchers in the field of interactive video search to directly compare their tools at work. In this paper, we present results from the second Video Browser Showdown (VBS2013) and describe and evaluate the tools of all participating teams in detail. The evaluation results give insights on how exploratory video search tools are used and how they perform in direct comparison. Moreover, we compare the achieved performance to results from another user study where 16 participants employed a standard video player to complete the same tasks as performed in VBS2013. This comparison shows that the sophisticated tools enable better performance in general, but for some tasks common video players provide similar performance and could even outperform the expert tools. Our results highlight the need for further improvement of professional tools for interactive search in videos.},
  keywords = {Exploratory search, multimedia browsers, Video browsing, Video retrieval, Video search},
  number = {2},
  series = {{{IJMIR}}'14}
}

@inproceedings{MediaCyclingTEI2013,
  title = {Designing {{Tangible}}/{{Free}}-Form {{Applications}} for {{Navigation}} in {{Audio}}/{{Visual Collections}} (by {{Content}}-Based {{Similarity}})},
  shorttitle = {{{MediaCycling}}},
  booktitle = {7th {{Conference}} on {{Tangible}}, {{Embedded}} \& {{Embodied Interaction}}},
  author = {Frisson, Christian},
  date = {2013-02-10/2013-02-13},
  publisher = {{ACM}},
  location = {{Barcelona, Spain}},
  doi = {10.1145/2460625.2460686},
  abstract = {This paper focuses on one aspect of doctoral studies, within the last year of completion, consisting in designing applications for the navigation (by content-based similarity) in audio or video collections: the choice of tangible or free-form interfaces depending on use cases. One goal of this work is to determine which type of gestural interface suits best each chosen use case making use of navigation into media collections composed of audio or video elements, among: classifying sounds for electroacoustic music composition, derushing video, improvising instant music through an installation organizing and synchronizing audio loops. Prototype applications have been developed using the modular Media-Cycle framework for organization of media content by similarity. We conclude preliminarily that tangible interfaces are better-suited for focused expert tasks and free-form interfaces for multiple-user exploratory tasks, while a combination of both can create emergent practices.},
  isbn = {978-1-4503-1898-3},
  keywords = {free-form interfaces, Interface design, multimedia browsers, multimedia content organization, tangible interfaces, tangible user interfaces},
  series = {{{TEI}}'13}
}

@inproceedings{AudioCycleICME2013,
  title = {Nonlinear Dimensionality Reduction Approaches Applied to Music and Textural Sounds},
  shorttitle = {{{AudioCycle}}},
  booktitle = {14th {{International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Dupont, Stéphane and Ravet, Thierry and Picard-Limpens, Cécile and Frisson, Christian},
  date = {2013-07-15/2013-07-19},
  publisher = {{IEEE}},
  doi = {10.1109/ICME.2013.6607550},
  abstract = {Recently, various dimensionality reduction approaches have been proposed as alternatives to PCA or LDA. These improved approaches do not rely on a linearity assumption, and are hence capable of discovering more complex embeddings within different regions of the data sets. Despite their success on artificial datasets, it is not straightforward to predict which technique is the most appropriate for a given real dataset. In this paper, we empirically evaluate recent techniques on two real audio use cases: musical instrument loops used in music production and sound effects used in sound editing. ISOMAP and t-SNE are being compared to PCA in a visualization problem, where we end up with a two-dimensional view. Various evaluation measures are used: classification performance, as well as trustworthiness/continuity assessing the preservation of neighborhoods. Although PCA and ISOMAP can yield good continuity performance even locally (samples in the original space remain close-by in the low-dimensional one), they fail to preserve the structure of the data well enough to ensure that distinct subgroups remain separate in the visualization. We show that t-SNE presents the best performance, and can even be beneficial as a pre-processing stage for improving classification when the amount of labeled data is low.},
  keywords = {artificial datasets, audio and music analysis, audio signal processing, classification performance, complex embeddings, continuity performance, Databases, Dimensionality reduction, information retrieval, Instruments, ISOMAP, LDA, linearity assumption, Manifolds, Measurement, multimedia browsers, multimedia computing, multimedia information retrieval, music, Music, music production, musical instrument loops, musical instruments, nonlinear dimensionality reduction, PCA, principal component analysis, Principal component analysis, real audio use cases, sound editing, sound effects, Standards, t-SNE, textural sounds, visualization problem},
  series = {{{ICME}}'13}
}

@inproceedings{MashtaCycleINTETAIN2013,
  title = {{{MashtaCycle}}: On-Stage Improvised Audio Collage by Content-Based Similarity and Gesture Recognition},
  shorttitle = {{{MashtaCycle}}},
  booktitle = {5th {{International Conference}} on {{Intelligent Technologies}} for {{Interactive Entertainment}}},
  author = {Frisson, Christian and Keyaerts, Gauthier and Grisard, Fabien and Dupont, Stéphane and Ravet, Thierry and Zajéga, François and Guerra, Laura Colmenares and Todoroff, Todor and Dutoit, Thierry},
  date = {2013-07-03/2013-07-05},
  publisher = {{Springer}},
  location = {{Mons, Belgium}},
  doi = {10.1007/978-3-319-03892-6_14},
  abstract = {In this paper we present the outline of a performance in-progress. It brings together the skilled musical practices from Belgian audio collagist Gauthier Keyaerts aka Very Mash'ta; and the realtime, content-based audio browsing capabilities of the AudioCycle and LoopJam applications developed by the remaining authors. The tool derived from AudioCycle named MashtaCycle aids the preparation of collections of stem audio loops before performances by extracting content-based features (for instance timbre) used for the positioning of these sounds on a 2D visual map. The tool becomes an embodied on-stage instrument, based on a user interface which uses a depth-sensing camera, and augmented with the public projection of the 2D map. The camera tracks the position of the artist within the sensing area to trigger sounds similarly to the LoopJam installation. It also senses gestures from the performer interpreted with the Full Body Interaction (FUBI) framework, allowing to apply sound effects based on bodily movements. MashtaCycle blurs the boundary between performance and preparation, navigation and improvisation, installations and concerts.},
  keywords = {audio collage, content-based similarity, depth cameras, digital audio effects, digital musical instruments, gesture recognition, Human-music interaction, multimedia browsers, PureData},
  series = {{{INTETAIN}}'13}
}

@inproceedings{VideoCycleMMM2013VBS,
  title = {{{VideoCycle}}: {{User}}-{{Friendly Navigation}} by {{Similarity}} in {{Video Databases}}},
  shorttitle = {{{VideoCycle}}},
  booktitle = {19th {{International Conference}} on {{MultiMedia Modeling}}, {{Video Browser Showdown}} Special Session},
  author = {Frisson, Christian and Dupont, Stéphane and Moinet, Alexis and Picard, Cécile and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
  date = {2013-01-07/2013-01-09},
  pages = {550--553},
  publisher = {{Springer}},
  location = {{Huangshan, China}},
  doi = {10.1007/978-3-642-35728-2_66},
  abstract = {VideoCycle is a candidate application for this second Video Browser Showdown challenge. VideoCycle allows interactive intra-video and inter-shot navigation with dedicated gestural controllers. MediaCycle, the framework it is built upon, provides media organization by similarity, with a modular architecture enabling most of its workflow to be performed by plugins: feature extraction, clustering, segmentation, summarization, intra-media and inter-segment visualization. MediaCycle focuses on user experience with user interfaces that can be tailored to specific use cases.},
  keywords = {clustering, feature extraction, jog wheel, known-item search, Media browsers, multimedia browsers, multimedia information retrieval, timeline},
  series = {{{MMM}}'13 {{VBS}}}
}

@inproceedings{MediaSurfacesACMITS2013CmIS,
  title = {Designing Artfully-Mediated Interactive Surfaces Organizing Media Collections},
  shorttitle = {{{MediaSurfaces}}},
  booktitle = {Interactive {{Tabletops}} and {{Surfaces Conf}}., {{Collaboration}} Meets {{Interactive Surfaces Workshop}}},
  author = {Frisson, Christian and Schayes, Eric and Uyttenhove, Simon and {Stéphane Dupont} and Giot, Rudi and Dutoit, Thierry},
  date = {2013-10-06},
  location = {{St Andrews, Scotland, UK}},
  keywords = {interactive surfaces, multimedia browsers},
  series = {{{ACM ITS}}'13 {{CmIS}}}
}

@unpublished{LavaAMPUIST2013SIC,
  title = {{{LavaAMP}}: Surrounding the Beats in Music Tracks by Streaming Colorful Blobs},
  shorttitle = {{{LavaAMPUIST2013SIC}}},
  author = {Frisson, Christian and Schayes, Eric},
  date = {2013-10-08/2013-10-11},
  url = {https://uist.acm.org/uist2013/contest.php},
  abstract = {LavaAMP surrounds the beats in music tracks by streaming colorful blobs. LavaAMP is inspired from vacuum tube amps and lava lamps. Each onset detected in music tracks would trigger an pump impulse in one bottle with cyclic permuting order, creating a flow of colored blobs.},
  eventtitle = {26th {{Symposium}} on {{User Interface Software}} and {{Technology}}, {{Student Innovation Contest}}},
  keywords = {beat tracking, fountain development kit, liquid display, Microsoft Pumpspark, multimedia browsers, music information retrieval, PureData},
  series = {{{ACM UIST}}'13 {{SIC}}},
  type = {Student Innovation Contest},
  venue = {{St. Andrews, Scotland, UK}}
}

@unpublished{VideodromeINTETAIN2013,
  title = {Videodrome: A {{Timeline}} of {{Linked Media Art}}/{{Science}}/{{Technology Events}}},
  shorttitle = {{{VideodromeINTETAIN2013}}},
  author = {Frisson, Christian},
  date = {2013-07-03/2013-07-05},
  url = {http://archive.intetain.org/2013/show/sponsorship},
  eventtitle = {5th {{Intl}}. {{Conf}}. on {{Intelligent Technologies}} for {{Interactive Entertainment}}, {{LinkedTV Demo Session}}},
  keywords = {linked tv, multimedia browsers, timeline},
  note = {https://cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=1NSouRUMxc2KJ5AowT463ae7rOULoTwuSVR6\_\_ZNISlw\&font=Default\&lang=en\&initial\_zoom=2\&height=650},
  series = {{{INTETAIN}}'13},
  type = {Workshop Presentation},
  venue = {{Mons, Belgium}}
}

@artwork{MashtaCycle,
  title = {{{MashtaCycle}}},
  shorttitle = {{{MashtaCycle}}},
  author = {Keyaerts, Gauthier},
  date = {2013},
  editora = {Frisson, Christian},
  editoratype = {collaborator},
  keywords = {digital musical instruments, multimedia browsers}
}

@thesis{FabienGrisardMScINPG2013,
  title = {Gestural Interface for Performative Composition from Collections of Audio Samples.},
  shorttitle = {{{FabienGrisardMScINPG2013}}},
  author = {Grisard, Fabien},
  date = {2013},
  institution = {{Institut National Polytechnique de Grenoble, France}},
  editora = {Dutoit, Thierry and Frisson, Christian},
  editoratype = {collaborator},
  keywords = {digital musical instruments, gestural control, media browser, multimedia browsers, PureData, sound painting},
  type = {MSc}
}

@inproceedings{LoopJamNIME2012,
  title = {{{LoopJam}}: Turning the Dance Floor into a Collaborative Instrumental Map},
  shorttitle = {{{LoopJam}}},
  booktitle = {12th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Frisson, Christian and Dupont, Stéphane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
  editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
  date = {2012-05-21/2012-05-23},
  location = {{Ann Arbor, Michigan}},
  doi = {10.5281/zenodo.1178255},
  abstract = {This paper presents the LoopJam installation which allows participants to interact with a sound map using a 3D computer vision tracking system. The sound map results from similarity-based clustering of sounds. The playback of these sounds is controlled by the positions or gestures of participants tracked with a Kinect depth-sensing camera. The beat-inclined bodily movements of participants in the installation are mapped to the tempo of played sounds, while the playback speed is synchronized by default among all sounds. We presented and tested an early version of the installation to three exhibitions in Belgium, Italy and France. The reactions among participants ranged between curiosity and amusement.},
  keywords = {audio similarity, depth sensors, digital musical instruments, Interactive music systems and retrieval, multimedia browsers, user interaction and interfaces},
  series = {{{NIME}}'12}
}

@inproceedings{LoopJamJIM2012,
  title = {{{LoopJam}}: Une Carte Musicale Collaborative Sur La Piste de Danse},
  shorttitle = {{{LoopJam}}},
  booktitle = {19èmes {{Journées}} d'{{Informatique Musicale}}},
  author = {Frisson, Christian and Dupont, Stéphane and Moinet, Alexis and Leroy, Julien and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
  date = {2012-05-09/2012-05-11},
  pages = {101--105},
  location = {{Mons, Belgium}},
  url = {https://hal.archives-ouvertes.fr/hal-03041774},
  abstract = {Ce papier présente l'installation LoopJam qui permet aux visiteurs d'interagir avec une carte musicale par le biais d'un système de suivi gestuel par vision informatique. La carte sonore résulte d'un partitionnement des sons en groupes par similarité basée sur leur signal. Le rendu sonore est contrôlé par les positions ou gestes des participants captés par une caméra Kinect détectant la profondeur de la scène 3D. Les mouvements des participants exprimant une mesure ou un tempo sont corrélés à la vitesse de lecture commune à tous les échantillons synchronisés par le moteur audio. Nous avons présenté et testé une première version de cette installation lors de trois expositions en Belgique, Italie et France. Les réactions parmi les participants ont varié entre la curiosité et l'amusement.},
  keywords = {audio similarity, depth sensors, digital musical instruments, Interactive music systems and retrieval, multimedia browsers, user interaction and interfaces},
  series = {{{JIM}}'12}
}

@inproceedings{MakamCycleCompMusic2012,
  title = {Improving the {{Understanding}} of {{Turkish Makam Music}} through the {{MediaCycle Framework}}},
  shorttitle = {{{MakamCycle}}},
  booktitle = {2nd {{CompMusic Workshop}}},
  author = {Babacan, Onur and Frisson, Christian and Dutoit, Thierry},
  date = {2012-07-12/2012-07-13},
  pages = {25--28},
  location = {{Istanbul, Turkey}},
  url = {http://compmusic.upf.edu/system/files/static_files/06-Onur-Babacan-et-al-2nd-CompMusic-Workshop-2012_0.pdf},
  keywords = {multimedia browsers, music, music information retrieval},
  series = {{{CompMusic}}'12}
}

@report{MedianeumNUMEDIART2012,
  title = {Medianeum: Crafting Interactive Timelines from Multimedia Content},
  shorttitle = {{{MedianeumNUMEDIART2012}}},
  author = {Zajéga, François and Picard, Cécile and René, Julie and Puleo, Antonin and Decuypere, Justine and Frisson, Christian and Ravet, Thierry and Mancas, Matei},
  date = {2012-06},
  pages = {1--7},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  keywords = {multimedia browsers, timeline},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{MakamCycleNUMEDIART2012,
  title = {{{MakamCycle}}: Improving the Understanding of {{Turkish Makam Music}} through the {{MediaCycle Framework}}},
  shorttitle = {{{MakamCycleNUMEDIART2012}}},
  author = {Frisson, Christian and Babacan, Onur and Dutoit, Thierry},
  date = {2012-06},
  pages = {17--20},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  keywords = {multimedia browsers, music information retrieval},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@artwork{LoopJam,
  title = {{{LoopJam}}},
  shorttitle = {{{LoopJam}}},
  author = {Frisson, Christian},
  date = {2012},
  keywords = {digital musical instruments, multimedia browsers}
}

@artwork{TheListeningRoom,
  title = {The {{Listening Room}}},
  shorttitle = {{{TheListeningRoom}}},
  author = {Frisson, Christian},
  date = {2012},
  keywords = {digital musical instruments, multimedia browsers}
}

@inproceedings{MediaCycleISEA2011,
  title = {Similarity in Media Content: Digital Art Perspectives},
  shorttitle = {{{MediaCycle}}},
  booktitle = {17th {{Symposium}} on {{Electronic Art}}},
  author = {Frisson, Christian and Dupont, Stéphane and Siebert, Xavier and Dutoit, Thierry},
  date = {2011-09-14/2011-09-21},
  location = {{Istanbul, Turkey}},
  url = {https://web.archive.org/web/20171029070517/https://isea2011.sabanciuniv.edu/paper/similarity-media-content-digital-art-perspectives},
  abstract = {This essay examines how media content navigation by similarity can foster new practices in digital arts, blurring the boundaries between composing/performing, curating/authoring, creating/interpreting. With MediaCycle, a framework for browsing media databases by similarity, we created several prototypes: a website for browsing dancers' identities through video recordings, a collaborative dancefloor for music creation.},
  keywords = {content-based similarity, digital art, Media browsers, media content, multimedia browsers, new media arts, similarity},
  series = {{{ISEA}}'11}
}

@report{MediaBlenderNUMEDIART2011,
  title = {{{MediaBlender}} : {{Interactive Multimedia Segmentation}}},
  shorttitle = {{{MediaBlenderNUMEDIART2011}}},
  author = {Dupont, Stéphane and Frisson, Christian and Urbain, Jérôme and Mahmoudi, Sidi and Siebert, Xavier},
  date = {2011-03},
  pages = {1--6},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  keywords = {multimedia browsers},
  number = {1},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{LoopJamNUMEDIART2011,
  title = {{{LoopJam}}: A Collaborative Musical Map on the Dance Floor},
  shorttitle = {{{LoopJamNUMEDIART2011}}},
  author = {Frisson, Christian and Dupont, Stéphane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier},
  date = {2011-06},
  pages = {37--40},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  keywords = {digital musical instruments, multimedia browsers},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{RT-MediaCycleNUMEDIART2011,
  title = {{{RT}}-{{MediaCycle}}: {{Towards}} a Real-Time Use of {{MediaCycle}} in Performances and Video Installations},
  shorttitle = {{{RT}}-{{MediaCycleNUMEDIART2011}}},
  author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Delcourt, Bernard},
  date = {2011-09},
  pages = {55--58},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  keywords = {PureData},
  number = {3},
  series = {{{QPSR}} of the Numediart Research Program}
}

@inproceedings{AudioGardenAM2010,
  title = {Towards {{User}}-{{Friendly Audio Creation}}},
  shorttitle = {{{AudioGarden}}},
  booktitle = {5th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Picard, Cécile and Frisson, Christian and Vanderdonckt, Jean and Tardieu, Damien and Dutoit, Thierry},
  date = {2010-09-15/2010-09-18},
  publisher = {{ACM}},
  location = {{Pitea, Sweden}},
  doi = {10.1145/1859799.1859820},
  abstract = {This paper presents a new approach to sound composition for soundtrack composers and sound designers. We propose a tool for usable sound manipulation and composition that targets sound variety and expressive rendering of the composition. We first automatically segment audio recordings into atomic grains which are displayed on our navigation tool according to signal properties. To perform the synthesis, the user selects one recording as model for rhythmic pattern and timbre evolution, and a set of audio grains. Our synthesis system then processes the chosen sound material to create new sound sequences based on onset detection on the recording model and similarity measurements between the model and the selected grains. With our method, we can create a large variety of sound events such as those encountered in virtual environments or other training simulations, but also sound sequences that can be integrated in a music composition. We present a usability-minded interface that allows to manipulate and tune sound sequences in an appropriate way for sound design.},
  isbn = {978-1-4503-0046-9},
  keywords = {audio analysis & synthesis, Audio Analysis and Synthesis, content-based audio similarity, Content-based Audio Similarity, digital musical instruments, interactive sound composing, Interactive Sound Composing, multi-fidelity prototyping, Multi-fidelity Prototyping, multimedia browsers},
  series = {{{AM}}'10}
}

@inproceedings{AudioCycleAESConv2010,
  title = {Browsing {{Sound}} and {{Music Libraries}} by {{Similarity}}},
  shorttitle = {{{AudioCycle}}},
  booktitle = {128th {{Audio Engineering Society Convention}}},
  author = {Dupont, Stéphane and Frisson, Christian and Siebert, Xavier and Tardieu, Damien},
  date = {2010-05-22/2010-05-25},
  location = {{London, UK}},
  abstract = {This paper presents a prototype tool for browsing through multimedia libraries using content-based multimedia information retrieval techniques. It is composed of several groups of components for multimedia analysis, data mining, interactive visualization, as well as connection with external hardware controllers. The musical application of this tool, uses descriptors of timbre, harmony, as well as rhythm and two different approaches for exploring/browsing content. First, a dynamic data mining allows the user to group sounds into clusters according to those different criteria, whose importance can be weighted interactively. In a second mode, sounds that are similar to a query are returned to the user, and can be used to further proceed with the search. The browsing steps are then stored and visualized as a tree. This approach also borrows from multi-criteria optimization concept to return a relevant list of similar sounds.},
  keywords = {content-based similarity, information visualization, multimedia browsers, multimedia information retrieval, music information retrieval},
  series = {{{AES Conv}}'10}
}

@inproceedings{DotConIHM2010,
  title = {Conception Centrée Utilisateur de Prototypes Interactifs Pour La Gestion de Contenu Multimedia Par Similarité},
  shorttitle = {{{DotCon}}},
  booktitle = {22ème {{Conférence Francophone}} Sur l'{{Interaction Homme}}-{{Machine}}},
  author = {Frisson, Christian},
  date = {2010-09-20/2010-09-23},
  publisher = {{ACM}},
  location = {{Luxembourg}},
  url = {http://ihm2010.afihm.org/programme/rencontres-doctorales.html},
  abstract = {Cet article présente les travaux en cours d’une recherche doctorale visant à proposer une méthodologie de conception centrée utilisateur et de prototypage rapide afin de concevoir des applications interactives destinées à la navigation par similarité dans des bases de données multimedia, adaptées à des cas d’utilisation divers et profils d’utilisateurs variés. Les modalités d’interaction sont volontairement restreintes à la visualisation d’information et l’interaction manuelle. Une méthode de développement rapide, réutilisable et durable est proposée, exemplifiée par quelques prototypes à évaluer par des tests utilisateur.},
  keywords = {interaction gestuelle, multimedia browsers, navigation hypermedia, Prototypage rapide, visualisation d’information},
  series = {{{IHM}}'10}
}

@inproceedings{DeviceCycleNIME2010,
  title = {{{DeviceCycle}}: Rapid and Reusable Prototyping of Gestural Interfaces, Applied to Audio Browsing by Similarity},
  shorttitle = {{{DeviceCycle}}},
  booktitle = {10th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Frisson, Christian and Dupont, Stéphane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry and Macq, Benoit},
  date = {2010-06-15/2010-06-18},
  location = {{Sydney, NSW, Australia}},
  doi = {10.5281/zenodo.1177771},
  abstract = {This paper presents the development of rapid and reusable gestural interface prototypes for navigation by similarity in an audio database and for sound manipulation, using the AudioCycle application. For this purpose, we propose and follow guidelines for rapid prototyping that we apply using the PureData visual programming environment. We have mainly developed three prototypes of manual control: one combining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits and shortcomings we experienced while prototyping using this approach.},
  isbn = {978-0-646-53482-4},
  keywords = {audio database, browsing by similarity, digital musical instruments, force feedback, gestural interfaces, haptics, Human-computer interaction, multimedia browsers, PureData, rapid prototyping},
  series = {{{NIME}}'10}
}

@inproceedings{MultimodalGuitarNIME2010,
  title = {{{MultimodalGuitar}}: A {{Toolbox}} for {{Augmented Guitar Performances}}},
  shorttitle = {{{MultimodalGuitar}}},
  booktitle = {10th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Reboursière, Loïc and Frisson, Christian and Lähdeoja, Otso and Mills III, John Anderson and Picard, Cécile and Todoroff, Todor},
  date = {2010-06-15/2010-06-18},
  location = {{Sydney, Australia}},
  doi = {10.5281/zenodo.1177881},
  abstract = {This project aims at studying how recent interactive and interactions technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed and developed a freely-available toolbox for multimodal guitar performances, compliant with the PureData and Max/MSP modular environments for a more widespread use, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing, modal synthesis, infinite sustain, rearranged looping and “smart” harmonizing.},
  isbn = {978-0-646-53482-4},
  keywords = {audio synthesis, Augmented guitar, digital audio effects, digital musical instruments, gestural sensing, hexaphonic guitar, multimodal interaction, polyphonic transcription, PureData},
  series = {{{NIME}}'10}
}

@inproceedings{CoMediAnnotateeNTERFACE2010,
  title = {{{CoMediAnnotate}}: Towards More Usable Multimedia Content Annotation by Adapting the User Interface},
  shorttitle = {{{CoMediAnnotate}}},
  booktitle = {6th {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Frisson, Christian and Alaçam, Sema and Coşkun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
  date = {2010-07-12/2010-08-06},
  location = {{Amsterdam, Netherlands}},
  abstract = {This project aims at improving the user experience regarding multimedia content annotation. We evaluated and compared current timeline-based annotation tools, so as to elicit user requirements. We address two issues: 1) adapting the user interface, by supporting more input modalities through a rapid prototyping tool and by offering alternative visualization techniques of temporal signals; and 2) covering more steps of the annotation workflow besides the task of annotation itself: notably recording multimodal signals. We developed input devices components for the OpenInterface (OI) platform for rapid prototyping of multimodal interfaces: multitouch screen, jog wheels and pen-based solutions. We modified an annotation tool created with the Smart Sensor Integration (SSI) toolkit and componentized it in OI so as to bind its controls to different input devices. We produced mockups sketches towards a new design of an improved user interface for multimedia content annotation, and started developing a rough prototype using the Processing Development Environment. Our solution allows to produce several prototypes by varying the interaction pipeline: changing input modalities and using either the initial GUI of the annotation tool, or the newly-designed one. We target usability testing to validate our solution and determine which input modalities combination best suits given use cases.},
  keywords = {gestural interaction, information visualization, multimedia browsers, Multimodal annotation, rapid prototyping},
  number = {3},
  series = {{{eNTERFACE}}'10}
}

@article{SofaModalJASP2010,
  title = {Advances in {{Modal Analysis Using}} a {{Robust}} and {{Multiscale Method}}},
  shorttitle = {{{SofaModal}}},
  author = {Picard, Cécile and Frisson, Christian and Faure, François and Drettakis, George and Kry, Paul G.},
  date = {2010-02},
  journaltitle = {EURASIP J. Adv. Signal Process},
  volume = {2010},
  issn = {1110-8657},
  doi = {10.1155/2010/392782},
  abstract = {This paper presents a new approach to modal synthesis for rendering sounds of virtual objects. We propose a generic method that preserves sound variety across the surface of an object at different scales of resolution and for a variety of complex geometries. The technique performs automatic voxelization of a surface model and automatic tuning of the parameters of hexahedral finite elements, based on the distribution of material in each cell. The voxelization is performed using a sparse regular grid embedding of the object, which permits the construction of plausible lower resolution approximations of the modal model. We can compute the audible impulse response of a variety of objects. Our solution is robust and can handle nonmanifold geometries that include both volumetric and surface parts. We present a system which allows us to manipulate and tune sounding objects in an appropriate way for games, training simulations, and other interactive virtual environments.},
  keywords = {audio synthesis, finite element modeling, modal analysis, modal synthesis, PureData},
  series = {{{JASP}}'10}
}

@report{MoViNUMEDIART2010,
  title = {{{MoVi}}: {{MediaCycle Audio}} and {{Visualization}} Improvements},
  shorttitle = {{{MoViNUMEDIART2010}}},
  author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Tardieu, Damien},
  date = {2010-03},
  pages = {5--8},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {multimedia browsers},
  number = {1},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{AudioGardenNUMEDIART2010,
  title = {{{AudioGarden}}: Towards a {{Usable Tool}} for {{Composite Audio Creation}}},
  shorttitle = {{{AudioGardenNUMEDIART2010}}},
  author = {Frisson, Christian and Picard, Cécile and Tardieu, Damien},
  date = {2010-06},
  pages = {33--36},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {digital musical instruments, multimedia browsers},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{CoMediAnnotateNUMEDIART2010,
  title = {{{CoMediAnnotate}}: Towards More Usable Multimedia Content Annotation by Adapting the User Interface},
  shorttitle = {{{CoMediAnnotateNUMEDIART2010}}},
  author = {Frisson, Christian and Alaçam, Sema and Coşkun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
  date = {2010-09},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {gestural interaction, information visualization, multimedia browsers, Multimodal annotation, rapid prototyping},
  number = {3},
  series = {{{QPSR}} of the Numediart Research Program}
}

@inproceedings{AudioCycleCBMI2009,
  title = {{{AudioCycle}}: {{Browsing Musical Loop Libraries}}},
  shorttitle = {{{AudioCycle}}},
  booktitle = {7th {{Workshop}} on {{Content}}-{{Based Multimedia Indexing}}},
  author = {Dupont, Stéphane and Dubuisson, Thomas and Urbain, Jérôme and Sebbe, Raphaël and family=Alessandro, given=Nicolas, prefix=d', useprefix=true and Frisson, Christian},
  date = {2009-06-03/2009-06-05},
  pages = {73--80},
  publisher = {{IEEE}},
  doi = {10.1109/CBMI.2009.19},
  abstract = {This paper presents AudioCycle, a prototype application for browsing through music loop libraries. AudioCycle provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation, and listen to individual audio extracts, searching for those of interest. AudioCycle draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Concepts developped here will be of interest to DJs, remixers, musicians, soundtrack composers, but also sound designers and foley artists. Possible extension to multimedia libraries are also suggested.},
  keywords = {audio feature extraction, Media content, multimedia browsers, multimedia information retrieval},
  series = {{{CBMI}}'09}
}

@inproceedings{AudioCycleICME2009,
  title = {{{AudioCycle}}: {{A}} Similarity-Based Visualization of Musical Libraries},
  shorttitle = {{{AudioCycle}}},
  booktitle = {10th {{Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Urbain, Jérôme and Dubuisson, Thomas and Dupont, Stéphane and Frisson, Christian and Sebbe, Raphaël and family=Alessandro, given=Nicolas, prefix=d', useprefix=true},
  date = {2009-06-28/2009-07-03},
  pages = {1847--1848},
  publisher = {{IEEE}},
  doi = {10.1109/ICME.2009.5202887},
  abstract = {This paper presents AudioCycle, a prototype application for browsing through music loop libraries. AudioCycle provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation and listen to individual audio extracts. AudioCycle draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Possible extension to multimedia libraries are also suggested.},
  keywords = {3D rendering, browsing, multimedia browsers, music similarities, visualization},
  series = {{{ICME}}'09}
}

@inproceedings{MultimodalGuitareNTERFACE2009,
  title = {Multimodal {{Guitar}}: {{Performance Toolbox}} and {{Study Workbench}}},
  shorttitle = {Multimodal {{Guitar}}},
  booktitle = {5th {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Frisson, Christian and Reboursière, Loïc and Chu, Wen-Yang and Lähdeoja, Otso and Mills III, John Anderson and Picard, Cécile and Shen, Ao and Todoroff, Todor},
  date = {2009-07-13/2009-08-08},
  location = {{Genova, Italy}},
  abstract = {This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. We investigate two axes, 1) “A gestural/polyphonic sensing/processing toolbox to augment guitar performances”, and 2) “An interactive guitar score following environment for adaptive learning”. These approaches share quite similar technological challenges (sensing, analysis, processing, synthesis and interaction methods) and dissemination intentions (community-based, low-cost, open-source whenever possible), while leading to different applications (respectively artistic and educational), still targeted towards experienced players and beginners.},
  keywords = {audio synthesis, Audio- and polyphonic multi-pitch transcription, digital audio effects, digital musical instruments, finger tracking, guitar score following, multimodal fusion, multimodal interaction and gestural sensing, particle filtering},
  series = {{{eNTERFACE}}'09}
}

@report{BehavioralInstallationsNUMEDIART2009,
  title = {Behavioral {{Installations}}: {{Emergent}} Audiovisual Installations Influenced by Visitors' Behaviours},
  shorttitle = {{{BehavioralInstallationsNUMEDIART2009}}},
  author = {Filatriau, Jean-Julien and Frisson, Christian and Reboursière, Loïc and Siebert, Xavier and Todoroff, Todor},
  date = {2009-03},
  pages = {9--17},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {multimedia browsers},
  number = {1},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{BodilyBenchmarkNUMEDIART2009,
  title = {Bodily {{Benchmark}}: {{Gestural}}/{{Physiological Analysis}} by {{Remote}}/{{Wearable Sensing}}},
  shorttitle = {{{BodilyBenchmarkNUMEDIART2009}}},
  author = {Frisson, Christian and Reboursière, Loïc and Todoroff, Todor and Filatriau, Jean-Julien},
  date = {2009-06},
  pages = {41--57},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {digital musical instruments},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{MultimodalGuitarNUMEDIART2009,
  title = {Multimodal {{Guitar}}: {{Performance Toolbox}} and {{Study Workbench}}},
  shorttitle = {{{MultimodalGuitarNUMEDIART2009}}},
  author = {Frisson, Christian and Reboursière, Loïc and Chu, Wen-Yang and Lähdeoja, Otso and III, John Anderson Mills and Picard, Cécile and Shen, Ao and Todoroff, Todor},
  date = {2009-09},
  pages = {67--84},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {digital musical instruments, PureData},
  number = {3},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{MultiMediaCycleNUMEDIART2009,
  title = {{{MultiMediaCycle}}: {{Consolidating}} the {{HyForge Framework}} towards {{Improved Scalability}} and {{Usability}}},
  shorttitle = {{{MultiMediaCycleNUMEDIART2009}}},
  author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Tardieu, Damien},
  date = {2009-12},
  pages = {113--117},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {multimedia browsers},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@unpublished{MultimodalGuitarJEIGE2009,
  title = {Objectifs Du Projet {{Multimodal Guitar}}},
  shorttitle = {{{MultimodalGuitarJEIGE2009}}},
  author = {Frisson, Christian and Reboursière, Loïc},
  date = {2009-05-18/2009-05-19},
  eventtitle = {Journées d'{{Étude Identités}} de La {{Guitare Électrique}}},
  keywords = {digital musical instruments, guitar, multimodal, PureData},
  series = {{{JEIGE}}'09},
  type = {Workshop Presentation},
  venue = {{Maison des Sciences de l'Homme Paris-Nord, Saint Denis, France}}
}

@artwork{MetaCrane,
  title = {Méta-{{Crâne}}},
  shorttitle = {{{MetaCrane}}},
  author = {Israel, Thomas},
  date = {2009},
  url = {http://www.thomasisrael.be/pf/meta-crane/},
  editora = {Frisson, Christian},
  editoratype = {collaborator},
  keywords = {multimedia browsers}
}

@artwork{BioDiva,
  title = {{{BioDiva}}},
  shorttitle = {{{BioDiva}}},
  author = {Moletta, Laurence},
  date = {2009},
  editora = {Frisson, Christian},
  editoratype = {collaborator},
  keywords = {digital musical instruments}
}

@thesis{RemyLabbeMScUCLouvain2009,
  title = {Sound Spatialisation through Multi-Camera Analysis of Gestures},
  shorttitle = {{{RemyLabbeMScUCLouvain2009}}},
  author = {Labbé, Rémy},
  date = {2009},
  institution = {{Université catholique de Louvain, Belgium}},
  editora = {Macq, Benoît and Filatriau, Jean-Julien and Frisson, Christian and Van Brussel, Christian},
  editoratype = {collaborator},
  keywords = {computer vision, digital musical instruments, PureData, sound spatialization},
  type = {MSc}
}

@report{AudioSkimmingNUMEDIART2008,
  title = {Audio {{Skimming}}},
  shorttitle = {{{AudioSkimmingNUMEDIART2008}}},
  author = {Couvreur, Laurent and Bettens, Frédéric and Drugman, Thomas and Frisson, Christian and Jottrand, Matthieu and Mancas, Matei and Moinet, Alexis},
  date = {2008-03},
  pages = {1--16},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {multimedia browsers, PureData},
  number = {1},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{AudioThumbnailingNUMEDIART2008,
  title = {Audio {{Thumbnailing}}},
  shorttitle = {{{AudioThumbnailingNUMEDIART2008}}},
  author = {Couvreur, Laurent and Bettens, Frédéric and Drugman, Thomas and Dubuisson, Thomas and Dupont, Stéphane and Frisson, Christian and Jottrand, Matthieu and Mancas, Matei},
  date = {2008-06},
  pages = {67--85},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {multimedia browsers},
  number = {2},
  series = {{{QPSR}} of the Numediart Research Program}
}

@report{AudioCycleNUMEDIART2008,
  title = {Audio {{Cycle}}},
  shorttitle = {{{AudioCycleNUMEDIART2008}}},
  author = {Dupont, Stéphane and family=Alessandro, given=Nicolas, prefix=d', useprefix=true and Dubuisson, Thomas and Frisson, Christian and Sebbe, Raphaël and Urbain, Jérôme},
  date = {2008-12},
  pages = {119--127},
  institution = {{numediart Research Program on Digital Art Technologies}},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  keywords = {multimedia browsers},
  number = {4},
  series = {{{QPSR}} of the Numediart Research Program}
}

@inproceedings{PhysioContenteNTERFACE2007,
  title = {Audiovisual {{Content Generation Controlled}} by {{Physiological Signals}} for {{Clinical}} and {{Artistic Applications}}},
  shorttitle = {{{PhysioContent}}},
  booktitle = {3rd {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Benovoy, Mitchel and Brouse, Andrew and Corcoran, Thomas Greg and Drayson, Hannah and Erkut, Cumhur and Filatriau, Jean-Julien and Frisson, Christian and Gundogdu, Umut and Knapp, Ben and Lehembre, Rémy and Mühl, Christian and Pérez, Miguel Angel Ortiz and Sayin, Alaattin and Soleymani, Mohammad and Tahiroğlu, Koray},
  date = {2007-07-16/2007-08-10},
  pages = {103--116},
  location = {{Istanbul, Turkey}},
  url = {http://www.cmpe.boun.edu.tr/enterface07/outputs/final/p8report.pdf},
  abstract = {While an extensive palette of sound and visual generation techniques have been developed during the era of digital signal processing, the design of innovative virtual instruments has come to dramatic fruition over the last decade. The use of measured biological signals to drive these instruments proposes some new and powerful tools for clinical, scientific and artistic applications. Over the period of one month - during the eNTERFACE’07 summer workshop in  ̇Istanbul, Turkey - researchers from the fields of human-computer interfaces, sound synthesis and new media art worked together towards this common goal. A framework for auditory display and bio-musical applications was established upon which were based different experimental prototypes. Diverse methods for the analysis of measured physiological signals and of mapping the extracted parameters to sound and visual synthesis processes were explored. Biologically-driven musical instruments and data displays for clinical and medical purposes were built. From this have emerged some worthwhile perspectives on future research. This report summarises the results of that project.},
  keywords = {Auditory display, Biologically-augmented performances, Biosignals, Brain-computer interfaces, digital musical instruments, Interactive arts, Multimodal interfaces, Sonification},
  series = {{{eNTERFACE}}'07}
}

@thesis{FlorentCouchariereMScUCLouvain2007,
  title = {Analysis of the Drummer Instrumental Gesture through Electromyography ({{EMG}})},
  shorttitle = {{{FlorentCouchariereMScUCLouvain2007}}},
  author = {Coucharière, Florent},
  date = {2007},
  institution = {{Université catholique de Louvain, Belgium}},
  editora = {Macq, Benoît and Filatriau, Jean-Julien and Frisson, Christian},
  editoratype = {collaborator},
  keywords = {drums, electromyography, emg, sensor},
  type = {MSc}
}

@thesis{ChristianFrissonMScINPG2006,
  title = {Comparaison de Techniques de Synthèse de Type Signal Additive/Source-Filtre et Physique de Type Particulaire: Modèle de Représentation, Environnement, Type de Contrôle et Accès},
  shorttitle = {{{ChristianFrissonMScINPG2006}}},
  author = {Frisson, Christian},
  date = {2006},
  institution = {{Institut National Polytechnique de Grenoble, France}},
  location = {{ACROE, Grenoble, France + McGill University, Montreal, Canada}},
  editora = {Cadoz, Claude and Depalle, Philippe and Wanderley, Marcelo M.},
  editoratype = {collaborator},
  keywords = {audio synthesis, sound synthesis},
  type = {MSc}
}

@unpublished{HelmholtzJFIS2005,
  title = {Helmoltz: Un Outil de Caractérisation Des Deux Premiers Modes de La Guitare},
  shorttitle = {{{HelmholtzJFIS2005}}},
  author = {Gautier, François and Le Carrou, Jean-Loïc and Collin, Erwan and Dufaud, Jérémie and Frisson, Christian},
  date = {2005-07},
  url = {http://www.itemm.fr/jfis/},
  abstract = {Ce projet porte sur l’étude vibro-acoustique de la guitare. L’objectif est de fournir au luthier une chaîne de mesure simplifiée permettant d’extraire les paramètres d’un modèle à deux degrés de liberté de la guitare. Cet appui graphique lui permettra d’associer les caractéristiques physiques d’une guitare (qualité du bois, masse volumique des composants, raideur de la table d’harmonie) avec une courbe de réponse en fréquence. Nous devons ainsi mettre à disposition au luthier un banc de mesure simple, rapide à l’utilisation et peu coûteux lui permettant de mesurer la réponse en fréquence de caisses de résonance et d'obtenir des paramètres objectifs.},
  eventtitle = {Journées {{Facture Instrumentale}} \& {{Sciences}}: “{{Mettre}} En {{Commun}}”},
  keywords = {guitar, luthier, modal analysis, Vibroacoustics},
  series = {{{JFIS}}'05},
  type = {Workshop Presentation},
  venue = {{Le Mans, France}}
}

@thesis{ChristianFrissonMEngENSIM2005,
  title = {Développement de l’instrumentarium de l’{{Eobody}}},
  shorttitle = {{{ChristianFrissonMEngENSIM2005}}},
  author = {Frisson, Christian},
  date = {2005},
  institution = {{Ecole Nationale Supérieure d’Ingénieurs du Mans, France}},
  location = {{Eowave, Paris, France}},
  editora = {Sirguy, Marc and Gaviot, Etienne},
  editoratype = {collaborator},
  keywords = {digital musical instruments, midi, sensor},
  type = {MEng}
}


