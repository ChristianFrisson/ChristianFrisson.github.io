@article{DataChangesTVCG2020,
	title = {Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2019.2934538},
	series = {{TVCG}'20},
	shorttitle = {{DataChanges}},
	abstract = {Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.},
	pages = {12--22},
	number = {1},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Walny, Jagoda and Frisson, Christian and West, Mieka and Kosminsky, Doris and Knudsen, Søren and Carpendale, Sheelagh and Willett, Wesley},
	date = {2020-01},
	year = {2020},
	note = {{ZSCC}: {NoCitationData}[s0] 
Publisher: {IEEE}},
	keywords = {design process, Data visualization, Information visualization, Tools, Software, Collaboration, data mapping, Design tools, Task analysis, design handoff}
}


@inproceedings{TorqueTunerNIME2020,
	location = {Birmihgham, {UK}},
	title = {{TorqueTuner}: A self contained module for designing rotary haptic force feedback for digital musical instruments},
	series = {{NIME}'20},
	shorttitle = {{TorqueTuner}},
	booktitle = {20th Conf. on New Interfaces for Musical Expression},
	author = {Kirkegaard, Mathias and Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M.},
	date = {2020},
	year = {2020},
	url = {https://www.nime.org/proceedings/2020/nime2020_paper52.pdf},
	note = {{ZSCC}: {NoCitationData}[s0] 
Place: Birmingham, {UK}}
}

@inproceedings{CIRMMTVisHCII2020,
	location = {Copenhagen, Denmark},
	title = {A Visualization Tool for the {CIRMMT} Distinguished Lecture Series},
	doi = {10.1007/978-3-030-50020-7_10},
	series = {{HCII}'20},
	shorttitle = {{CIRMMTVis}},
	booktitle = {22nd International Conference on Human-Computer Interaction},
	publisher = {Springer},
	author = {Wanderley, Marcelo M. and Bredholt, Mathias and Frisson, Christian},
	date = {2020},
	year = {2020},
	note = {{ZSCC}: {NoCitationData}[s0] 
Place: Copenhagen, Denmark}
}

@inproceedings{RepHapHAPTICS2020WIP,
	location = {Washington, {DC}, {USA}},
	title = {{RepHap}: towards an open source platform for benchmarking haptic devices leveraging the Robot Operating System ecosystem},
	series = {{HAPTICS}'20 {WIP}},
	shorttitle = {{RepHap}},
	booktitle = {Haptics Symposium, Works in Progress},
	publisher = {{IEEE}},
	author = {Frisson, Christian and Delbos, Benjamin and Désourdy, Félix and Ding, Steve and Wanderley, Marcelo M. and Lévesque, Vincent and Gallacher, Colin},
	date = {2020},
	year = {2020},
	note = {{ZSCC}: {NoCitationData}[s0] 
Place: Washington, {DC}, {USA}}
}

@inproceedings{FreesoundTrackerHAID2019,
	location = {Lille, France},
	title = {Haptic techniques for browsing sound maps organized by similarity},
	url = {https://hal.inria.fr/hal-02050235},
	series = {{HAID}'19},
	shorttitle = {{FreesoundTracker}},
	pages = {1},
	booktitle = {9th Intl. Workshop on Haptic and Audio Interaction Design},
	author = {Frisson, Christian and Gallacher, Colin and Wanderley, Marcelo M.},
	date = {2019-03},
	year = {2019},
	note = {{ZSCC}: 0000000 
Place: Lille, France}
}

@inproceedings{DataMappingVIS2018InfoVisPosters,
	location = {Berlin, Germany},
	title = {Discovering the Data Mapping of an Unfamiliar Visualization},
	series = {{VIS}'18 {InfoVis} Posters},
	shorttitle = {{DataMapping}},
	booktitle = {23rd Conf. on Information Visualization},
	publisher = {{IEEE}},
	author = {Hynes, Lisa and Huynh, Tina and Storteboom, Sarah and Walny, Jagoda and Frisson, Christian and Kosminsky, Doris and West, Mieka and Carpendale, Sheelagh and Willett, Wesley},
	date = {2018},
	year = {2018},
	note = {{ZSCC}: {NoCitationData}[s0] 
Video: https://vimeo.com/290331230}
}

@inproceedings{EnergyVisCHI2018EA,
	location = {Montreal, Canada},
	title = {Democratizing Open Energy Data for Public Discourse using Visualization},
	isbn = {978-1-4503-5621-3},
	doi = {10.1145/3170427.3186539},
	series = {{CHI}'18 {EA}},
	shorttitle = {{EnergyVis}},
	abstract = {For this demo, we will show two interactive visualizations: Energy Futures and Pipeline Incidents. We designed and developed these visualizations as part of an open data initiative that aims to create interactive data visualizations to help make Canada's energy data publicly accessible, transparent, and understandable. This work was conducted in collaboration with the National Energy Board of Canada ({NEB}) and a visualization software development company, {VizworX}.},
	booktitle = {35th Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Knudsen, Søren and Vermeulen, Jo and Kosminsky, Doris and Walny, Jagoda and West, Mieka and Frisson, Christian and Adriel Aseniero, Bon and {MacDonald} Vermeulen, Lindsay and Perin, Charles and Quach, Lien and Buk, Peter and Tabuli, Katrina and Chopra, Shreya and Willett, Wesley and Carpendale, Sheelagh},
	date = {2018},
	year = {2018},
	note = {{ZSCC}: 0000001},
	keywords = {information visualization, data visualization, open data, citizen engagement, data democratization, Open data}
}

@inproceedings{VibroTactileWidgetsUIST2017Adjunct,
	location = {Quebec, Qc, Canada},
	title = {Designing Vibrotactile Widgets with Printed Actuators and Sensors},
	isbn = {978-1-4503-5419-6},
	doi = {10.1145/3131785.3131800},
	series = {{UIST}'17 Adjunct},
	shorttitle = {{VibroTactileWidgets}},
	abstract = {Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.},
	pages = {11--13},
	booktitle = {30th Symposium on User Interface Software and Technology},
	publisher = {{ACM}},
	author = {Frisson, Christian and Decaudin, Julien and Pietrzak, Thomas and Ng, Alexander and Poncet, Pauline and Casset, Fabrice and Latour, Antoine and Brewster, Stephen A.},
	date = {2017},
	year = {2017},
	note = {{ZSCC}: 0000005},
	keywords = {vibrotactile feedback, Vibrotactile feedback, printed electronics, piezoelectric ink, printed actuators, printed sensors, thin-film actuators, widgets}
}

@inproceedings{StereoHapticsSIGGRAPH2016,
	location = {Anaheim, California, {USA}},
	title = {Stereohaptics: A Haptic Interaction Toolkit for Tangible Virtual Experiences},
	isbn = {978-1-4503-4373-2},
	doi = {10.1145/2929484.2970273},
	series = {{SIGGRAPH}'16},
	shorttitle = {{StereoHaptics}},
	abstract = {With a recent rise in the availability of affordable head mounted gear sets, various sensory stimulations (e.g., visual, auditory and haptics) are integrated to provide seamlessly embodied virtual experience in areas such as education, entertainment, therapy and social interactions. Currently, there is an abundance of available toolkits and application programming interfaces ({APIs}) for generating the visual and audio content. However, such richness in hardware technologies and software tools is missing in designing haptic experiences. Current solutions to integrate haptic effects are limited due to: i) a user's rigid adaptation to new hardware and software technologies, ii) limited scalability of the existing tools to incorporate haptic hardware and applications, iii) inflexible authoring capabilities, iv) missing infrastructure for storing, playback and distribution, and v) and unreliable hardware for long term usage.},
	booktitle = {{ACM} {SIGGRAPH} 2016 Studio},
	publisher = {{ACM}},
	author = {Israr, Ali and Zhao, Siyan and {McIntosh}, Kyna and Schwemler, Zachary and Fritz, Adam and Mars, John and Bedford, Job and Frisson, Christian and Huerta, Ivan and Kosek, Maggie and Koniaris, Babis and Mitchell, Kenny},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: {NoCitationData}[s0]
Type: Tutorial},
	keywords = {Haptics, vibrotactile, toolkit, audio signal processing, {PureData}}
}

@inproceedings{ComixTripDIS2016Companion,
	location = {Brisbane, Australia},
	title = {{ComixTrip}: Reading Comic Books with Text Sequenced through Gaze Tracking},
	isbn = {978-1-4503-4315-2},
	doi = {10.1145/2908805.2909405},
	series = {{DIS}'16 Companion},
	shorttitle = {{ComixTrip}},
	abstract = {Paper-based comic books are rendered on a static medium, where time can alternatively be sequenced through space. People usually prefer to read comic books at their pace. When comic books are digital, their medium becomes dynamic and interactive, how can the readers' experience be redesigned? We present {ComixTrip}, a system with which people can read digital comic books with sequential media cues responsively adapted to their reading pace. Our system relies on low-cost eye tracking both for measuring how people read; and for sequencing speech balloons semi-automatically: once a balloon is read, the next one is displayed. We ran a study to analyse how people read text paragraphs with diverse spatial layouts by tracking their gaze. Our preliminary results show that we may accurately track when people have read balloons in comic books. Our system needs to be improved regarding inter- and intra-person reading speed variations.},
	booktitle = {Proceedings of the 2016 {ACM} Conference Companion Publication on Designing Interactive Systems},
	publisher = {{ACM}},
	author = {Rochette, Alexis and Goossens, Cédric and Giot, Rudi and Frisson, Christian},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: 0000000},
	keywords = {eye tracking, comic book, reading time, Comic book, Eye tracking, Reading time}
}

@inproceedings{InspectorWidgetCHI2016EA,
	location = {San Jose, California, {USA}},
	title = {{InspectorWidget}: A System to Analyze Users Behaviors in Their Applications},
	isbn = {978-1-4503-4082-3},
	doi = {10.1145/2851581.2892388},
	series = {{CHI}'16 {EA}},
	shorttitle = {{InspectorWidget}},
	abstract = {We propose {InspectorWidget}, an opensource application to track and analyze users' behaviors in interactive software. The key contributions of our application are: 1) it works with closed applications that do not provide source code nor scripting capabilities; 2) it covers the whole pipeline of software analysis from logging input events to visual statistics through browsing and programmable annotation; 3) it allows post-recording logging; and 4) it does not require programming skills. To achieve this, {InspectorWidget} combines low-level event logging (e.g. mouse and keyboard events) and high-level screen features (e.g. interface widgets) captured though computer vision techniques. {InspectorWidget} benefits end users, usability experts and {HCI} researchers.},
	booktitle = {33rd Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Frisson, Christian and Malacria, Sylvain and Bailly, Gilles and Dutoit, Thierry},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: 0000005},
	keywords = {computer vision, automatic annotation, logging, Logging}
}

@inproceedings{InfoPhysTEI2016,
	location = {Eindhoven, Netherlands},
	title = {{InfoPhys}: Direct Manipulation of Information Visualisation through a Force-Feedback Pointing Device},
	isbn = {978-1-4503-3582-9},
	doi = {10.1145/2839462.2856545},
	series = {{TEI}'16},
	shorttitle = {{InfoPhys}},
	abstract = {Information visualisation is the transformation of abstract data into visual, interactive representations. In this paper we present {InfoPhys}, a device that enables the direct, tangible manipulation of visualisations. {InfoPhys} makes use of a force-feedback pointing device to simulate haptic feedback while the user explores visualisations projected on top of the device. We present a use case illustrating the trends in ten years of {TEI} proceedings and how {InfoPhys} allows users to feel and manipulate these trends. The technical and software aspects of our prototype are presented, and promising improvements and future work opened by {InfoPhys} are then discussed.},
	booktitle = {10th Conference on Tangible, Embedded \& Embodied Interaction},
	publisher = {{ACM}},
	author = {Frisson, Christian and Dumas, Bruno},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: 0000000},
	keywords = {direct manipulation, information visualization, haptics, force-feedback user interfaces, physical visualisation, Tangible user interfaces}
}


@thesis{ChristianFrissonPhDUMONS2015,
	title = {Designing interaction for browsing media collections (by similarity)},
	url = {https://tel.archives-ouvertes.fr/tel-01570858},
	shorttitle = {{ChristianFrissonPhDUMONS}2015},
	abstract = {Sound designers source sounds in massive and heavily tagged collections. When searching for media content, once queries are filtered by keywords, hundreds of items are left to be reviewed. How can we present these results efficiently? This doctoral work aims at improving the usability of browsers of media collections by blending techniques from multimedia information retrieval ({MIR}) and human-computer interaction ({HCI}). We produced an in-depth state-of-the-art on media browsers. We overviewed {HCI} and {MIR} techniques that support our work: organization by content-based similarity ({MIR}), information visualization and gestural interaction ({HCI}). We developed the {MediaCycle} framework for organization by content-based similarity and the {DeviceCycle} toolbox for rapid prototyping of gestural interaction, both facilitated the design of several media browsers. We evaluated the usability of some of our media browsers. Our main contribution is {AudioMetro}, an interactive visualization of sound collections. Sounds are represented by content-based glyphs, mapping perceptual sharpness (audio) to brightness and contour (visual). These glyphs are positioned in a starfield display using Student t-distributed Stochastic Neighbor Embedding ({tSNE}) for dimension reduction, then a proximity grid optimized for preserving direct neighbors. Known-item search evaluation shows that our technique significantly outperforms a grid of sounds represented by dots and ordered by filename.},
	institution = {University of Mons, numediart Institute, Belgium},
	type = {phdthesis},
	author = {Frisson, Christian},
	date = {2015-02-17},
	year = {2015},
	langid = {english},
	note = {{ZSCC}: 0000002},
	keywords = {Haptics, Search user interfaces, Glyph Design, Information visualization, Browsing, Force-feedback, Haptic {UIs}, Human computer interaction {HCI}, Information visualisation, Multimedia information retrieval, Music information retrieval, Music information retrieval {MIR}, Similarity, Tangible user interface, Tangible user interface {TUI}, Tangible user interfaces {TUIs}}
}

@inproceedings{AudioMetroAM2014,
	location = {Aalborg, Denmark},
	title = {{AudioMetro}: Directing Search for Sound Designers Through Content-based Cues},
	isbn = {978-1-4503-3032-9},
	doi = {10.1145/2636879.2636880},
	series = {{AM}'14},
	shorttitle = {{AudioMetro}},
	abstract = {Sound designers source sounds in massive collections, heavily tagged by themselves and sound librarians. For each query, once successive keywords attained a limit to filter down the results, hundreds of sounds are left to be reviewed. {AudioMetro} combines a new content-based information visualization technique with instant audio feedback to facilitate this part of their workflow. We show through user evaluations by known-item search in collections of textural sounds that a default grid layout ordered by filename unexpectedly outperforms content-based similarity layouts resulting from a recent dimension reduction technique (Student-t Stochastic Neighbor Embedding), even when complemented with content-based glyphs that emphasize local neighborhoods and cue perceptual features. We propose a solution borrowed from image browsing: a proximity grid, whose density we optimize for nearest neighborhood preservation among the closest cells. Not only does it remove overlap but we show through a subsequent user evaluation that it also helps to direct the search. We based our experiments on an open dataset (the {OLPC} sound library) for replicability.},
	booktitle = {9th {AudioMostly} Conference on Interaction with Sound},
	publisher = {{ACM}},
	author = {Frisson, Christian and Dupont, Stéphane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
	date = {2014-10-01},
	year = {2014},
	note = {{ZSCC}: 0000006},
	keywords = {content-based similarity, known-item search, media browsers, music information retrieval, sound effects, visual variables, dimension reduction, Media browsers, proximity grids, Student-t Stochastic Neighbor Embedding, {tSNE}}
}

@inproceedings{TangibleHaystackTEI2014,
	location = {Munich, Germany},
	title = {Tangible Needle, Digital Haystack: Tangible Interfaces for Reusing Media Content Organized by Similarity},
	isbn = {978-1-4503-2635-3},
	doi = {10.1145/2540930.2540983},
	series = {{TEI}'14},
	shorttitle = {{TangibleHaystack}},
	abstract = {This paper presents the design process of a desk-set tangible user interface for the navigation and manipulation of media content organized by content-based similarity with off-the-shelf/flea market devices. For intra-media navigation, a refurbished portable vinyl player has its inside mechanics replaced by a webcam monitoring circular gray code analyzed through computer vision for position/speed tracking. For inter-media navigation, a 3D force-feedback controller is mounted in upright position on a truss with cell clamps, repurposed as trackpad. For media recomposition, motorized faders recall the effect presets of the closest/last selected media item.},
	booktitle = {8th Conference on Tangible, Embedded \& Embodied Interaction},
	publisher = {{ACM}},
	author = {Frisson, Christian and Rocca, François and Dupont, Stéphane and Dutoit, Thierry and Grobet, Damien and Giot, Rudi and El Brouzi, Mohammed and Bouaziz, Samir and Yvart, Willy and Merviel, Sylvie},
	date = {2014},
	year = {2014},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {tangible interfaces, force-feedback user interfaces, Tangible user interfaces, content-based similarity, contextual inquiry, known-item search}
}

@inproceedings{MetropolitanViewsMMM2014Mediadrom,
	location = {Dublin, Ireland},
	title = {Scenarizing Metropolitan Views: {FlanoGraphing} the Urban Spaces},
	isbn = {978-3-319-04116-2},
	doi = {10.1007/978-3-319-04117-9_2},
	series = {{MMM}'14 Mediadrom},
	shorttitle = {Metropolitan Views},
	abstract = {The recent decade has seen a rapid evolution in the field of digital media. Mobile devices are now being integrated into every aspect of urban life. {GPS}, sensor technologies and augmented reality have transformed the new generation of mobile devices from a communication and information platform into a navigational tool, fostering new ways of perceiving reality and image building. Touch sensor technology has changed the screen into a joint input and display device. In this paper we present the {FlanoGraph}, an application for smartphones and tablets designed to take benefit of the changes induced by mobile devices. We first briefly outline the conceptual background, evoking the work of some researchers in the fields of 'Non Representational Theory', mobile media, and computational data processing. We then present and describe the {FlanoGraph} through a set of use cases. Finally, we conclude discussing some techniques necessary for the development of the application.},
	booktitle = {20th International Conference on {MultiMedia} Modeling, Mediadrom special session},
	publisher = {Springer},
	author = {Jacobs, Bénédicte and Jacobs, Laure-Anne and Frisson, Christian and Yvart, Willy and Dutoit, Thierry and Leleu-Merviel, Sylvie},
	date = {2014-01-07},
	year = {2014},
	note = {{ZSCC}: 0000001},
	keywords = {user interface design, timeline, data visualization, navigation, abstracting technologies, database management, {FlanoGraph}, gestural interaction, {GPS}, information retrieval, sensing technologies, summarizing technologies}
}

@inproceedings{CADastreExquisseMMM2014Mediadrom,
	location = {Dublin, Ireland},
	title = {Scenarizing {CADastre} Exquisse: A Crossover Between Snoezeling in Hospitals/Domes, and Authoring/Experiencing Soundful Comic Strips},
	isbn = {978-3-319-04116-2},
	doi = {10.1007/978-3-319-04117-9_3},
	series = {{MMM}'14 Mediadrom},
	shorttitle = {{CADastre} Exquisse},
	abstract = {This paper aims at providing scenarios for the design of authoring and experiencing environments for interactive soundful comic strips. One setting would be a virtual immersive environment made of a dome including spherical projection, surround sound, where visitors comfortably lying down on an interactive mattress can appreciate exquisite corpses floating on the ceiling of the dome, animated, with sound, dependent of the overall behavior of the visitors. On tabletops, creators can generate comic-strip-like creatures by collage or sketching, and associate audiovisual behaviors and soundscapes to these. This creation system will be used in hospitals towards a living lab comforting patients in accepting their health trip. Both settings are inspired by snoezelen methods. These crossover scenarios associate a project by L'Art-Chétype retained to be featured for Mons 2015 {EU} Capital of Culture and other partners aiming at designing an environment for experiencing/authoring interactive comic-strips augmented with sound.},
	booktitle = {20th International Conference on {MultiMedia} Modeling, Mediadrom special session},
	publisher = {Springer},
	author = {Sabato, Cédric and Giraudet, Aurélien and Delattre, Virginie and Desnos, Yves and Frisson, Christian and Giot, Rudi and Yvart, Willy and Rocca, François and Dupont, Stéphane and Bemden, Guy Vandem and Leleu-Merviel, Sylvie and Dutoit, Thierry},
	date = {2014-01-07},
	year = {2014},
	note = {{ZSCC}: 0000001},
	keywords = {virtual environments, collaborative media authoring, interactive comic strips, snoezelen}
}

@inproceedings{MediaCyclingTEI2013,
	location = {Barcelona, Spain},
	title = {Designing Tangible/Free-form Applications for Navigation in Audio/Visual Collections (by Content-based Similarity)},
	isbn = {978-1-4503-1898-3},
	doi = {10.1145/2460625.2460686},
	series = {{TEI}'13},
	shorttitle = {{MediaCycling}},
	abstract = {This paper focuses on one aspect of doctoral studies, within the last year of completion, consisting in designing applications for the navigation (by content-based similarity) in audio or video collections: the choice of tangible or free-form interfaces depending on use cases. One goal of this work is to determine which type of gestural interface suits best each chosen use case making use of navigation into media collections composed of audio or video elements, among: classifying sounds for electroacoustic music composition, derushing video, improvising instant music through an installation organizing and synchronizing audio loops. Prototype applications have been developed using the modular Media-Cycle framework for organization of media content by similarity. We conclude preliminarily that tangible interfaces are better-suited for focused expert tasks and free-form interfaces for multiple-user exploratory tasks, while a combination of both can create emergent practices.},
	booktitle = {7th Conference on Tangible, Embedded \& Embodied Interaction},
	publisher = {{ACM}},
	author = {Frisson, Christian},
	date = {2013-02-10},
	year = {2013},
	note = {{ZSCC}: 0000002},
	keywords = {tangible interfaces, tangible user interfaces, interface design, free-form interfaces, multimedia content organization, Interface design}
}

@inproceedings{AudioCycleICME2013,
	title = {Nonlinear dimensionality reduction approaches applied to music and textural sounds},
	doi = {10.1109/ICME.2013.6607550},
	series = {{ICME}'13},
	shorttitle = {{AudioCycle}},
	abstract = {Recently, various dimensionality reduction approaches have been proposed as alternatives to {PCA} or {LDA}. These improved approaches do not rely on a linearity assumption, and are hence capable of discovering more complex embeddings within different regions of the data sets. Despite their success on artificial datasets, it is not straightforward to predict which technique is the most appropriate for a given real dataset. In this paper, we empirically evaluate recent techniques on two real audio use cases: musical instrument loops used in music production and sound effects used in sound editing. {ISOMAP} and t-{SNE} are being compared to {PCA} in a visualization problem, where we end up with a two-dimensional view. Various evaluation measures are used: classification performance, as well as trustworthiness/continuity assessing the preservation of neighborhoods. Although {PCA} and {ISOMAP} can yield good continuity performance even locally (samples in the original space remain close-by in the low-dimensional one), they fail to preserve the structure of the data well enough to ensure that distinct subgroups remain separate in the visualization. We show that t-{SNE} presents the best performance, and can even be beneficial as a pre-processing stage for improving classification when the amount of labeled data is low.},
	booktitle = {14th International Conference on Multimedia and Expo},
	publisher = {{IEEE}},
	author = {Dupont, Stéphane and Ravet, Thierry and Picard-Limpens, Cécile and Frisson, Christian},
	date = {2013-07-15},
	year = {2013},
	note = {{ZSCC}: 0000009},
	keywords = {music, multimedia computing, Measurement, Instruments, multimedia information retrieval, Standards, Music, musical instruments, Databases, sound effects, information retrieval, audio signal processing, artificial datasets, audio and music analysis, classification performance, complex embeddings, continuity performance, Dimensionality reduction, {ISOMAP}, {LDA}, linearity assumption, Manifolds, music production, musical instrument loops, nonlinear dimensionality reduction, {PCA}, principal component analysis, Principal component analysis, real audio use cases, sound editing, t-{SNE}, textural sounds, visualization problem}
}


@inproceedings{AudioGardenAM2010,
	location = {Pitea, Sweden},
	title = {Towards User-Friendly Audio Creation},
	isbn = {978-1-4503-0046-9},
	doi = {10.1145/1859799.1859820},
	series = {{AM}'10},
	shorttitle = {{AudioGarden}},
	abstract = {This paper presents a new approach to sound composition for soundtrack composers and sound designers. We propose a tool for usable sound manipulation and composition that targets sound variety and expressive rendering of the composition. We first automatically segment audio recordings into atomic grains which are displayed on our navigation tool according to signal properties. To perform the synthesis, the user selects one recording as model for rhythmic pattern and timbre evolution, and a set of audio grains. Our synthesis system then processes the chosen sound material to create new sound sequences based on onset detection on the recording model and similarity measurements between the model and the selected grains. With our method, we can create a large variety of sound events such as those encountered in virtual environments or other training simulations, but also sound sequences that can be integrated in a music composition. We present a usability-minded interface that allows to manipulate and tune sound sequences in an appropriate way for sound design.},
	booktitle = {5th {AudioMostly} Conference on Interaction with Sound},
	publisher = {{ACM}},
	author = {Picard, Cécile and Frisson, Christian and Vanderdonckt, Jean and Tardieu, Damien and Dutoit, Thierry},
	date = {2010-09-15},
	year = {2010},
	note = {{ZSCC}: 0000002},
	keywords = {audio analysis \& synthesis, content-based audio similarity, interactive sound composing, multi-fidelity prototyping, Audio Analysis and Synthesis, Content-based Audio Similarity, Interactive Sound Composing, Multi-fidelity Prototyping}
}

@article{SofaModalJASP2010,
	title = {Advances in Modal Analysis Using a Robust and Multiscale Method},
	volume = {2010},
	issn = {1110-8657},
	doi = {10.1155/2010/392782},
	series = {{JASP}'10},
	shorttitle = {{SofaModal}},
	abstract = {This paper presents a new approach to modal synthesis for rendering sounds of virtual objects. We propose a generic method that preserves sound variety across the surface of an object at different scales of resolution and for a variety of complex geometries. The technique performs automatic voxelization of a surface model and automatic tuning of the parameters of hexahedral finite elements, based on the distribution of material in each cell. The voxelization is performed using a sparse regular grid embedding of the object, which permits the construction of plausible lower resolution approximations of the modal model. We can compute the audible impulse response of a variety of objects. Our solution is robust and can handle nonmanifold geometries that include both volumetric and surface parts. We present a system which allows us to manipulate and tune sounding objects in an appropriate way for games, training simulations, and other interactive virtual environments.},
	journaltitle = {{EURASIP} J. Adv. Signal Process},
	author = {Picard, Cécile and Frisson, Christian and Faure, François and Drettakis, George and Kry, Paul G.},
	date = {2010-02},
	year = {2010},
	note = {{ZSCC}: 0000007},
	keywords = {audio synthesis, finite element modeling, Modal analysis, modal synthesis}
}

@artwork{MashtaCycle,
	title = {{MashtaCycle}},
	shorttitle = {{MashtaCycle}},
	author = {Keyaerts, Gauthier},
	editora = {Frisson, Christian},
	editoratype = {collaborator},
	date = {2013},
	year = {2013}
}

@artwork{MetaCrane,
	title = {Méta-Crâne},
	url = {http://www.thomasisrael.be/pf/meta-crane/},
	shorttitle = {{MetaCrane}},
	author = {Israel, Thomas},
	editora = {Frisson, Christian},
	editoratype = {collaborator},
	date = {2009},
	year = {2009}
}

@artwork{BioDiva,
	title = {{BioDiva}},
	shorttitle = {{BioDiva}},
	author = {Moletta, Laurence},
	editora = {Frisson, Christian},
	editoratype = {collaborator},
	date = {2009},
	year = {2009}
}

@artwork{LoopJam,
	title = {{LoopJam}},
	shorttitle = {{LoopJam}},
	author = {Frisson, Christian},
	date = {2012},
	year = {2012}
}

@artwork{TheListeningRoom,
	title = {The Listening Room},
	shorttitle = {{TheListeningRoom}},
	author = {Frisson, Christian},
	date = {2012},
	year = {2012}
}

@software{InspectorWidget,
	title = {{InspectorWidget}: an opensource suite to track and analyze users behaviors in their applications},
	url = {https://github.com/InspectorWidget},
	shorttitle = {{InspectorWidget}},
	author = {Frisson, Christian and Malacria, Sylvain},
	date = {2016},
	year = {2016},
	note = {License: {GPL}}
}

@unpublished{MechaMediaDuinoFFM2016,
	location = {Montreal, {QC}, Canada},
	title = {Force-Feedback (Rotary) Audio Browsing},
	url = {http://www.cirmmt.org/activities/workshops/research/ffedback_music},
	shorttitle = {{MechaMediaDuinoFFM}2016},
	abstract = {A subset of not so new interfaces for musical expression have been traditionally employed in an artistic and scientific field related to and generative of computer music: physical/tangible controls for media browsing. Cyclic representations of time might have been the motivation for the use of rotary control for temporal media (audio and video). Rotary controls have been widely used by experts in audio edition and video montage even before their systems were computerized, with passive proprioceptive and kinesthetic feedback (on hands) limited by the physical controls during their design and fabrication. Why are there no cost-effective commercial devices for force-feedback rotary control widely available now for digital systems, with user-definable mappings, besides the upcoming Microsoft Surface Dial? Can we just make one from off-the-shelf and repurposed components? This talk will start with a short overview of past personal projects on tangible-to-force-feedback media browsing. 
The core of the talk is to provide a log reporting hands-on attempts in replicating interaction techniques for force-feedback audio browsing from seminal papers, towards a "hello world" tutorial, using a recent low-cost opensource and openhardware servo motor project ({MechaDuino}) and a fork of a visual programming environment dedicated for audio/control dataflow ({PurrData} out of {PureData}) that had already been used for prototyping force-feedback and music applications.},
	type = {Invited Talk},
	howpublished = {Invited Talk},
	eventtitle = {Force-Feedback \& Music Symposium},
	note = {Force-Feedback \& Music Symposium},
	author = {Frisson, Christian},
	date = {2016},
	year = {2016},
	keywords = {media browsing, tangible user interfaces, haptics, audio browsing, Force-feedback user interfaces, jog wheel, rotary}
}

@inproceedings{SonixCycleAM2016,
	title = {A Semantic and Content-Based Search User Interface for Browsing Large Collections of Foley Sounds},
	isbn = {978-1-4503-4822-5},
	doi = {10.1145/2986416.2986436},
	series = {{AM}'16},
	shorttitle = {{SonixCycle}},
	abstract = {Sound designers select the sounds they use among massive collections of recordings. They usually rely on text-based queries to narrow down a subset from these collections when looking for specific content. However, when it comes to unknown collections, this approach can fail to precisely retrieve files according to their content. We investigate an audio search engine that associates content-based features and semantic meta-data using Apache Solr deployed in a fully integrated server architecture. In order to facilitate the task of browsing the sounds, we also propose a search user interface in which the user can perform both text-based queries and visual browsing in a window where sounds are organized according to their audio features. A preliminary evaluation of the performances helped to optimize the parameters of the system.},
	booktitle = {11th {AudioMostly} Conference on Interaction with Sound},
	publisher = {{ACM}},
	author = {Urbain, Gabriel and Frisson, Christian and Moinet, Alexis and Dutoit, Thierry},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: 0000001},
	keywords = {information visualization, multimedia information retrieval, Search user interfaces, Sound effects}
}

@inproceedings{WebAudioHapticsWAC2016,
	location = {Atlanta, Georgia, {USA}},
	title = {{WebAudioHaptics}: Tutorial on Haptics with Web Audio},
	url = {https://WebAudioHaptics.github.io},
	series = {{WAC}'16},
	shorttitle = {{WebAudioHaptics}},
	abstract = {The Web Audio Haptics {WAC} Tutorial 2016 will explore how to create meaningful haptic content that engages different areas of the body using off-the-shelf hardware and open source software running on a web browser using Web Audio technologies. Participants will 1) learn the basic theories of tactile illusions; 2) get an overview on actuators and sensors; 3) explore tactile illusions using web-based audio tools and a box connecting actuators and sensors to their computer audio I/O; and 4) ideate use cases in groups. Tutorial material will remain available from: https://github.com/{WebAudioHaptics}},
	booktitle = {2nd Web Audio Conference},
	author = {Frisson, Christian and Pietrzak, Thomas and Zhao, Siyan and Israr, Ali},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: 0000003},
	keywords = {Haptics, vibrotactile, toolkit, audio signal processing, {PureData}, Web Audio}
}

@inproceedings{AudioMetroISMIR2014,
	location = {Taipei, Taiwan},
	title = {A proximity grid optimization method to improve audio search for sound design},
	doi = {10.5281/zenodo.1417245},
	series = {{ISMIR}'14},
	shorttitle = {{AudioMetro}},
	abstract = {Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques.},
	booktitle = {15th International Symposium on Music Information Retrieval},
	author = {Frisson, Christian and Dupont, Stéphane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
	date = {2014-10-27},
	year = {2014},
	note = {{ZSCC}: 0000001},
	keywords = {content-based similarity, known-item search, music information retrieval, sound effects, visual variables, dimension reduction, Media browsers, Student-t Stochastic Neighbor Embedding, {tSNE}}
}

@inproceedings{BaltazarsMMM2014Mediadrom,
	location = {Dublin, Ireland},
	title = {An interactive device for exploring thematically sorted art-works},
	doi = {10.1007/978-3-319-04117-9_4},
	series = {{MMM}'14 Mediadrom},
	shorttitle = {Baltazars},
	abstract = {This Mediadrom artful post-{TV} scenario consists in sketching the user interface of an interactive media content browsing system for exploring thematically sorted artworks, from the art field of plastic theater, merging art pieces at the intersection of the visual and the performing arts. Combining a touchscreen and an hypermedia browser of image and video content with expert annotations, this system can be installed in venues such as museum and media libraries, and performance spaces as satellite installation to plastic theater performances.},
	booktitle = {20th International Conference on {MultiMedia} Modeling, Mediadrom special session},
	publisher = {Springer},
	author = {Baltazar, Aurélie and Baltazar, Pascal and Frisson, Christian},
	date = {2014-01-07},
	year = {2014},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {hypermedia browser, interactive installation, multimedia annotation, plastic theater}
}

@article{VideoBrowserShowdownIJMIR2014,
	title = {The Video Browser Showdown: a live evaluation of interactive video search tools},
	volume = {3},
	doi = {10.1007/s13735-013-0050-8},
	series = {{IJMIR}'14},
	shorttitle = {{VideoBrowserShowdown}},
	abstract = {The Video Browser Showdown evaluates the performance of exploratory video search tools on a common data set in a common environment and in presence of the audience. The main goal of this competition is to enable researchers in the field of interactive video search to directly compare their tools at work. In this paper, we present results from the second Video Browser Showdown ({VBS}2013) and describe and evaluate the tools of all participating teams in detail. The evaluation results give insights on how exploratory video search tools are used and how they perform in direct comparison. Moreover, we compare the achieved performance to results from another user study where 16 participants employed a standard video player to complete the same tasks as performed in {VBS}2013. This comparison shows that the sophisticated tools enable better performance in general, but for some tasks common video players provide similar performance and could even outperform the expert tools. Our results highlight the need for further improvement of professional tools for interactive search in videos.},
	pages = {1--15},
	number = {2},
	journaltitle = {International Journal of Multimedia Information Retrieval},
	author = {Schoeffmann, Klaus and Ahlström, David and Bailer, Werner and Cobârzan, Claudiu and Hopfgartner, Frank and {McGuinness}, Kevin and Gurrin, Cathal and Frisson, Christian and Le, Duy-Dinh and Fabro, Manfred and Bai, Hongliang and Weiss, Wolfgang},
	date = {2014},
	year = {2014},
	note = {{ZSCC}: {NoCitationData}[s0] 
Publisher: Springer},
	keywords = {Exploratory search, Video browsing, Video retrieval, Video search}
}

@unpublished{LavaAMPUIST2013SIC,
	location = {St. Andrews, Scotland, {UK}},
	title = {{LavaAMP}: surrounding the beats in music tracks by streaming colorful blobs},
	url = {https://uist.acm.org/uist2013/contest.php},
	shorttitle = {{LavaAMPUIST}2013SIC},
	abstract = {{LavaAMP} surrounds the beats in music tracks by streaming colorful blobs. {LavaAMP} is inspired from vacuum tube amps and lava lamps. Each onset detected in music tracks would trigger an pump impulse in one bottle with cyclic permuting order, creating a flow of colored blobs.},
	type = {Student Innovation Contest},
	howpublished = {Student Innovation Contest},
	eventtitle = {26th Symposium on User Interface Software and Technology, Student Innovation Contest},
	note = {26th Symposium on User Interface Software and Technology, Student Innovation Contest},
	author = {Frisson, Christian and Schayes, Eric},
	date = {2013-10-08},
	year = {2013},
	keywords = {music information retrieval, {PureData}, beat tracking, fountain development kit, liquid display, Media browser, Microsoft Pumpspark}
}

@inproceedings{MashtaCycleINTETAIN2013,
	location = {Mons, Belgium},
	title = {{MashtaCycle}: on-stage improvised audio collage by content-based similarity and gesture recognition},
	doi = {10.1007/978-3-319-03892-6_14},
	series = {{INTETAIN}'13},
	shorttitle = {{MashtaCycle}},
	abstract = {In this paper we present the outline of a performance in-progress. It brings together the skilled musical practices from Belgian audio collagist Gauthier Keyaerts aka Very Mash'ta; and the realtime, content-based audio browsing capabilities of the {AudioCycle} and {LoopJam} applications developed by the remaining authors. The tool derived from {AudioCycle} named {MashtaCycle} aids the preparation of collections of stem audio loops before performances by extracting content-based features (for instance timbre) used for the positioning of these sounds on a 2D visual map. The tool becomes an embodied on-stage instrument, based on a user interface which uses a depth-sensing camera, and augmented with the public projection of the 2D map. The camera tracks the position of the artist within the sensing area to trigger sounds similarly to the {LoopJam} installation. It also senses gestures from the performer interpreted with the Full Body Interaction ({FUBI}) framework, allowing to apply sound effects based on bodily movements. {MashtaCycle} blurs the boundary between performance and preparation, navigation and improvisation, installations and concerts.},
	booktitle = {Proceedings of the 5th International Conference on Intelligent Technologies for Interactive Entertainment},
	publisher = {Springer},
	author = {Frisson, Christian and Keyaerts, Gauthier and Grisard, Fabien and Dupont, Stéphane and Ravet, Thierry and Zajéga, François and Guerra, Laura Colmenares and Todoroff, Todor and Dutoit, Thierry},
	date = {2013-07-03},
	year = {2013},
	note = {{ZSCC}: 0000002},
	keywords = {gesture recognition, content-based similarity, audio collage, depth cameras, digital audio effects, Human-music interaction}
}

@inproceedings{VideoCycleMMM2013VBS,
	location = {Huangshan, China},
	title = {{VideoCycle}: User-Friendly Navigation by Similarity in Video Databases},
	doi = {10.1007/978-3-642-35728-2_66},
	series = {{MMM}'13 {VBS}},
	shorttitle = {{VideoCycle}},
	abstract = {{VideoCycle} is a candidate application for this second Video Browser Showdown challenge. {VideoCycle} allows interactive intra-video and inter-shot navigation with dedicated gestural controllers. {MediaCycle}, the framework it is built upon, provides media organization by similarity, with a modular architecture enabling most of its workflow to be performed by plugins: feature extraction, clustering, segmentation, summarization, intra-media and inter-segment visualization. {MediaCycle} focuses on user experience with user interfaces that can be tailored to specific use cases.},
	pages = {550--553},
	booktitle = {19th International Conference on {MultiMedia} Modeling, Video Browser Showdown special session},
	publisher = {Springer},
	author = {Frisson, Christian and Dupont, Stéphane and Moinet, Alexis and Picard, Cécile and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
	date = {2013-01-07},
	year = {2013},
	note = {{ZSCC}: 0000009},
	keywords = {timeline, feature extraction, multimedia information retrieval, clustering, known-item search, jog wheel, Media browsers}
}

@inproceedings{LoopJamNIME2012,
	location = {Ann Arbor, Michigan},
	title = {{LoopJam}: turning the dance floor into a collaborative instrumental map},
	doi = {10.5281/zenodo.1178255},
	series = {{NIME}'12},
	shorttitle = {{LoopJam}},
	abstract = {This paper presents the {LoopJam} installation which allows participants to interact with a sound map using a 3D computer vision tracking system. The sound map results from similarity-based clustering of sounds. The playback of these sounds is controlled by the positions or gestures of participants tracked with a Kinect depth-sensing camera. The beat-inclined bodily movements of participants in the installation are mapped to the tempo of played sounds, while the playback speed is synchronized by default among all sounds. We presented and tested an early version of the installation to three exhibitions in Belgium, Italy and France. The reactions among participants ranged between curiosity and amusement.},
	booktitle = {12th Conference on New Interfaces for Musical Expression},
	author = {Frisson, Christian and Dupont, Stéphane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
	editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
	date = {2012-05-21},
	year = {2012},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {audio similarity, depth sensors, Interactive music systems and retrieval, user interaction and interfaces}
}

@inproceedings{LoopJamJIM2012,
	location = {Mons, Belgium},
	title = {{LoopJam}: une carte musicale collaborative sur la piste de danse},
	url = {http://jim.afim-asso.org/jim12/pdf/jim2012_17_p_frisson.pdf},
	series = {{JIM}'12},
	shorttitle = {{LoopJam}},
	abstract = {Ce papier présente l'installation {LoopJam} qui permet aux visiteurs d'interagir avec une carte musicale par le biais d'un système de suivi gestuel par vision informatique. La carte sonore résulte d'un partitionnement des sons en groupes par similarité basée sur leur signal. Le rendu sonore est contrôlé par les positions ou gestes des participants captés par une caméra Kinect détectant la profondeur de la scène 3D. Les mouvements des participants exprimant une mesure ou un tempo sont corrélés à la vitesse de lecture commune à tous les échantillons synchronisés par le moteur audio. Nous avons présenté et testé une première version de cette installation lors de trois expositions en Belgique, Italie et France. Les réactions parmi les participants ont varié entre la curiosité et l'amusement.},
	pages = {101--105},
	booktitle = {19èmes Journées d'Informatique Musicale},
	author = {Frisson, Christian and Dupont, Stéphane and Moinet, Alexis and Leroy, Julien and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
	date = {2012},
	year = {2012},
	note = {{ZSCC}: 0000000},
	keywords = {audio similarity, depth sensors, Interactive music systems and retrieval, user interaction and interfaces}
}

@inproceedings{MediaCycleISEA2011,
	location = {Istanbul, Turkey},
	title = {Similarity in media content: digital art perspectives},
	url = {https://isea2011.sabanciuniv.edu/paper/similarity-media-content-digital-art-perspectives},
	series = {{ISEA}'11},
	shorttitle = {{MediaCycle}},
	abstract = {This essay examines how media content navigation by similarity can foster new practices in digital arts, blurring the boundaries between composing/performing, curating/authoring, creating/interpreting. With {MediaCycle}, a framework for browsing media databases by similarity, we created several prototypes: a website for browsing dancers' identities through video recordings, a collaborative dancefloor for music creation.},
	booktitle = {17th Symposium on Electronic Art},
	author = {Frisson, Christian and Dupont, Stéphane and Siebert, Xavier and Dutoit, Thierry},
	date = {2011-09-14},
	year = {2011},
	note = {{ZSCC}: 0000006},
	keywords = {content-based similarity, Media browsers, digital art, media content, new media arts, similarity}
}

@inproceedings{AudioCycleAESConv2010,
	location = {London, {UK}},
	title = {Browsing Sound and Music Libraries by Similarity},
	series = {{AES} Conv'10},
	shorttitle = {{AudioCycle}},
	abstract = {This paper presents a prototype tool for browsing through multimedia libraries using content-based multimedia information retrieval techniques. It is composed of several groups of components for multimedia analysis, data mining, interactive visualization, as well as connection with external hardware controllers. The musical application of this tool, uses descriptors of timbre, harmony, as well as rhythm and two different approaches for exploring/browsing content. First, a dynamic data mining allows the user to group sounds into clusters according to those different criteria, whose importance can be weighted interactively. In a second mode, sounds that are similar to a query are returned to the user, and can be used to further proceed with the search. The browsing steps are then stored and visualized as a tree. This approach also borrows from multi-criteria optimization concept to return a relevant list of similar sounds.},
	booktitle = {128th Audio Engineering Society Convention},
	publisher = {{AES}},
	author = {Dupont, Stéphane and Frisson, Christian and Siebert, Xavier and Tardieu, Damien},
	date = {2010-05-22},
	year = {2010},
	note = {{ZSCC}: 0000015},
	keywords = {information visualization, content-based similarity, music information retrieval, Multimedia information retrieval}
}

@inproceedings{DotConIHM2010,
	location = {Luxembourg},
	title = {Conception centrée utilisateur de prototypes interactifs pour la gestion de contenu multimedia par similarité},
	url = {http://ihm2010.afihm.org/programme/rencontres-doctorales.html},
	series = {{IHM}'10},
	shorttitle = {{DotCon}},
	abstract = {Cet article présente les travaux en cours d’une recherche doctorale visant à proposer une méthodologie de conception centrée utilisateur et de prototypage rapide afin de concevoir des applications interactives destinées à la navigation par similarité dans des bases de données multimedia, adaptées à des cas d’utilisation divers et profils d’utilisateurs variés. Les modalités d’interaction sont volontairement restreintes à la visualisation d’information et l’interaction manuelle. Une méthode de développement rapide, réutilisable et durable est proposée, exemplifiée par quelques prototypes à évaluer par des tests utilisateur.},
	booktitle = {22ème Conférence Francophone sur l'Interaction Homme-Machine},
	publisher = {{ACM}},
	author = {Frisson, Christian},
	date = {2010-09-20},
	year = {2010},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {interaction gestuelle, navigation hypermedia, Prototypage rapide, visualisation d’information}
}

@inproceedings{DeviceCycleNIME2010,
	location = {Sydney, {NSW}, Australia},
	title = {{DeviceCycle}: rapid and reusable prototyping of gestural interfaces, applied to audio browsing by similarity},
	isbn = {978-0-646-53482-4},
	doi = {10.5281/zenodo.1177771},
	series = {{NIME}'10},
	shorttitle = {{DeviceCycle}},
	abstract = {This paper presents the development of rapid and reusable gestural interface prototypes for navigation by similarity in an audio database and for sound manipulation, using the {AudioCycle} application. For this purpose, we propose and follow guidelines for rapid prototyping that we apply using the {PureData} visual programming environment. We have mainly developed three prototypes of manual control: one combining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits and shortcomings we experienced while prototyping using this approach.},
	booktitle = {10th Conference on New Interfaces for Musical Expression},
	author = {Frisson, Christian and Dupont, Stéphane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry and Macq, Benoit},
	date = {2010-06-15},
	year = {2010},
	note = {{ZSCC}: 0000016},
	keywords = {Human-computer interaction, rapid prototyping, audio database, browsing by similarity, gestural interfaces}
}

@inproceedings{MultimodalGuitarNIME2010,
	location = {Sydney, Australia},
	title = {{MultimodalGuitar}: a Toolbox for Augmented Guitar Performances},
	isbn = {978-0-646-53482-4},
	doi = {10.5281/zenodo.1177881},
	series = {{NIME}'10},
	shorttitle = {{MultimodalGuitar}},
	abstract = {This project aims at studying how recent interactive and interactions technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed and developed a freely-available toolbox for multimodal guitar performances, compliant with the {PureData} and Max/{MSP} modular environments for a more widespread use, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing, modal synthesis, infinite sustain, rearranged looping and “smart” harmonizing.},
	booktitle = {10th Conference on New Interfaces for Musical Expression},
	author = {Reboursière, Loïc and Frisson, Christian and Lähdeoja, Otso and Mills {III}, John Anderson and Picard, Cécile and Todoroff, Todor},
	date = {2010-06-15},
	year = {2010},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {multimodal interaction, audio synthesis, digital audio effects, Augmented guitar, gestural sensing, hexaphonic guitar, polyphonic transcription}
}

@inproceedings{AudioCycleCBMI2009,
	title = {{AudioCycle}: Browsing Musical Loop Libraries},
	doi = {10.1109/CBMI.2009.19},
	series = {{CBMI}'09},
	shorttitle = {{AudioCycle}},
	abstract = {This paper presents {AudioCycle}, a prototype application for browsing through music loop libraries. {AudioCycle} provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation, and listen to individual audio extracts, searching for those of interest. {AudioCycle} draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Concepts developped here will be of interest to {DJs}, remixers, musicians, soundtrack composers, but also sound designers and foley artists. Possible extension to multimedia libraries are also suggested.},
	pages = {73--80},
	booktitle = {7th Workshop on Content-Based Multimedia Indexing},
	publisher = {{IEEE}},
	author = {Dupont, Stéphane and Dubuisson, Thomas and Urbain, Jérôme and Sebbe, Raphaël and d'Alessandro, Nicolas and Frisson, Christian},
	date = {2009-06-03},
	year = {2009},
	note = {{ZSCC}: 0000012},
	keywords = {multimedia information retrieval, audio feature extraction, Media content}
}

@inproceedings{AudioCycleICME2009,
	title = {{AudioCycle}: A similarity-based visualization of musical libraries},
	doi = {10.1109/ICME.2009.5202887},
	series = {{ICME}'09},
	shorttitle = {{AudioCycle}},
	abstract = {This paper presents {AudioCycle}, a prototype application for browsing through music loop libraries. {AudioCycle} provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation and listen to individual audio extracts. {AudioCycle} draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Possible extension to multimedia libraries are also suggested.},
	pages = {1847--1848},
	booktitle = {10th Conference on Multimedia and Expo},
	publisher = {{IEEE}},
	author = {Urbain, Jérôme and Dubuisson, Thomas and Dupont, Stéphane and Frisson, Christian and Sebbe, Raphaël and d'Alessandro, Nicolas},
	date = {2009-07-28},
	year = {2009},
	note = {{ZSCC}: 0000005},
	keywords = {visualization, browsing, 3D rendering, music similarities, Browsing}
}

@inproceedings{CoMediAnnotateeNTERFACE2010,
	location = {Amsterdam, Netherlands},
	title = {{CoMediAnnotate}: towards more usable multimedia content annotation by adapting the user interface},
	series = {{eNTERFACE}'10},
	shorttitle = {{CoMediAnnotate}},
	abstract = {This project aims at improving the user experience regarding multimedia content annotation. We evaluated and compared current timeline-based annotation tools, so as to elicit user requirements. We address two issues: 1) adapting the user interface, by supporting more input modalities through a rapid prototyping tool and by offering alternative visualization techniques of temporal signals; and 2) covering more steps of the annotation workflow besides the task of annotation itself: notably recording multimodal signals.
We developed input devices components for the {OpenInterface} ({OI}) platform for rapid prototyping of multimodal interfaces: multitouch screen, jog wheels and pen-based solutions. We modified an annotation tool created with the Smart Sensor Integration ({SSI}) toolkit and componentized it in {OI} so as to bind its controls to different input devices. We produced mockups sketches towards a new design of an improved user interface for multimedia content annotation, and started developing a rough prototype using the Processing Development Environment.
Our solution allows to produce several prototypes by varying the interaction pipeline: changing input modalities and using either the initial {GUI} of the annotation tool, or the newly-designed one. We target usability testing to validate our solution and determine which input modalities combination best suits given use cases.},
	booktitle = {6th Summer School on Multimodal Interfaces},
	author = {Frisson, Christian and Alaçam, Sema and Coşkun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
	date = {2010-08-12},
	year = {2010},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {information visualization, rapid prototyping, gestural interaction, Multimodal annotation}
}

@inproceedings{MultimodalGuitareNTERFACE2009,
	location = {Genova, Italy},
	title = {Multimodal Guitar: Performance Toolbox and Study Workbench},
	series = {{eNTERFACE}'09},
	shorttitle = {Multimodal Guitar},
	abstract = {This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. We investigate two axes, 1) “A gestural/polyphonic sensing/processing toolbox to augment guitar performances”, and 2) “An interactive guitar score following environment for adaptive learning”. These approaches share quite similar technological challenges (sensing, analysis, processing, synthesis and interaction methods) and dissemination intentions (community-based, low-cost, open-source whenever possible), while leading to different applications (respectively artistic and educational), still targeted towards experienced players and beginners.},
	booktitle = {5th Summer School on Multimodal Interfaces},
	author = {Frisson, Christian and Reboursière, Loïc and Chu, Wen-Yang and Lähdeoja, Otso and Mills {III}, John Anderson and Picard, Cécile and Shen, Ao and Todoroff, Todor},
	date = {2009-08-13},
	year = {2009},
	note = {{ZSCC}: 0000010},
	keywords = {multimodal fusion, audio synthesis, digital audio effects, Audio- and polyphonic multi-pitch transcription, finger tracking, guitar score following, multimodal interaction and gestural sensing, particle filtering}
}

@inproceedings{PhysioContenteNTERFACE2007,
	location = {Istanbul, Turkey},
	title = {Audiovisual Content Generation Controlled by Physiological Signals for Clinical and Artistic Applications},
	url = {http://www.cmpe.boun.edu.tr/enterface07/outputs/final/p8report.pdf},
	series = {{eNTERFACE}'07},
	shorttitle = {{PhysioContent}},
	abstract = {While an extensive palette of sound and visual generation techniques have been developed during the era of digital signal processing, the design of innovative virtual instruments has come to dramatic fruition over the last decade. The use of measured biological signals to drive these instruments proposes some new and powerful tools for clinical, scientific and artistic applications. Over the period of one month - during the {eNTERFACE}’07 summer workshop in  ̇Istanbul, Turkey - researchers from the fields of human-computer interfaces, sound synthesis and new media art worked together towards this common goal.
A framework for auditory display and bio-musical applications was established upon which were based different experimental prototypes. Diverse methods for the analysis of measured physiological signals and of mapping the extracted parameters to sound and visual synthesis processes were explored. Biologically-driven musical instruments and data displays for clinical and medical purposes were built. From this have emerged some worthwhile perspectives on future research. This report summarises the results of that project.},
	pages = {103--116},
	booktitle = {3rd Summer School on Multimodal Interfaces},
	author = {Benovoy, Mitchel and Brouse, Andrew and Corcoran, Thomas Greg and Drayson, Hannah and Erkut, Cumhur and Filatriau, Jean-Julien and Frisson, Christian and Gundogdu, Umut and Knapp, Ben and Lehembre, Rémy and Mühl, Christian and Pérez, Miguel Angel Ortiz and Sayin, Alaattin and Soleymani, Mohammad and Tahiroğlu, Koray},
	date = {2007-08-16},
	year = {2007},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {Auditory display, Biologically-augmented performances, Biosignals, Brain-computer interfaces, Interactive arts, Multimodal interfaces, Sonification}
}

@unpublished{HelmholtzJFIS2005,
	location = {Le Mans, France},
	title = {Helmoltz: un outil de caractérisation des deux premiers modes de la guitare},
	url = {http://www.itemm.fr/jfis/},
	shorttitle = {{HelmholtzJFIS}2005},
	abstract = {Ce projet porte sur l’étude vibro-acoustique de la guitare. L’objectif est de fournir au luthier une chaîne de mesure simplifiée permettant d’extraire les paramètres d’un modèle à deux degrés de liberté de la guitare. Cet appui graphique lui permettra d’associer les caractéristiques physiques d’une guitare (qualité du bois, masse volumique des composants, raideur de la table d’harmonie) avec une courbe de réponse en fréquence.
Nous devons ainsi mettre à disposition au luthier un banc de mesure simple, rapide à l’utilisation et peu coûteux lui permettant de mesurer la réponse en fréquence de caisses de résonance et d'obtenir des paramètres objectifs.},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {Journées Facture Instrumentale \& Sciences: “Mettre en Commun”},
	note = {Journées Facture Instrumentale \& Sciences: “Mettre en Commun”},
	author = {Gautier, François and Le Carrou, Jean-Loïc and Collin, Erwan and Dufaud, Jérémie and Frisson, Christian},
	date = {2005-07},
	year = {2005},
	keywords = {modal analysis, guitar, luthier, Vibroacoustics}
}

@report{PrintgetsHAPPINESS2017,
	title = {Guidelines for Next Generation Haptic Demonstrator},
	url = {https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b760277f&appId=PPGMS},
	shorttitle = {{PrintgetsHAPPINESS}2017},
	number = {D.17},
	institution = {{HAPPINESS} {EU} H2020 645145},
	author = {Frisson, Christian and Decaudin, Julien and Lopez, Mario Sanz and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Latour, Antoine},
	date = {2017},
	year = {2017},
	note = {{ZSCC}: {NoCitationData}[s0]}
}

@report{VibroTactileWidgetsHAPPINESS2017,
	title = {Interaction techniques leveraging haptic feedback on new interactive surfaces (final)},
	shorttitle = {{VibroTactileWidgetsHAPPINESS}2017},
	number = {D2.4},
	institution = {{HAPPINESS} {EU} H2020 645145},
	author = {Frisson, Christian and Decaudin, Julien and Lopez, Mario Sanz and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Gaffary, Yoren and Lecuyer, Anatole and Latour, Antoine},
	date = {2017},
	year = {2017},
	note = {{ZSCC}: {NoCitationData}[s0]}
}

@report{AudioSkimmingNUMEDIART2008,
	title = {Audio Skimming},
	shorttitle = {{AudioSkimmingNUMEDIART}2008},
	pages = {1--16},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Couvreur, Laurent and Bettens, Frédéric and Drugman, Thomas and Frisson, Christian and Jottrand, Matthieu and Mancas, Matei and Moinet, Alexis},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2008},
	year = {2008},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 1}
}

@report{AudioThumbnailingNUMEDIART2008,
	title = {Audio Thumbnailing},
	shorttitle = {{AudioThumbnailingNUMEDIART}2008},
	pages = {67--85},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Couvreur, Laurent and Bettens, Frédéric and Drugman, Thomas and Dubuisson, Thomas and Dupont, Stéphane and Frisson, Christian and Jottrand, Matthieu and Mancas, Matei},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2008},
	year = {2008},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@report{AudioCycleNUMEDIART2008,
	title = {Audio Cycle},
	shorttitle = {{AudioCycleNUMEDIART}2008},
	pages = {119--127},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Dupont, Stéphane and d'Alessandro, Nicolas and Dubuisson, Thomas and Frisson, Christian and Sebbe, Raphaël and Urbain, Jérôme},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2008},
	year = {2008},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 4}
}

@report{BehavioralInstallationsNUMEDIART2009,
	title = {Behavioral Installations: Emergent audiovisual installations influenced by visitors' behaviours},
	shorttitle = {{BehavioralInstallationsNUMEDIART}2009},
	pages = {9--17},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Filatriau, Jean-Julien and Frisson, Christian and Reboursière, Loïc and Siebert, Xavier and Todoroff, Todor},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2009},
	year = {2009},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 1}
}

@report{BodilyBenchmarkNUMEDIART2009,
	title = {Bodily Benchmark: Gestural/Physiological Analysis by Remote/Wearable Sensing},
	shorttitle = {{BodilyBenchmarkNUMEDIART}2009},
	pages = {41--57},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Frisson, Christian and Reboursière, Loïc and Todoroff, Todor and Filatriau, Jean-Julien},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2009},
	year = {2009},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@report{MultimodalGuitarNUMEDIART2009,
	title = {Multimodal Guitar: Performance Toolbox and Study Workbench},
	shorttitle = {{MultimodalGuitarNUMEDIART}2009},
	pages = {67--84},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Frisson, Christian and Reboursière, Loïc and Chu, Wen-Yang and Lähdeoja, Otso and {III}, John Anderson Mills and Picard, Cécile and Shen, Ao and Todoroff, Todor},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2009},
	year = {2009},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 3}
}

@report{MultiMediaCycleNUMEDIART2009,
	title = {{MultiMediaCycle}: Consolidating the {HyForge} Framework towards Improved Scalability and Usability},
	shorttitle = {{MultiMediaCycleNUMEDIART}2009},
	pages = {113--117},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Tardieu, Damien},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2009},
	year = {2009},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@report{MoViNUMEDIART2010,
	title = {{MoVi}: {MediaCycle} Audio and Visualization improvements},
	shorttitle = {{MoViNUMEDIART}2010},
	pages = {5--8},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Tardieu, Damien},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2010},
	year = {2010},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 1}
}

@report{AudioGardenNUMEDIART2010,
	title = {{AudioGarden}: towards a Usable Tool for Composite Audio Creation},
	shorttitle = {{AudioGardenNUMEDIART}2010},
	pages = {33--36},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Frisson, Christian and Picard, Cécile and Tardieu, Damien},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2010},
	year = {2010},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@report{CoMediAnnotateNUMEDIART2010,
	title = {{CoMediAnnotate}: towards more usable multimedia content annotation by adapting the user interface},
	shorttitle = {{CoMediAnnotateNUMEDIART}2010},
	pages = {45--55},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Frisson, Christian and Alaçam, Sema and Coşkun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
	editora = {Dutoit, Thierry and Macq, Benoît},
	editoratype = {collaborator},
	date = {2010},
	year = {2010},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 3}
}

@report{MediaBlenderNUMEDIART2011,
	title = {{MediaBlender} : Interactive Multimedia Segmentation},
	shorttitle = {{MediaBlenderNUMEDIART}2011},
	pages = {1--6},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Dupont, Stéphane and Frisson, Christian and Urbain, Jérôme and Mahmoudi, Sidi and Siebert, Xavier},
	editora = {Dutoit, Thierry},
	editoratype = {collaborator},
	date = {2011},
	year = {2011},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 1}
}

@report{LoopJamNUMEDIART2011,
	title = {{LoopJam}: a collaborative musical map on the dance floor},
	shorttitle = {{LoopJamNUMEDIART}2011},
	pages = {37--40},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Frisson, Christian and Dupont, Stéphane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier},
	editora = {Dutoit, Thierry},
	editoratype = {collaborator},
	date = {2011},
	year = {2011},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@report{RT-MediaCycleNUMEDIART2011,
	title = {{RT}-{MediaCycle}: Towards a real-time use of {MediaCycle} in performances and video installations},
	shorttitle = {{RT}-{MediaCycleNUMEDIART}2011},
	pages = {55--58},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Delcourt, Bernard},
	editora = {Dutoit, Thierry},
	editoratype = {collaborator},
	date = {2011},
	year = {2011},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 3}
}

@report{MedianeumNUMEDIART2012,
	title = {Medianeum: crafting interactive timelines from multimedia content},
	shorttitle = {{MedianeumNUMEDIART}2012},
	pages = {1--7},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Zajéga, François and Picard, Cécile and René, Julie and Puleo, Antonin and Decuypere, Justine and Frisson, Christian and Ravet, Thierry and Mancas, Matei},
	editora = {Dutoit, Thierry},
	editoratype = {collaborator},
	date = {2012},
	year = {2012},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@report{MakamCycleNUMEDIART2012,
	title = {{MakamCycle}: improving the understanding of Turkish Makam Music through the {MediaCycle} Framework},
	shorttitle = {{MakamCycleNUMEDIART}2012},
	pages = {17--20},
	institution = {numediart Research Program on Digital Art Technologies},
	author = {Frisson, Christian and Babacan, Onur and Dutoit, Thierry},
	editora = {Dutoit, Thierry},
	editoratype = {collaborator},
	date = {2012},
	year = {2012},
	note = {{ZSCC}: {NoCitationData}[s0] 
Issue: 2}
}

@unpublished{ComproVisCIRMMT2020,
	location = {Montreal, Qc, Canada},
	title = {Overview of gestural interaction with sound information, towards force-feedback},
	url = {https://frisson.re/ComproVisCIRMMT2020/},
	shorttitle = {{ComproVisCIRMMT}2020},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {{CIRMMT} Workshop: Composing \& improvising with information},
	note = {{CIRMMT} Workshop: Composing \& improvising with information},
	author = {Frisson, Christian},
	date = {2020-02},
	year = {2020}
}

@unpublished{HapticAudioFeedbacksIAM2019,
	location = {Montreal, Qc, Canada},
	title = {An overview on Haptic \& Audio research and applications},
	url = {https://frisson.re/HapticAudioFeedbacksIAM2019},
	shorttitle = {{HapticAudioFeedbacksIAM}2019},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {9th Interactive Audio Montreal edition: Haptics in Interaction and Game Design},
	note = {9th Interactive Audio Montreal edition: Haptics in Interaction and Game Design},
	author = {Frisson, Christian},
	date = {2019-06},
	year = {2019}
}

@unpublished{MediaHapticsICRA2019SHI,
	location = {Montreal, Qc, Canada},
	title = {Towards Opensource Authoring Toolkits for Designing Soft+Stiff Haptic Interactions},
	url = {https://frisson.re/MediaHapticsICRA2019SHI/},
	shorttitle = {{MediaHapticsICRA}2019SHI},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {{IEEE} {ICRA}'19 Soft Haptic Interaction workshop: Modeling, Design and Application},
	note = {{IEEE} {ICRA}'19 Soft Haptic Interaction workshop: Modeling, Design and Application},
	author = {Frisson, Christian and Gallacher, Colin and Wanderley, Marcelo M.},
	date = {2019-05},
	year = {2019}
}

@unpublished{OpenForceFeedbackHAID2019,
	location = {Lille, France},
	title = {Open technologies for force-feedback in artistic creation},
	url = {https://frisson.re/OpenForceFeedbackHAID2019/},
	shorttitle = {{OpenForceFeedbackHAID}2019},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {9th Intl. Workshop on Haptic and Audio Interaction Design},
	note = {9th Intl. Workshop on Haptic and Audio Interaction Design},
	author = {Sinclair, Stephen and Leonard, James and Villeneuve, Jérôme and Frisson, Christian},
	date = {2019-03},
	year = {2019}
}

@unpublished{HapticProxyVIS2018DataPhys,
	location = {Berlin, Germany},
	title = {Haptics as a sustainable proxy for exploring design variables for data physicalization},
	url = {https://frisson.re/Vis18DataPhys},
	shorttitle = {{HapticProxyVIS}2018DataPhys},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {{IEEE} {VIS} 2018 Workshop: Toward a Design Language for Data Physicalization},
	note = {{IEEE} {VIS} 2018 Workshop: Toward a Design Language for Data Physicalization},
	author = {Frisson, Christian and Wanderley, Marcelo M. and Willett, Wesley and Carpendale, Sheelagh},
	date = {2018},
	year = {2018}
}

@inproceedings{AuracleeNTERFACE2014,
	location = {Bilbao, Spain},
	title = {Auracle: how are salient cues situated in audiovisual content?},
	series = {{eNTERFACE}'14},
	shorttitle = {Auracle},
	booktitle = {10th Summer School on Multimodal Interfaces},
	author = {Frisson, Christian and Riche, Nicolas and Coutrot, Antoine and Delestage, Charles-Alexandre and Dupont, Stéphane and Ferhat, Onur and Guyader, Nathalie and Mahmoudi, Sidi Ahmed and Mancas, Matei and Mital, Parag K. and Echániz, Alicia Prieto and Rocca, François and Rochette, Alexis and Yvart, Willy},
	date = {2014-07-08},
	year = {2014},
	note = {{ZSCC}: {NoCitationData}[s0]},
	keywords = {eye tracking, Eye Tracking}
}

@unpublished{VideodromeINTETAIN2013,
	location = {Mons, Belgium},
	title = {Videodrome: a Timeline of Linked Media Art/Science/Technology Events},
	url = {http://archive.intetain.org/2013/show/sponsorship},
	shorttitle = {{VideodromeINTETAIN}2013},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {5th Intl. Conf. on Intelligent Technologies for Interactive Entertainment, {LinkedTV} Demo Session},
	note = {5th Intl. Conf. on Intelligent Technologies for Interactive Entertainment, {LinkedTV} Demo Session},
	author = {Frisson, Christian},
	date = {2013-07-03},
	year = {2013}
}

@unpublished{MediaSurfacesITS2013CmIS,
	location = {St Andrews, Scotland, {UK}},
	title = {Designing artfully-mediated interactive surfaces organizing media collections},
	shorttitle = {{MediaSurfacesITS}2013CmIS},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {{ACM} {ITS}'13 workshop: Collaboration meets Interactive Surfaces},
	note = {{ACM} {ITS}'13 workshop: Collaboration meets Interactive Surfaces},
	author = {Frisson, Christian and Schayes, Eric and Uyttenhove, Simon and {Stéphane Dupont} and Giot, Rudi and Dutoit, Thierry},
	date = {2013-10-06},
	year = {2013}
}

@unpublished{MakamCycleCompMusic2012,
	location = {Istanbul, Turkey},
	title = {Improving the Understanding of Turkish Makam Music through the {MediaCycle} Framework},
	url = {http://compmusic.upf.edu/system/files/static_files/06-Onur-Babacan-et-al-2nd-CompMusic-Workshop-2012_0.pdf},
	shorttitle = {{MakamCycleCompMusic}2012},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {2nd {CompMusic} Workshop},
	note = {2nd {CompMusic} Workshop},
	author = {Babacan, Onur and Frisson, Christian and Dutoit, Thierry},
	date = {2012-07-12},
	year = {2012}
}

@unpublished{MultimodalGuitarJEIGE2009,
	location = {Maison des Sciences de l'Homme Paris-Nord, Saint Denis, France},
	title = {Objectifs du projet Multimodal Guitar},
	shorttitle = {{MultimodalGuitarJEIGE}2009},
	type = {Workshop Presentation},
	howpublished = {Workshop Presentation},
	eventtitle = {Journées d'Étude Identités de la Guitare Électrique},
	note = {Journées d'Étude Identités de la Guitare Électrique},
	author = {Frisson, Christian and Reboursière, Loïc},
	date = {2009-05-18},
	year = {2009}
}

@report{VibroTactileWidgetsHAPPINESS2016,
	title = {Interaction techniques leveraging haptic feedback on new interactive surfaces (intermediate)},
	shorttitle = {{VibroTactileWidgetsHAPPINESS}2016},
	number = {D2.3},
	institution = {{HAPPINESS} {EU} H2020 645145},
	author = {Frisson, Christian and Decaudin, Julien and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Gaffary, Yoren and Lecuyer, Anatole and Latour, Antoine},
	date = {2016},
	year = {2016},
	note = {{ZSCC}: {NoCitationData}[s1]}
}

@thesis{MathiasKirkegaardMAMcGill2020,
	title = {A combined hardware/software framework for designing self contained 1DoF force-feedback interactions for {DMI}'s (in-progress)},
	shorttitle = {{MathiasKirkegaardMAMcGill}2020},
	institution = {{McGill} University, Montreal, Canada},
	type = {{MA}},
	author = {Kirkegaard, Mathias},
	editora = {Wanderley, Marcelo M. and Frisson, Christian},
	editoratype = {collaborator},
	date = {2020},
	year = {2020},
	note = {Role: co-advisor}
}

@thesis{MathiasBredholtMAMcGill2020,
	title = {Live Looping of Gestural Control Data in Musical Performance (in progress)},
	shorttitle = {{MathiasBredholtMAMcGill}2020},
	institution = {{McGill} University, Montreal, Canada},
	type = {{MA}},
	author = {Bredholt, Mathias},
	editora = {Wanderley, Marcelo M. and Meneses, Eduardo and Frisson, Christian},
	editoratype = {collaborator},
	date = {2020},
	year = {2020},
	note = {Role: co-advisor}
}

@thesis{OnurFerhatPhDUAB2017,
	title = {A Gaze Estimation Method and System for Natural Light Cameras.},
	shorttitle = {{OnurFerhatPhDUAB}2017},
	institution = {Universitat Autònoma Barcelona, Spain},
	type = {phdthesis},
	author = {Ferhat, Onur},
	editora = {Vilariño, Fernando and Villanueva, Arantzazu and Karatzas, Dimosthenis and Parraga, Carlos Alejandro and Gomez Bigorda, Luis and Super, Hans},
	editoratype = {collaborator},
	date = {2017},
	year = {2017},
	note = {Role: reader}
}

@thesis{FabienGrisardMScINPG2013,
	title = {Gestural interface for performative composition from collections of audio samples.},
	shorttitle = {{FabienGrisardMScINPG}2013},
	institution = {Institut National Polytechnique de Grenoble, France},
	type = {{MSc}},
	author = {Grisard, Fabien},
	editora = {Dutoit, Thierry and Frisson, Christian},
	editoratype = {collaborator},
	date = {2013},
	year = {2013},
	note = {Role: co-advisor}
}

@thesis{RemyLabbeMScUCLouvain2009,
	title = {Sound spatialisation through multi-camera analysis of gestures},
	shorttitle = {{RemyLabbeMScUCLouvain}2009},
	institution = {Université catholique de Louvain, Belgium},
	type = {{MSc}},
	author = {Labbé, Rémy},
	editora = {Macq, Benoît and Filatriau, Jean-Julien and Frisson, Christian and Van Brussel, Christian},
	editoratype = {collaborator},
	date = {2009},
	year = {2009},
	note = {Role: co-advisor}
}

@thesis{FlorentCouchariereMScUCLouvain2007,
	title = {Analysis of the drummer instrumental gesture through electromyography ({EMG})},
	shorttitle = {{FlorentCouchariereMScUCLouvain}2007},
	institution = {Université catholique de Louvain, Belgium},
	type = {{MSc}},
	author = {Coucharière, Florent},
	editora = {Macq, Benoît and Filatriau, Jean-Julien and Frisson, Christian},
	editoratype = {collaborator},
	date = {2007},
	year = {2007},
	note = {Role: co-advisor}
}

@thesis{ChristianFrissonMScINPG2006,
	location = {{ACROE}, Grenoble, France + {McGill} University, Montreal, Canada},
	title = {Comparaison de techniques de synthèse de type signal additive/source-filtre et physique de type particulaire: modèle de représentation, environnement, type de contrôle et accès},
	shorttitle = {{ChristianFrissonMScINPG}2006},
	institution = {Institut National Polytechnique de Grenoble, France},
	type = {{MSc}},
	author = {Frisson, Christian},
	editora = {Cadoz, Claude and Depalle, Philippe and Wanderley, Marcelo M.},
	editoratype = {collaborator},
	date = {2006},
	year = {2006}
}

@thesis{ChristianFrissonMEngENSIM2005,
	location = {Eowave, Paris, France},
	title = {Développement de l’instrumentarium de l’Eobody},
	shorttitle = {{ChristianFrissonMEngENSIM}2005},
	institution = {Ecole Nationale Supérieure d’Ingénieurs du Mans, France},
	type = {{MEng}},
	author = {Frisson, Christian},
	editora = {Sirguy, Marc and Gaviot, Etienne},
	editoratype = {collaborator},
	date = {2005},
	year = {2005}
}

@software{libhappiness,
	title = {libhappiness: library for driving piezo actuators and sensing capacitive touch},
	url = {https://gitlab.inria.fr/Loki/happiness/libhappiness},
	shorttitle = {libhappiness},
	author = {Decaudin, Julien and Frisson, Christian and Pietrzak, Thomas},
	date = {2017},
	year = {2017},
	note = {License:{LGPL}}
}

@software{CIRMMTSpeakerSeriesVis,
	title = {{CIRMMT} Distinguished Speaker Series Visualizations},
	url = {https://idmil.gitlab.io/CIRMMT_visualizations/},
	shorttitle = {{CIRMMTSpeakerSeriesVis}},
	author = {Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M.},
	date = {2019},
	year = {2019}
}

@software{DataEmpowermentSpeakerSeries,
	title = {Data Empowerment Speaker Series Website},
	url = {http://speakerseries.ilab.cpsc.ucalgary.ca/},
	shorttitle = {{DataEmpowermentSpeakerSeries}},
	author = {Frisson, Christian and West, Mieka and Pusch, Ricky},
	date = {2017},
	year = {2017}
}

@software{RepHap,
	title = {{RepHap}: an open source platform for benchmarking haptic devices leveraging the Robot Operating System ecosystem},
	url = {https://github.com/RepHap},
	shorttitle = {{RepHap}},
	author = {Frisson, Christian},
	date = {2019},
	year = {2019}
}

@software{EnergyVisConditions,
	title = {Conditions on {CER}-regulated energy projects Information Visualization},
	url = {https://apps2.cer-rec.gc.ca/conditions/},
	shorttitle = {{EnergyVisConditions}},
	publisher = {Canada Energy Regulator},
	date = {2017},
	year = {2017}
}

@software{HCI-MTL-Group,
	title = {{HCI} Montreal Group Map},
	url = {https://hcimtl.github.io/group/},
	shorttitle = {{HCI}-{MTL}-Group},
	author = {Frisson, Christian},
	date = {2020},
	year = {2020}
}