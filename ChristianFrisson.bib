@thesis{AlbertNgaboNiyonsengaMAMcGill2023,
  type = {MArts},
  title = {Reliability, {{Maintainability}} and {{Support}} of {{Digital Musical Instruments}}},
  author = {Niyonsenga, Albert-Ngabo},
  editora = {Frisson, Christian and Wanderley, Marcelo M.},
  editoratype = {collaborator},
  date = {2023},
  institution = {{McGill University}},
  location = {{IDMIL}}
}

@inproceedings{AudioCycleAESConv2010,
  title = {Browsing {{Sound}} and {{Music Libraries}} by {{Similarity}}},
  shorttitle = {{{AudioCycle}}},
  booktitle = {128th {{Audio Engineering Society Convention}}},
  author = {Dupont, Stéphane and Frisson, Christian and Siebert, Xavier and Tardieu, Damien},
  date = {2010-05-22/2010-05-25},
  series = {{{AES Conv}}'10},
  location = {{London, UK}},
  abstract = {This paper presents a prototype tool for browsing through multimedia libraries using content-based multimedia information retrieval techniques. It is composed of several groups of components for multimedia analysis, data mining, interactive visualization, as well as connection with external hardware controllers. The musical application of this tool, uses descriptors of timbre, harmony, as well as rhythm and two different approaches for exploring/browsing content. First, a dynamic data mining allows the user to group sounds into clusters according to those different criteria, whose importance can be weighted interactively. In a second mode, sounds that are similar to a query are returned to the user, and can be used to further proceed with the search. The browsing steps are then stored and visualized as a tree. This approach also borrows from multi-criteria optimization concept to return a relevant list of similar sounds.},
  keywords = {content-based similarity,information visualization,multimedia browsers,multimedia information retrieval,music information retrieval}
}

@inproceedings{AudioCycleCBMI2009,
  title = {{{AudioCycle}}: {{Browsing Musical Loop Libraries}}},
  shorttitle = {{{AudioCycle}}},
  booktitle = {7th {{Workshop}} on {{Content-Based Multimedia Indexing}}},
  author = {Dupont, Stéphane and Dubuisson, Thomas and Urbain, Jérôme and Sebbe, Raphaël and family=Alessandro, given=Nicolas, prefix=d', useprefix=true and Frisson, Christian},
  date = {2009-06-03/2009-06-05},
  series = {{{CBMI}}'09},
  pages = {73--80},
  publisher = {{IEEE}},
  doi = {10.1109/CBMI.2009.19},
  abstract = {This paper presents AudioCycle, a prototype application for browsing through music loop libraries. AudioCycle provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation, and listen to individual audio extracts, searching for those of interest. AudioCycle draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Concepts developped here will be of interest to DJs, remixers, musicians, soundtrack composers, but also sound designers and foley artists. Possible extension to multimedia libraries are also suggested.},
  keywords = {audio feature extraction,information visualization,Media content,multimedia browsers,multimedia information retrieval}
}

@inproceedings{AudioCycleICME2009,
  title = {{{AudioCycle}}: {{A}} Similarity-Based Visualization of Musical Libraries},
  shorttitle = {{{AudioCycle}}},
  booktitle = {10th {{Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Urbain, Jérôme and Dubuisson, Thomas and Dupont, Stéphane and Frisson, Christian and Sebbe, Raphaël and family=Alessandro, given=Nicolas, prefix=d', useprefix=true},
  date = {2009-06-28/2009-07-03},
  series = {{{ICME}}'09},
  pages = {1847--1848},
  publisher = {{IEEE}},
  doi = {10.1109/ICME.2009.5202887},
  abstract = {This paper presents AudioCycle, a prototype application for browsing through music loop libraries. AudioCycle provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation and listen to individual audio extracts. AudioCycle draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Possible extension to multimedia libraries are also suggested.},
  keywords = {3D rendering,browsing,information visualization,multimedia browsers,music similarities,visualization}
}

@inproceedings{AudioCycleICME2013,
  title = {Nonlinear Dimensionality Reduction Approaches Applied to Music and Textural Sounds},
  shorttitle = {{{AudioCycle}}},
  booktitle = {14th {{International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Dupont, Stéphane and Ravet, Thierry and Picard-Limpens, Cécile and Frisson, Christian},
  date = {2013-07-15/2013-07-19},
  series = {{{ICME}}'13},
  publisher = {{IEEE}},
  doi = {10.1109/ICME.2013.6607550},
  abstract = {Recently, various dimensionality reduction approaches have been proposed as alternatives to PCA or LDA. These improved approaches do not rely on a linearity assumption, and are hence capable of discovering more complex embeddings within different regions of the data sets. Despite their success on artificial datasets, it is not straightforward to predict which technique is the most appropriate for a given real dataset. In this paper, we empirically evaluate recent techniques on two real audio use cases: musical instrument loops used in music production and sound effects used in sound editing. ISOMAP and t-SNE are being compared to PCA in a visualization problem, where we end up with a two-dimensional view. Various evaluation measures are used: classification performance, as well as trustworthiness/continuity assessing the preservation of neighborhoods. Although PCA and ISOMAP can yield good continuity performance even locally (samples in the original space remain close-by in the low-dimensional one), they fail to preserve the structure of the data well enough to ensure that distinct subgroups remain separate in the visualization. We show that t-SNE presents the best performance, and can even be beneficial as a pre-processing stage for improving classification when the amount of labeled data is low.},
  keywords = {artificial datasets,audio and music analysis,audio signal processing,classification performance,complex embeddings,continuity performance,Databases,Dimensionality reduction,information retrieval,information visualization,Instruments,ISOMAP,LDA,linearity assumption,Manifolds,Measurement,multimedia browsers,multimedia computing,multimedia information retrieval,music,Music,music production,musical instrument loops,musical instruments,nonlinear dimensionality reduction,PCA,principal component analysis,Principal component analysis,real audio use cases,sound editing,sound effects,Standards,t-SNE,textural sounds,visualization problem}
}

@report{AudioCycleNUMEDIART2008,
  title = {Audio {{Cycle}}},
  shorttitle = {{{AudioCycleNUMEDIART2008}}},
  author = {Dupont, Stéphane and family=Alessandro, given=Nicolas, prefix=d', useprefix=true and Dubuisson, Thomas and Frisson, Christian and Sebbe, Raphaël and Urbain, Jérôme},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2008-12},
  number = {4},
  pages = {119--127},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers}
}

@inproceedings{AudioGardenAM2010,
  title = {Towards {{User-Friendly Audio Creation}}},
  shorttitle = {{{AudioGarden}}},
  booktitle = {5th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Picard, Cécile and Frisson, Christian and Vanderdonckt, Jean and Tardieu, Damien and Dutoit, Thierry},
  date = {2010-09-15/2010-09-18},
  series = {{{AM}}'10},
  publisher = {{ACM}},
  location = {{Pitea, Sweden}},
  doi = {10.1145/1859799.1859820},
  abstract = {This paper presents a new approach to sound composition for soundtrack composers and sound designers. We propose a tool for usable sound manipulation and composition that targets sound variety and expressive rendering of the composition. We first automatically segment audio recordings into atomic grains which are displayed on our navigation tool according to signal properties. To perform the synthesis, the user selects one recording as model for rhythmic pattern and timbre evolution, and a set of audio grains. Our synthesis system then processes the chosen sound material to create new sound sequences based on onset detection on the recording model and similarity measurements between the model and the selected grains. With our method, we can create a large variety of sound events such as those encountered in virtual environments or other training simulations, but also sound sequences that can be integrated in a music composition. We present a usability-minded interface that allows to manipulate and tune sound sequences in an appropriate way for sound design.},
  isbn = {978-1-4503-0046-9},
  keywords = {audio analysis \& synthesis,Audio Analysis and Synthesis,content-based audio similarity,Content-based Audio Similarity,digital musical instruments,information visualization,interactive sound composing,Interactive Sound Composing,multi-fidelity prototyping,Multi-fidelity Prototyping,multimedia browsers}
}

@report{AudioGardenNUMEDIART2010,
  title = {{{AudioGarden}}: Towards a {{Usable Tool}} for {{Composite Audio Creation}}},
  shorttitle = {{{AudioGardenNUMEDIART2010}}},
  author = {Frisson, Christian and Picard, Cécile and Tardieu, Damien},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2010-06},
  number = {2},
  pages = {33--36},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {digital musical instruments,information visualization,multimedia browsers}
}

@inproceedings{AudioMetroAM2014,
  title = {{{AudioMetro}}: {{Directing Search}} for {{Sound Designers Through Content-based Cues}}},
  shorttitle = {{{AudioMetro}}},
  booktitle = {9th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Frisson, Christian and Dupont, Stéphane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
  date = {2014-10-01/2014-10-03},
  series = {{{AM}}'14},
  publisher = {{ACM}},
  location = {{Aalborg, Denmark}},
  doi = {10.1145/2636879.2636880},
  abstract = {Sound designers source sounds in massive collections, heavily tagged by themselves and sound librarians. For each query, once successive keywords attained a limit to filter down the results, hundreds of sounds are left to be reviewed. AudioMetro combines a new content-based information visualization technique with instant audio feedback to facilitate this part of their workflow. We show through user evaluations by known-item search in collections of textural sounds that a default grid layout ordered by filename unexpectedly outperforms content-based similarity layouts resulting from a recent dimension reduction technique (Student-t Stochastic Neighbor Embedding), even when complemented with content-based glyphs that emphasize local neighborhoods and cue perceptual features. We propose a solution borrowed from image browsing: a proximity grid, whose density we optimize for nearest neighborhood preservation among the closest cells. Not only does it remove overlap but we show through a subsequent user evaluation that it also helps to direct the search. We based our experiments on an open dataset (the OLPC sound library) for replicability.},
  isbn = {978-1-4503-3032-9},
  keywords = {content-based similarity,dimension reduction,information visualization,known-item search,media browsers,multimedia browsers,music information retrieval,proximity grids,sound effects,Student-t Stochastic Neighbor Embedding,t-SNE,visual variables}
}

@inproceedings{AudioMetroISMIR2014,
  title = {A Proximity Grid Optimization Method to Improve Audio Search for Sound Design},
  shorttitle = {{{AudioMetro}}},
  booktitle = {15th {{International Symposium}} on {{Music Information Retrieval}}},
  author = {Frisson, Christian and Dupont, Stéphane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
  date = {2014-10-27/2014-10-31},
  series = {{{ISMIR}}'14},
  location = {{Taipei, Taiwan}},
  doi = {10.5281/zenodo.1417245},
  abstract = {Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques.},
  keywords = {content-based similarity,dimension reduction,information visualization,known-item search,Media browsers,multimedia browsers,music information retrieval,sound effects,Student-t Stochastic Neighbor Embedding,t-SNE,visual variables}
}

@report{AudioSkimmingNUMEDIART2008,
  title = {Audio {{Skimming}}},
  shorttitle = {{{AudioSkimmingNUMEDIART2008}}},
  author = {Couvreur, Laurent and Bettens, Frédéric and Drugman, Thomas and Frisson, Christian and Jottrand, Matthieu and Mancas, Matei and Moinet, Alexis},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2008-03},
  number = {1},
  pages = {1--16},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers,PureData}
}

@report{AudioThumbnailingNUMEDIART2008,
  title = {Audio {{Thumbnailing}}},
  shorttitle = {{{AudioThumbnailingNUMEDIART2008}}},
  author = {Couvreur, Laurent and Bettens, Frédéric and Drugman, Thomas and Dubuisson, Thomas and Dupont, Stéphane and Frisson, Christian and Jottrand, Matthieu and Mancas, Matei},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2008-06},
  number = {2},
  pages = {67--85},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers}
}

@inproceedings{AuracleeNTERFACE2014,
  title = {Auracle: How Are Salient Cues Situated in Audiovisual Content?},
  shorttitle = {Auracle},
  booktitle = {10th {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Frisson, Christian and Riche, Nicolas and Coutrot, Antoine and Delestage, Charles-Alexandre and Dupont, Stéphane and Ferhat, Onur and Guyader, Nathalie and Mahmoudi, Sidi Ahmed and Mancas, Matei and Mital, Parag K. and Echániz, Alicia Prieto and Rocca, François and Rochette, Alexis and Yvart, Willy},
  date = {2014-06-08/2014-07-05},
  series = {{{eNTERFACE}}'14},
  location = {{Bilbao, Spain}},
  keywords = {eye tracking,multimedia browsers}
}

@inproceedings{BaltazarsMMM2014Mediadrom,
  title = {An Interactive Device for Exploring Thematically Sorted Art-Works},
  shorttitle = {Baltazars},
  booktitle = {20th {{International Conference}} on {{MultiMedia Modeling}}, {{Mediadrom}} Special Session},
  author = {Baltazar, Aurélie and Baltazar, Pia and Frisson, Christian},
  date = {2014-01-07/2014-01-10},
  series = {{{MMM}}'14 {{Mediadrom}}},
  publisher = {{Springer}},
  location = {{Dublin, Ireland}},
  doi = {10.1007/978-3-319-04117-9_4},
  abstract = {This Mediadrom artful post-TV scenario consists in sketching the user interface of an interactive media content browsing system for exploring thematically sorted artworks, from the art field of plastic theater, merging art pieces at the intersection of the visual and the performing arts. Combining a touchscreen and an hypermedia browser of image and video content with expert annotations, this system can be installed in venues such as museum and media libraries, and performance spaces as satellite installation to plastic theater performances.},
  keywords = {hypermedia browser,interactive installation,multimedia annotation,multimedia browsers,plastic theater}
}

@report{BehavioralInstallationsNUMEDIART2009,
  title = {Behavioral {{Installations}}: {{Emergent}} Audiovisual Installations Influenced by Visitors' Behaviours},
  shorttitle = {{{BehavioralInstallationsNUMEDIART2009}}},
  author = {Filatriau, Jean-Julien and Frisson, Christian and Reboursière, Loïc and Siebert, Xavier and Todoroff, Todor},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2009-03},
  number = {1},
  pages = {9--17},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {multimedia browsers}
}

@artwork{BioDiva,
  title = {{{BioDiva}}},
  shorttitle = {{{BioDiva}}},
  author = {Moletta, Laurence},
  editora = {Frisson, Christian},
  editoratype = {collaborator},
  date = {2009},
  keywords = {digital musical instruments}
}

@report{BodilyBenchmarkNUMEDIART2009,
  title = {Bodily {{Benchmark}}: {{Gestural}}/{{Physiological Analysis}} by {{Remote}}/{{Wearable Sensing}}},
  shorttitle = {{{BodilyBenchmarkNUMEDIART2009}}},
  author = {Frisson, Christian and Reboursière, Loïc and Todoroff, Todor and Filatriau, Jean-Julien},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2009-06},
  number = {2},
  pages = {41--57},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {digital musical instruments}
}

@thesis{BradyBoettcherMAMcGill2023,
  type = {MArts},
  title = {Bringing {{DMIs}} and {{Mapping Tools}} to {{Maturity}}},
  author = {Boettcher, Brady},
  editora = {Meneses, Eduardo A. L. and Frisson, Christian and Wanderley, Marcelo M},
  editoratype = {collaborator},
  date = {2023},
  institution = {{McGill University}},
  location = {{IDMIL}}
}

@inproceedings{CADastreExquisseMMM2014Mediadrom,
  title = {Scenarizing {{CADastre Exquisse}}: {{A Crossover Between Snoezeling}} in {{Hospitals}}/{{Domes}}, and {{Authoring}}/{{Experiencing Soundful Comic Strips}}},
  shorttitle = {{{CADastre Exquisse}}},
  booktitle = {20th {{International Conference}} on {{MultiMedia Modeling}}, {{Mediadrom}} Special Session},
  author = {Sabato, Cédric and Giraudet, Aurélien and Delattre, Virginie and Desnos, Yves and Frisson, Christian and Giot, Rudi and Yvart, Willy and Rocca, François and Dupont, Stéphane and Bemden, Guy Vandem and Leleu-Merviel, Sylvie and Dutoit, Thierry},
  date = {2014-01-07/2014-01-10},
  series = {{{MMM}}'14 {{Mediadrom}}},
  publisher = {{Springer}},
  location = {{Dublin, Ireland}},
  doi = {10.1007/978-3-319-04117-9_3},
  abstract = {This paper aims at providing scenarios for the design of authoring and experiencing environments for interactive soundful comic strips. One setting would be a virtual immersive environment made of a dome including spherical projection, surround sound, where visitors comfortably lying down on an interactive mattress can appreciate exquisite corpses floating on the ceiling of the dome, animated, with sound, dependent of the overall behavior of the visitors. On tabletops, creators can generate comic-strip-like creatures by collage or sketching, and associate audiovisual behaviors and soundscapes to these. This creation system will be used in hospitals towards a living lab comforting patients in accepting their health trip. Both settings are inspired by snoezelen methods. These crossover scenarios associate a project by L'Art-Chétype retained to be featured for Mons 2015 EU Capital of Culture and other partners aiming at designing an environment for experiencing/authoring interactive comic-strips augmented with sound.},
  isbn = {978-3-319-04116-2},
  keywords = {collaborative media authoring,interactive comic strips,snoezelen,virtual environments}
}

@thesis{CapstoneCarletonIMD2021,
  type = {Bachelor (Capstone)},
  title = {{{IMD Capstone Projects}}: {{Retrograde}} ({{VFX Short Film Trailer}}) and {{Project Azamusk}} ({{3D Puzzle Game}})},
  shorttitle = {{{CapstoneCarletonIMD2021}}},
  date = {2021},
  institution = {{Carleton University}},
  location = {{School of Information Technology}},
  url = {https://capstone.bitdegree.ca/IMD/index.php?Year=2021}
}

@unpublished{CarletonITEC5027F2022,
  type = {Course},
  title = {Data {{Interaction Techniques}}},
  shorttitle = {{{CarletonITEC5027F2022}}},
  author = {Frisson, Christian},
  date = {2022},
  abstract = {I taught graduate students how humans (e.g., end-users, knowledge-users and expert-users) interact with data ecosystems including data collection, storage, analysis and visualization. Students learned techniques, methods and tools to support human interaction with data based on capabilities of machines and needs of humans. Students learned how to sketch, design, critique and develop (Altair, Tableau, Python Notebooks, Vega-Lite etc..) information visualizations through activities, assignments and group projects.},
  eventtitle = {{{ITEC}} 5207},
  venue = {{Carleton University}}
}

@thesis{ChristianFrissonMEngENSIM2005,
  type = {MEng},
  title = {Développement de l’instrumentarium de l’{{Eobody}}},
  shorttitle = {{{ChristianFrissonMEngENSIM2005}}},
  author = {Frisson, Christian},
  editora = {Sirguy, Marc and Gaviot, Etienne},
  editoratype = {collaborator},
  date = {2005},
  institution = {{Ecole Nationale Supérieure d’Ingénieurs du Mans, France}},
  location = {{Eowave, Paris, France}},
  keywords = {digital musical instruments,midi,sensor}
}

@thesis{ChristianFrissonMScINPG2006,
  type = {MSc},
  title = {Comparaison de Techniques de Synthèse de Type Signal Additive/Source-Filtre et Physique de Type Particulaire: Modèle de Représentation, Environnement, Type de Contrôle et Accès},
  shorttitle = {{{ChristianFrissonMScINPG2006}}},
  author = {Frisson, Christian},
  editora = {Cadoz, Claude and Depalle, Philippe and Wanderley, Marcelo M.},
  editoratype = {collaborator},
  date = {2006},
  institution = {{Institut National Polytechnique de Grenoble, France}},
  location = {{ACROE, Grenoble, France + McGill University, Montreal, Canada}},
  keywords = {audio synthesis,sound synthesis}
}

@thesis{ChristianFrissonPhDUMONS2015,
  type = {phdthesis},
  title = {Designing Interaction for Browsing Media Collections (by Similarity)},
  shorttitle = {{{ChristianFrissonPhDUMONS2015}}},
  author = {Frisson, Christian},
  date = {2015-02-17},
  institution = {{University of Mons, numediart Institute, Belgium}},
  url = {https://tel.archives-ouvertes.fr/tel-01570858},
  abstract = {Sound designers source sounds in massive and heavily tagged collections. When searching for media content, once queries are filtered by keywords, hundreds of items are left to be reviewed. How can we present these results efficiently? This doctoral work aims at improving the usability of browsers of media collections by blending techniques from multimedia information retrieval (MIR) and human-computer interaction (HCI). We produced an in-depth state-of-the-art on media browsers. We overviewed HCI and MIR techniques that support our work: organization by content-based similarity (MIR), information visualization and gestural interaction (HCI). We developed the MediaCycle framework for organization by content-based similarity and the DeviceCycle toolbox for rapid prototyping of gestural interaction, both facilitated the design of several media browsers. We evaluated the usability of some of our media browsers. Our main contribution is AudioMetro, an interactive visualization of sound collections. Sounds are represented by content-based glyphs, mapping perceptual sharpness (audio) to brightness and contour (visual). These glyphs are positioned in a starfield display using Student t-distributed Stochastic Neighbor Embedding (tSNE) for dimension reduction, then a proximity grid optimized for preserving direct neighbors. Known-item search evaluation shows that our technique significantly outperforms a grid of sounds represented by dots and ordered by filename.},
  langid = {english},
  keywords = {browsing,force-feedback,Glyph Design,haptic,haptics,Human computer interaction,Information visualization,multimedia browsers,multimedia information retrieval,music information retrieval,PureData,search user interfaces,similarity,tangible user interfaces}
}

@software{CIRMMTSpeakerSeriesVis,
  type = {Information Visualization Website},
  title = {{{CIRMMT Distinguished Speaker Series Visualizations}}},
  shorttitle = {{{CIRMMTSpeakerSeriesVis}}},
  author = {Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M.},
  date = {2019/2020},
  url = {https://idmil.gitlab.io/CIRMMT_visualizations/},
  keywords = {data visualization,information visualization,multimedia browsers,music,science and technology}
}

@inproceedings{CIRMMTVisHCII2020,
  title = {A {{Visualization Tool}} for the {{CIRMMT Distinguished Lecture Series}}},
  shorttitle = {{{CIRMMTVis}}},
  booktitle = {22nd {{International Conference}} on {{Human-Computer Interaction}}},
  author = {Wanderley, Marcelo M. and Bredholt, Mathias and Frisson, Christian},
  date = {2020},
  series = {{{HCII}}'20},
  publisher = {{Springer}},
  location = {{Copenhagen, Denmark}},
  doi = {10.1007/978-3-030-50020-7_10},
  keywords = {data visualization,information visualization,multimedia browsers,music,science and technology,video lectures}
}

@inproceedings{CoMediAnnotateeNTERFACE2010,
  title = {{{CoMediAnnotate}}: Towards More Usable Multimedia Content Annotation by Adapting the User Interface},
  shorttitle = {{{CoMediAnnotate}}},
  booktitle = {6th {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Frisson, Christian and Alaçam, Sema and Coşkun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
  date = {2010-07-12/2010-08-06},
  series = {{{eNTERFACE}}'10},
  number = {3},
  location = {{Amsterdam, Netherlands}},
  abstract = {This project aims at improving the user experience regarding multimedia content annotation. We evaluated and compared current timeline-based annotation tools, so as to elicit user requirements. We address two issues: 1) adapting the user interface, by supporting more input modalities through a rapid prototyping tool and by offering alternative visualization techniques of temporal signals; and 2) covering more steps of the annotation workflow besides the task of annotation itself: notably recording multimodal signals. We developed input devices components for the OpenInterface (OI) platform for rapid prototyping of multimodal interfaces: multitouch screen, jog wheels and pen-based solutions. We modified an annotation tool created with the Smart Sensor Integration (SSI) toolkit and componentized it in OI so as to bind its controls to different input devices. We produced mockups sketches towards a new design of an improved user interface for multimedia content annotation, and started developing a rough prototype using the Processing Development Environment. Our solution allows to produce several prototypes by varying the interaction pipeline: changing input modalities and using either the initial GUI of the annotation tool, or the newly-designed one. We target usability testing to validate our solution and determine which input modalities combination best suits given use cases.},
  keywords = {gestural interaction,information visualization,multimedia browsers,Multimodal annotation,rapid prototyping}
}

@report{CoMediAnnotateNUMEDIART2010,
  title = {{{CoMediAnnotate}}: Towards More Usable Multimedia Content Annotation by Adapting the User Interface},
  shorttitle = {{{CoMediAnnotateNUMEDIART2010}}},
  author = {Frisson, Christian and Alaçam, Sema and Coşkun, Emirhan and Ertl, Dominik and Kayalar, Ceren and Lawson, Lionel and Lingenfelser, Florian and Wagner, Johannes},
  date = {2010-09},
  number = {3},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {gestural interaction,information visualization,multimedia browsers,Multimodal annotation,rapid prototyping}
}

@inproceedings{ComixTripDIS2016Companion,
  title = {{{ComixTrip}}: {{Reading Comic Books}} with {{Text Sequenced}} through {{Gaze Tracking}}},
  shorttitle = {{{ComixTrip}}},
  booktitle = {2016 {{ACM Conference Companion Publication}} on {{Designing Interactive Systems}}},
  author = {Rochette, Alexis and Goossens, Cédric and Giot, Rudi and Frisson, Christian},
  date = {2016},
  series = {{{DIS}}'16 {{Companion}}},
  publisher = {{ACM}},
  location = {{Brisbane, Australia}},
  doi = {10.1145/2908805.2909405},
  abstract = {Paper-based comic books are rendered on a static medium, where time can alternatively be sequenced through space. People usually prefer to read comic books at their pace. When comic books are digital, their medium becomes dynamic and interactive, how can the readers' experience be redesigned? We present ComixTrip, a system with which people can read digital comic books with sequential media cues responsively adapted to their reading pace. Our system relies on low-cost eye tracking both for measuring how people read; and for sequencing speech balloons semi-automatically: once a balloon is read, the next one is displayed. We ran a study to analyse how people read text paragraphs with diverse spatial layouts by tracking their gaze. Our preliminary results show that we may accurately track when people have read balloons in comic books. Our system needs to be improved regarding inter- and intra-person reading speed variations.},
  isbn = {978-1-4503-4315-2},
  keywords = {comic book,eye tracking,multimedia browsers,reading time}
}

@unpublished{ComproVisCIRMMT2020,
  type = {Workshop Presentation},
  title = {Overview of Gestural Interaction with Sound Information, towards Force-Feedback},
  shorttitle = {{{ComproVisCIRMMT2020}}},
  author = {Frisson, Christian},
  date = {2020-02},
  series = {{{CIRMMT ComproVis}}'20},
  url = {https://frisson.re/ComproVisCIRMMT2020/},
  eventtitle = {{{CIRMMT Workshop}}: {{Composing}} \& Improvising with Information},
  venue = {{Montreal, Qc, Canada}},
  keywords = {audio,digital musical instruments,force feedback,haptics,information visualization,multimedia browsers,music information retrieval,PureData,sound}
}

@article{DataChangesTVCG2020,
  title = {Data {{Changes Everything}}: {{Challenges}} and {{Opportunities}} in {{Data Visualization Design Handoff}}},
  shorttitle = {{{DataChanges}}},
  author = {Walny, Jagoda and Frisson, Christian and West, Mieka and Kosminsky, Doris and Knudsen, Søren and Carpendale, Sheelagh and Willett, Wesley},
  date = {2020-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  series = {{{TVCG}}'20},
  volume = {26},
  number = {1},
  pages = {12--22},
  publisher = {{IEEE}},
  issn = {2160-9306},
  doi = {10.1109/TVCG.2019.2934538},
  abstract = {Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.},
  keywords = {Collaboration,data mapping,Data visualization,design handoff,design process,Design tools,documentation,Information visualization,Software,Task analysis,Tools}
}

@software{DataEmpowermentSpeakerSeries,
  type = {Information Visualization Website},
  title = {Data {{Empowerment Speaker Series Website}}},
  shorttitle = {{{DataEmpowermentSpeakerSeries}}},
  author = {Frisson, Christian and West, Mieka and Pusch, Ricky},
  date = {2017/2018},
  url = {http://speakerseries.ilab.cpsc.ucalgary.ca/},
  keywords = {information visualization}
}

@inproceedings{DataMappingVIS2018InfoVisPosters,
  title = {Discovering the {{Data Mapping}} of an {{Unfamiliar Visualization}}},
  shorttitle = {{{DataMapping}}},
  booktitle = {23rd {{Conference}} on {{Information Visualization}}},
  author = {Hynes, Lisa and Huynh, Tina and Storteboom, Sarah and Walny, Jagoda and Frisson, Christian and Kosminsky, Doris and West, Mieka and Carpendale, Sheelagh and Willett, Wesley},
  date = {2018},
  series = {{{VIS}}'18 {{InfoVis Posters}}},
  publisher = {{IEEE}},
  location = {{Berlin, Germany}},
  keywords = {data mapping,information visualization,mapping,open data}
}

@unpublished{DataVisDesignHandoffPolyMTL2020,
  type = {Guest Lecture},
  title = {Transfert de Conceptions de Visualisations de Données},
  shorttitle = {{{DataVisDesignHandoffPolyMTL2020}}},
  author = {{Christian Frisson}},
  editora = {Hurtut, Thomas},
  editoratype = {collaborator},
  date = {2020-04-14},
  url = {https://frisson.re/DataVisDesignHandoff-PolyMTL-INF8808-2020/},
  abstract = {Presentation (April 14th, 50 min) of Data Visualization Design Handoff (IEEE InfoVis’19 best paper) and complements.},
  eventtitle = {{{INF8808 Winter}} 2020 {{Data Visualization}}},
  venue = {{Polytechnique Montréal}},
  keywords = {documentation,information visualization}
}

@inproceedings{DeviceCycleNIME2010,
  title = {{{DeviceCycle}}: Rapid and Reusable Prototyping of Gestural Interfaces, Applied to Audio Browsing by Similarity},
  shorttitle = {{{DeviceCycle}}},
  booktitle = {10th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Frisson, Christian and Dupont, Stéphane and Siebert, Xavier and Tardieu, Damien and Dutoit, Thierry and Macq, Benoit},
  date = {2010-06-15/2010-06-18},
  series = {{{NIME}}'10},
  location = {{Sydney, NSW, Australia}},
  doi = {10.5281/zenodo.1177771},
  abstract = {This paper presents the development of rapid and reusable gestural interface prototypes for navigation by similarity in an audio database and for sound manipulation, using the AudioCycle application. For this purpose, we propose and follow guidelines for rapid prototyping that we apply using the PureData visual programming environment. We have mainly developed three prototypes of manual control: one combining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits and shortcomings we experienced while prototyping using this approach.},
  isbn = {978-0-646-53482-4},
  keywords = {audio database,browsing by similarity,digital musical instruments,force feedback,gestural interfaces,haptics,Human-computer interaction,multimedia browsers,PureData,rapid prototyping}
}

@inproceedings{DigitalAudioWorkbenchAM2021,
  title = {The {{IDMIL Digital Audio Workbench}}: {{An}} Interactive Online Application for Teaching Digital Audio Concepts},
  shorttitle = {{{DigitalAudioWorkbench}}},
  booktitle = {16th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Wanderley, Marcelo and West, Travis and Rohs, Josh and Meneses, Eduardo and Frisson, Christian},
  date = {2021-09-01},
  series = {{{AM}}'21},
  pages = {232--239},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  url = {http://doi.org/10.1145/3478384.3478397},
  urldate = {2022-02-08},
  abstract = {The Input Devices and Music Interaction Laboratory (IDMIL) Digital Audio Workbench (DAWb) is a web application designed for experimentation with key concepts in digital audio theory with interactive visualizations of each stage of the Analog-to-Digital Conversion (ADC) and Digital-to-Analog Conversion (DAC) processes. By experimenting with the simulation settings, numerous key concepts in digital signal theory can be illustrated, such as aliasing, quantization, critical sampling, anti-aliasing filtering and dithering. The interactive interface allows the simulation to be explored freely; users can modify parameters and examine the resulting signals visually through numerous graphs or listen to the resulting signals. The workbench has been extensively used during the 200-level Introduction to Digital Audio course at McGill University in Fall 2020.},
  isbn = {978-1-4503-8569-5},
  keywords = {digital audio,digital signal processing,explorable explanations,sound and music computing}
}

@inproceedings{DocRepNIME2021,
  title = {Documentation and {{Replicability}} in the {{NIME Community}}},
  shorttitle = {{{DocRep}}},
  booktitle = {21st {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Calegario, Filipe and Tragtenberg, João and Frisson, Christian and Meneses, Eduardo and Cusson, Vincent and Malloch, Joseph and Wanderley, Marcelo M.},
  date = {2021},
  series = {{{NIME}}'21},
  url = {https://nime.pubpub.org/pub/czq0nt9i},
  keywords = {digital musical instruments,documentation,replication}
}

@inproceedings{DotConIHM2010,
  title = {Conception Centrée Utilisateur de Prototypes Interactifs Pour La Gestion de Contenu Multimedia Par Similarité},
  shorttitle = {{{DotCon}}},
  booktitle = {22ème {{Conférence Francophone}} Sur l'{{Interaction Homme-Machine}}},
  author = {Frisson, Christian},
  date = {2010-09-20/2010-09-23},
  series = {{{IHM}}'10},
  publisher = {{ACM}},
  location = {{Luxembourg}},
  url = {http://ihm2010.afihm.org/programme/rencontres-doctorales.html},
  abstract = {Cet article présente les travaux en cours d’une recherche doctorale visant à proposer une méthodologie de conception centrée utilisateur et de prototypage rapide afin de concevoir des applications interactives destinées à la navigation par similarité dans des bases de données multimedia, adaptées à des cas d’utilisation divers et profils d’utilisateurs variés. Les modalités d’interaction sont volontairement restreintes à la visualisation d’information et l’interaction manuelle. Une méthode de développement rapide, réutilisable et durable est proposée, exemplifiée par quelques prototypes à évaluer par des tests utilisateur.},
  keywords = {interaction gestuelle,multimedia browsers,navigation hypermedia,Prototypage rapide,visualisation d’information}
}

@thesis{EliesJurquetSAT2022,
  type = {MEng},
  title = {Facial and Body Animation / Rigging for an Immersive Experience ({{LivePose}})},
  shorttitle = {{{EliesJurquetSAT2022}}},
  author = {Jurquet, Éliès},
  date = {2022},
  institution = {{Polytechnique Montréal + ENSEEIHT}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2021-03-interns/facial-and-body-rigging-internship/}
}

@inproceedings{EnergyVisCHI2018EA,
  title = {Democratizing {{Open Energy Data}} for {{Public Discourse}} Using {{Visualization}}},
  shorttitle = {{{EnergyVis}}},
  booktitle = {35th {{Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Knudsen, Søren and Vermeulen, Jo and Kosminsky, Doris and Walny, Jagoda and West, Mieka and Frisson, Christian and Adriel Aseniero, Bon and MacDonald Vermeulen, Lindsay and Perin, Charles and Quach, Lien and Buk, Peter and Tabuli, Katrina and Chopra, Shreya and Willett, Wesley and Carpendale, Sheelagh},
  date = {2018},
  series = {{{CHI}}'18 {{EA}}},
  publisher = {{ACM}},
  location = {{Montreal, Canada}},
  doi = {10.1145/3170427.3186539},
  abstract = {For this demo, we will show two interactive visualizations: Energy Futures and Pipeline Incidents. We designed and developed these visualizations as part of an open data initiative that aims to create interactive data visualizations to help make Canada's energy data publicly accessible, transparent, and understandable. This work was conducted in collaboration with the National Energy Board of Canada (NEB) and a visualization software development company, VizworX.},
  isbn = {978-1-4503-5621-3},
  keywords = {citizen engagement,data democratization,data visualization,information visualization,open data}
}

@software{EnergyVisConditions,
  type = {Information Visualization Website},
  title = {Conditions on {{CER-regulated}} Energy Projects {{Information Visualization}}},
  shorttitle = {{{EnergyVisConditions}}},
  date = {2017/2020},
  url = {https://apps2.cer-rec.gc.ca/conditions/},
  organization = {{Canada Energy Regulator}},
  keywords = {information visualization}
}

@thesis{EvaDecorpsSAT2022,
  type = {MEng},
  title = {Improvement of the Colorimetric Chain of a Projection Mapping Tool ({{Splash}})},
  shorttitle = {{{EvaDecorpsSAT2022}}},
  author = {Décorps, Éva},
  date = {2022},
  institution = {{ParisTech, France}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2020-09-interns/colorimetric-chain-splash/}
}

@thesis{EzraPierceSAT2023,
  type = {BEng},
  title = {Development of an Open-Source Toolkit for Creating Haptic Effects with Ultrasound Displays},
  shorttitle = {{{EzraPierceSAT2023}}},
  author = {Pierce, Ezra},
  date = {2023},
  institution = {{Carleton University}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2021-09-interns/ultrasound-haptics/}
}

@thesis{FabienGrisardMScINPG2013,
  type = {MSc},
  title = {Gestural Interface for Performative Composition from Collections of Audio Samples.},
  shorttitle = {{{FabienGrisardMScINPG2013}}},
  author = {Grisard, Fabien},
  editora = {Dutoit, Thierry and Frisson, Christian},
  editoratype = {collaborator},
  date = {2013},
  institution = {{Institut National Polytechnique de Grenoble, France}},
  location = {{UMONS, Belgium}},
  keywords = {digital musical instruments,gestural control,media browser,multimedia browsers,PureData,sound painting}
}

@thesis{FannyCacilieGSoC2023,
  type = {GSoC},
  title = {Motion {{Capture}} in {{WebXR}}: A Search for the Uniqueness in Replaying Live Performances},
  shorttitle = {{{FannyCacilieGSoC2023}}},
  author = {Cacilie, Fanny},
  editora = {Bolduc, Manuel and Fardon, Rochana and Frisson, Christian},
  editoratype = {collaborator},
  date = {2023},
  institution = {{CERTI Foundation}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-proposals/proposal-caciliefanny-motion-capture-in-webxr/},
  abstract = {Live performances and concerts promote a sense of uniqueness. It happens for two main reasons: the relationship between the artist and the public; and the relationship between the audience itself. It is a space in time where artists respond to the public as much as the public responds to the artists. Also, every person in the audience is surrounded by other people having similar feelings and sensations. Searching to create related energy and experience via live performances in Extended Reality, this proposal aims to develop a platform prototype to generate a fluid XR experience for users by reliving live performances through motion capture. The key technologies for this goal are: MediaPipe Pose, open source software for pose detection in a live streaming context, and WebXR for the development and hosting of virtual reality and augmented reality experiences on the web. Live performances have no analog due to their unique energy and feeling of shared experience. XR's experience with concerts and live performances does not necessarily wants to exist as a copy of performances in real-life, but as a proposal of different interactions. Therefore Extended Reality can create its paradigm in art and technology.}
}

@thesis{FlorentCouchariereMScUCLouvain2007,
  type = {MSc},
  title = {Analysis of the Drummer Instrumental Gesture through Electromyography ({{EMG}})},
  shorttitle = {{{FlorentCouchariereMScUCLouvain2007}}},
  author = {Coucharière, Florent},
  editora = {Macq, Benoît and Filatriau, Jean-Julien and Frisson, Christian},
  editoratype = {collaborator},
  date = {2007},
  institution = {{Université catholique de Louvain, Belgium}},
  location = {{Université catholique de Louvain, Belgium}},
  keywords = {drums,electromyography,emg,sensor}
}

@article{ForceFeedbackMusicMDPIArts2023,
  title = {Challenges and {{Opportunities}} of {{Force-Feedback}} in {{Music}} ({{Accepted}} for {{Publication}})},
  shorttitle = {{{ForceFeedbackMusic}}},
  author = {Frisson, Christian and Wanderley, Marcelo M.},
  date = {2023},
  journaltitle = {Arts, Special Issue "Feeling the Future—Haptic Audio"},
  series = {{{MDPI Arts}}'23},
  issn = {2076-0752},
  url = {https://www.mdpi.com/journal/arts/special_issues/feeling_the_future},
  issue = {Special Issue "Feeling the Future—Haptic Audio"}
}

@inproceedings{ForceHostNIME2022,
  title = {{{ForceHost}}: An Open-Source Toolchain for Generating Firmware Embedding the Authoring and Rendering of Audio and Force-Feedback Haptics},
  shorttitle = {{{ForceHost}}},
  booktitle = {22nd {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Frisson, Christian and Kirkegaard, Mathias and Pietrzak, Thomas and Wanderley, Marcelo M.},
  date = {2022},
  series = {{{NIME}}'22},
  doi = {10.21428/92fbeb44.76cfc96e},
  url = {https://nime.pubpub.org/pub/jtdpakvp/draft?access=ayl4xcmc},
  keywords = {digital musical instruments,information visualization,mapping,sound synthesis}
}

@inproceedings{FreesoundTrackerHAID2019,
  title = {Haptic Techniques for Browsing Sound Maps Organized by Similarity},
  shorttitle = {{{FreesoundTracker}}},
  booktitle = {9th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  author = {Frisson, Christian and Gallacher, Colin and Wanderley, Marcelo M.},
  date = {2019-03},
  series = {{{HAID}}'19},
  pages = {1},
  location = {{Lille, France}},
  url = {https://hal.inria.fr/hal-02050235},
  keywords = {force feedback,haptics,information visualization,multimedia browsers,sound effects,t-SNE,web audio}
}

@unpublished{HapticAudioFeedbacksIAM2019,
  type = {Workshop Presentation},
  title = {An Overview on {{Haptic}} \& {{Audio}} Research and Applications},
  shorttitle = {{{HapticAudioFeedbacksIAM2019}}},
  author = {Frisson, Christian},
  date = {2019-06},
  series = {{{CIRMMT IAM}}'19},
  url = {https://frisson.re/HapticAudioFeedbacksIAM2019},
  eventtitle = {9th {{Interactive Audio Montreal}} Edition: {{Haptics}} in {{Interaction}} and {{Game Design}}},
  venue = {{Montreal, Qc, Canada}},
  keywords = {audio,audio synthesis,digital musical instruments,force feedback,haptics,multimedia browsers}
}

@inproceedings{HapticProxyIEEEVIS2018Dataphys,
  title = {Haptics as a Sustainable Proxy for Exploring Design Variables for Data Physicalization},
  shorttitle = {{{HapticProxy}}},
  booktitle = {Workshop: {{Toward}} a {{Design Language}} for {{Data Physicalization}}},
  author = {Frisson, Christian and Wanderley, Marcelo M. and Willett, Wesley and Carpendale, Sheelagh},
  date = {2018},
  series = {{{IEEE VIS}}'18 {{Dataphys}}},
  location = {{Berlin, Germany}},
  url = {https://frisson.re/Vis18DataPhys},
  keywords = {data physicalization,force feedback,haptics,information visualization}
}

@unpublished{HapticSmartAssistantsHAID2022,
  title = {Imagining the {{Role}} of {{Haptics}} in {{Future Smart Assistant Technologies}}},
  shorttitle = {{{HapticSmartAssistants}}},
  author = {Fortin, Pascal and Lévesque, Vincent and Frisson, Christian},
  date = {2022-08-25},
  series = {{{HAID}}'22},
  url = {https://hapticsmartassistants.github.io/},
  urldate = {2020-09-09},
  abstract = {Despite their widespread adoption, commercial smart assistant technologies have not evolved significantly in the last decade from an interaction perspective. Indeed, a user explicitly formulates a request, e.g., using their voice, and the system attempts to respond to the user’s demand. We believe that haptics have the potential to drastically shift how we interact with smart assistant technologies by offering rich, private and social interaction opportunities. In this virtual workshop, we aim to bring together experts from both auditory and haptic interaction design to 1) exchange on how the subtleties of haptics could be employed to augment or address limitations of speech-based smart assistant technologies, and 2) collaboratively imagine what it would mean to live with a general purpose purely haptic smart assistant. Through our keynote presentation and interactive explorations of these core topics, we hope to build a unifying research agenda, share ongoing work and most importantly encourage collaborations in this nascent research space. This workshop is part of the 11th International Workshop on Haptic \& Audio Interaction Design (HAID 2022). As such, all workshop participants must also be registered to HAID 2022.},
  eventtitle = {12th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  langid = {english},
  keywords = {3d printing,haptics,piezoelectric ink,printed actuators,printed electronics,printed sensors,PureData,vibrotactile}
}

@unpublished{HaptificationPolyMTL2020,
  type = {Guest Lecture},
  title = {Haptification},
  shorttitle = {{{HaptificationPolyMTL2020}}},
  author = {{Christian Frisson}},
  editora = {Hurtut, Thomas},
  editoratype = {collaborator},
  date = {2020-06-04},
  url = {https://frisson.re/HaptificationPolyMTL2020},
  abstract = {Presentation (June 4th, 50 min) on haptic interaction for information visualization.},
  eventtitle = {{{INF8808E Summer}} 2020 {{Data Visualization}}},
  venue = {{Polytechnique Montréal}},
  keywords = {haptics,information visualization}
}

@software{HCI-MTL-Group,
  type = {Information Visualization Website},
  title = {{{HCI Montreal Group Map}}},
  shorttitle = {{{HCI-MTL-Group}}},
  author = {Frisson, Christian},
  date = {2020},
  url = {https://hcimtl.github.io/group/},
  keywords = {information visualization}
}

@unpublished{HelmholtzJFIS2005,
  type = {Workshop Presentation},
  title = {Helmoltz: Un Outil de Caractérisation Des Deux Premiers Modes de La Guitare},
  shorttitle = {{{HelmholtzJFIS2005}}},
  author = {Gautier, François and Le Carrou, Jean-Loïc and Collin, Erwan and Dufaud, Jérémie and Frisson, Christian},
  date = {2005-07},
  series = {{{JFIS}}'05},
  url = {http://www.itemm.fr/jfis/},
  abstract = {Ce projet porte sur l’étude vibro-acoustique de la guitare. L’objectif est de fournir au luthier une chaîne de mesure simplifiée permettant d’extraire les paramètres d’un modèle à deux degrés de liberté de la guitare. Cet appui graphique lui permettra d’associer les caractéristiques physiques d’une guitare (qualité du bois, masse volumique des composants, raideur de la table d’harmonie) avec une courbe de réponse en fréquence. Nous devons ainsi mettre à disposition au luthier un banc de mesure simple, rapide à l’utilisation et peu coûteux lui permettant de mesurer la réponse en fréquence de caisses de résonance et d'obtenir des paramètres objectifs.},
  eventtitle = {Journées {{Facture Instrumentale}} \& {{Sciences}}: “{{Mettre}} En {{Commun}}”},
  venue = {{Le Mans, France}},
  keywords = {guitar,luthier,modal analysis,Vibroacoustics}
}

@unpublished{ImmersiveArtsEDIACSITITEC50012023,
  type = {Invited Talk},
  title = {Challenges and {{Opportunities}} for {{Equity}}, {{Diversity}}, {{Inclusion}}, and {{Accessibility}} in {{Immersive Arts}}},
  author = {Frisson, Christian},
  date = {2023-04-10},
  series = {{{ITEC5001}}'23},
  url = {https://sat-mtl.gitlab.io/metalab/presentations/2023-04-10-Carleton-ITEC5001-Immersive-Arts-EDIA/},
  eventtitle = {Carleton {{School}} of {{Information Technology}} ({{CSIT}}) {{ITEC}} 5001 {{Graduate Student Seminar}}}
}

@unpublished{ImmersiveArtsEDIIVR2023,
  type = {Workshop Presentation},
  title = {Challenges and {{Opportunities}} for {{Equity Diversity}} and {{Inclusion}} in {{Immersive Arts}}},
  shorttitle = {{{ImmersiveArtsEDI}}},
  author = {Frisson, Christian and Durand, Emmanuel},
  date = {2023-03-06},
  series = {{{IVR}}'23},
  url = {https://sat-mtl.gitlab.io/metalab/presentations/2023-IVR/},
  eventtitle = {Workshop on {{Inclusion}} in {{Virtual Reality}}}
}

@unpublished{InclusiveDataVisualizationDagstuhl2023,
  title = {Inclusive {{Data Visualization}}},
  shorttitle = {{{InclusiveDataVisualizationDagstuhl2023}}},
  author = {Baker, Catie and Branig, Meinhardt and Butler, Matthew and Choe, Eun Kyoung and Choi, Soyoung and Dykes, Jason and Ellis, Kirsten and Engel, Christin and Frisson, Christian and Godfrey, Jonathan and Goncu, Cagatay and Hearst, Marti and Holloway, Leona and Joshi, Anirudha and Lee, Bongshin and Mankoff, Jennifer and Marinai, Simone and Marriott, Kim and Moritz, Dominik and Müller, Karin and Petrie, Helen and Satyanarayan, Arvind and Seo, JooYoung and Szafir, Danielle and Tanis, Shea and Thompson, John R. and Watanabe, Tetsuya and Weber, Gerhard and Weyers, Benjamin and Wilson, Stephanie and Wu, Keke},
  date = {2023-06-18},
  series = {Dagstuhl'23},
  url = {https://www.dagstuhl.de/23252},
  abstract = {We live in a data-driven world, where critical decisions are made based on data. Both experts and laypeople have access to large amounts of data and understanding data has become a core part of information work. Data visualization is a powerful means not only to analyze and explore data but also to identify and communicate insights. Most existing data visualizations, however, are designed on implicit assumptions about people's sensory, cognitive, and motor abilities. A lack of access to data visualization and the underlying data due to the differences in these abilities impacts educational and work opportunities, as well as health and lifestyle, posing a significant equity issue. To successfully address this important issue, visualization, accessibility, and other HCI researchers should work together to develop guidelines, methods, and techniques for increasing visualization accessibility. To build partnerships and develop a shared understanding of this important research topic, this Dagstuhl Seminar aims to bring together researchers and practitioners from relevant fields, including data visualization, accessibility and assistive technologies, mobile and tangible interaction, human-computer interaction, and vision science, as well as representatives from disability support organizations and people with lived experience of disability. The main goals of this seminar are to:     Increase awareness of accessibility in the data visualization community and awareness of data visualization in the accessibility community.     Identify open problems and challenges that establish a rigorous foundation for inclusive data visualization.     Develop a research agenda and plans for future activities in inclusive data visualization.     Establish a community around inclusive data visualization and create collaboration opportunities.  Leveraging the unique setting of Schloss Dagstuhl, the focus of this seminar will be an interactive dialog between seminar participants with multidisciplinary backgrounds. This seminar will be structured to facilitate the exchange of information and experiences, to stimulate discussion and brainstorming, to kickstart collaborations, and to identify novel aspects and ideas around inclusive data visualization. The outcomes of the seminar generated from the activities and discussions will provide the impetus for a critical overarching goal: making the data and visualization accessible to a broad range of people.},
  eventtitle = {Dagstuhl {{Seminar}} 23252}
}

@inproceedings{InfoPhysTEI2016,
  title = {{{InfoPhys}}: {{Direct Manipulation}} of {{Information Visualisation}} through a {{Force-Feedback Pointing Device}}},
  shorttitle = {{{InfoPhys}}},
  booktitle = {10th {{Conference}} on {{Tangible}}, {{Embedded}} \& {{Embodied Interaction}}},
  author = {Frisson, Christian and Dumas, Bruno},
  date = {2016},
  series = {{{TEI}}'16},
  publisher = {{ACM}},
  location = {{Eindhoven, Netherlands}},
  doi = {10.1145/2839462.2856545},
  abstract = {Information visualisation is the transformation of abstract data into visual, interactive representations. In this paper we present InfoPhys, a device that enables the direct, tangible manipulation of visualisations. InfoPhys makes use of a force-feedback pointing device to simulate haptic feedback while the user explores visualisations projected on top of the device. We present a use case illustrating the trends in ten years of TEI proceedings and how InfoPhys allows users to feel and manipulate these trends. The technical and software aspects of our prototype are presented, and promising improvements and future work opened by InfoPhys are then discussed.},
  isbn = {978-1-4503-3582-9},
  keywords = {direct manipulation,force-feedback user interfaces,haptics,information visualization,multimedia browsers,physical visualisation,tangible user interfaces}
}

@software{InspectorWidget,
  title = {{{InspectorWidget}}: An Opensource Suite to Track and Analyze Users Behaviors in Their Applications},
  shorttitle = {{{InspectorWidget}}},
  author = {Frisson, Christian and Malacria, Sylvain},
  date = {2016/2018},
  url = {https://github.com/InspectorWidget},
  keywords = {accessibility,computer vision,multimedia browsers,usability}
}

@inproceedings{InspectorWidgetCHI2016EA,
  title = {{{InspectorWidget}}: {{A System}} to {{Analyze Users Behaviors}} in {{Their Applications}}},
  shorttitle = {{{InspectorWidget}}},
  booktitle = {33rd {{Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Frisson, Christian and Malacria, Sylvain and Bailly, Gilles and Dutoit, Thierry},
  date = {2016},
  series = {{{CHI}}'16 {{EA}}},
  publisher = {{ACM}},
  location = {{San Jose, California, USA}},
  doi = {10.1145/2851581.2892388},
  abstract = {We propose InspectorWidget, an opensource application to track and analyze users' behaviors in interactive software. The key contributions of our application are: 1) it works with closed applications that do not provide source code nor scripting capabilities; 2) it covers the whole pipeline of software analysis from logging input events to visual statistics through browsing and programmable annotation; 3) it allows post-recording logging; and 4) it does not require programming skills. To achieve this, InspectorWidget combines low-level event logging (e.g. mouse and keyboard events) and high-level screen features (e.g. interface widgets) captured though computer vision techniques. InspectorWidget benefits end users, usability experts and HCI researchers.},
  isbn = {978-1-4503-4082-3},
  keywords = {accessibility api,automatic annotation,computer vision,logging,multimedia browsers,usability}
}

@unpublished{InspectorWidgetFOSDEM2017,
  type = {Workshop Presentation},
  title = {Developers Looking for Designers? {{Pitch}} Your Project: {{InspectorWidget}}},
  shorttitle = {{{InspectorWidgetFOSDEM2017}}},
  author = {Frisson, Christian},
  date = {2017-02},
  series = {{{FOSDEM}}'17},
  url = {https://archive.fosdem.org/2017/schedule/event/osd_pitch_your_project/},
  eventtitle = {17th {{Free}} and {{Open Source Developers}}’ {{European Meeting}}, {{Open Source Design}} Devroom},
  venue = {{Brussels, Belgium}},
  keywords = {accessibility,multimedia browsers,usability}
}

@inproceedings{InteractiveNetworkInstallationINTETAIN2014,
  title = {Interactive {{Network Installation}}},
  shorttitle = {{{InteractiveNetworkInstallation}}},
  booktitle = {Intelligent {{Technologies}} for {{Interactive Entertainment}}},
  author = {Laffineur, Ludovic and Degeest, Alexandra and Frisson, Christian and Giot, Rudi},
  editor = {Reidsma, Dennis and Choi, Insook and Bargar, Robin},
  date = {2014},
  series = {{{INTETAIN}}'14},
  pages = {140--143},
  publisher = {{Springer}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-08189-2_20},
  abstract = {The work discussed is this paper deals with a interactive installation to monitor the network flow in a artistic way. The system is developed in C++ grabs packets using LibPCap, analyses them at low level (e.g. packet length) and also provides high-level information (e.g. port number). This new approach is based more on the network flow analysis than on network services analysis. The software communicates with ©Resolume Avenue and ©Reaktor through OSC protocol. ©Resolume Avenue is a software for Video Jockey (VJ) purposes and ©Reaktor is a modular software music studio developed by Native Instrument. Users can actively take part to an interactive audiovisual exhibition system using their mobile device to send e-mails, listen to a web radio, surf on a website, read RSS feeds, in short, the experience begins once visitors exchange data with the network.},
  isbn = {978-3-319-08189-2},
  langid = {english},
  keywords = {installation,network,Sonification,visualisation}
}

@software{ISMIR2020MiniConfLBD,
  type = {Information Visualization Website},
  title = {{{ISMIR}} 2020 {{Late-Breaking}}/{{Demo Online Program}}},
  shorttitle = {{{ISMIR2020MiniConfLBD}}},
  author = {Frisson, Christian and {Evan Savage} and {Gabriel Vigliensoni} and {Néstor Nápoles López}},
  date = {2020},
  url = {https://program.ismir2020.net/lbds.html},
  keywords = {information visualization,music information retrieval}
}

@unpublished{LavaAMPUIST2013SIC,
  type = {Student Innovation Contest},
  title = {{{LavaAMP}}: Surrounding the Beats in Music Tracks by Streaming Colorful Blobs},
  shorttitle = {{{LavaAMPUIST2013SIC}}},
  author = {Frisson, Christian and Schayes, Eric},
  date = {2013-10-08/2013-10-11},
  series = {{{ACM UIST}}'13 {{SIC}}},
  url = {https://uist.acm.org/uist2013/contest.php},
  abstract = {LavaAMP surrounds the beats in music tracks by streaming colorful blobs. LavaAMP is inspired from vacuum tube amps and lava lamps. Each onset detected in music tracks would trigger an pump impulse in one bottle with cyclic permuting order, creating a flow of colored blobs.},
  eventtitle = {26th {{Symposium}} on {{User Interface Software}} and {{Technology}}, {{Student Innovation Contest}}},
  venue = {{St. Andrews, Scotland, UK}},
  keywords = {beat tracking,embedded visualizations,fountain development kit,liquid display,Microsoft Pumpspark,multimedia browsers,music information retrieval,PureData}
}

@inproceedings{LeBatonNIME2021,
  title = {Le {{Bâton}}: {{A Digital Musical Instrument Based}} on the {{Chaotic Triple Pendulum}}},
  shorttitle = {{{LeBaton}}},
  booktitle = {21st {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Skarha, Matthew and Cusson, Vincent and Frisson, Christian and Wanderley, Marcelo M.},
  date = {2021},
  series = {{{NIME}}'21},
  url = {https://nime.pubpub.org/pub/uh1zfz1f},
  keywords = {3d printing,digital musical instruments,embedded visualizations,information visualization,mapping,robotics,sound synthesis}
}

@software{libhappiness,
  title = {Libhappiness: Library for Driving Piezo Actuators and Sensing Capacitive Touch},
  shorttitle = {Libhappiness},
  author = {Decaudin, Julien and Frisson, Christian and Pietrzak, Thomas},
  date = {2017/2020},
  url = {https://gitlab.inria.fr/Loki/happiness/libhappiness},
  keywords = {haptics,PureData,vibrotactile}
}

@thesis{LisaHynesBScUofC2018,
  type = {BSc},
  title = {Discovering the {{Data Mapping}} of an {{Unfamiliar Visualization}}},
  shorttitle = {{{LisaHynesBScUofC2018}}},
  author = {Hynes, Lisa},
  date = {2018},
  institution = {{University of Calgary}},
  location = {{iLab}}
}

@inproceedings{LivePoseACMVRST2022,
  title = {{{LivePose}}: {{Democratizing Pose}} and {{Action Detection}} for {{Multimedia Arts}} and {{Hybrid Presence Applications}} on {{Open Edge Devices}}},
  shorttitle = {{{LivePose}}},
  booktitle = {28th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Frisson, Christian and Downs, Gabriel and Dumas, Marie-Eve and Askari, Farzaneh and Durand, Emmanuel},
  date = {2022},
  series = {{{ACM VRST}}'22},
  doi = {10.1145/3562939.3565660}
}

@inproceedings{LivePosePortalMozillaResponsibleAI2023,
  title = {{{LivePosePortal}}: Democratize Immersive Arts by Having Participants Train Both Themselves and the Deep-Learning Models for Camera-Based Group Interactivity in a Portal before Entering the Show (Selected among {{Top Ten Finalists}})},
  shorttitle = {{{LivePosePortal}}},
  booktitle = {Mozilla {{Responsible AI Challenge}}},
  author = {Rios, Victor and Auphan, Valentine and Colpron, Bruno and Ouellet-Delorme, Émile and Durand, Emmanuel and Frisson, Christian},
  date = {2023},
  series = {{{MozillaResponsibleAI}}'23},
  url = {https://sat-mtl.gitlab.io/metalab/presentations/2023-LivePosePortal-MozillaResponsibleAI/}
}

@artwork{LoopJam,
  title = {{{LoopJam}}},
  shorttitle = {{{LoopJam}}},
  author = {Frisson, Christian},
  date = {2012},
  keywords = {digital musical instruments,multimedia browsers}
}

@inproceedings{LoopJamJIM2012,
  title = {{{LoopJam}}: Une Carte Musicale Collaborative Sur La Piste de Danse},
  shorttitle = {{{LoopJam}}},
  booktitle = {19èmes {{Journées}} d'{{Informatique Musicale}}},
  author = {Frisson, Christian and Dupont, Stéphane and Moinet, Alexis and Leroy, Julien and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
  date = {2012-05-09/2012-05-11},
  series = {{{JIM}}'12},
  pages = {101--105},
  location = {{Mons, Belgium}},
  url = {https://hal.archives-ouvertes.fr/hal-03041774},
  abstract = {Ce papier présente l'installation LoopJam qui permet aux visiteurs d'interagir avec une carte musicale par le biais d'un système de suivi gestuel par vision informatique. La carte sonore résulte d'un partitionnement des sons en groupes par similarité basée sur leur signal. Le rendu sonore est contrôlé par les positions ou gestes des participants captés par une caméra Kinect détectant la profondeur de la scène 3D. Les mouvements des participants exprimant une mesure ou un tempo sont corrélés à la vitesse de lecture commune à tous les échantillons synchronisés par le moteur audio. Nous avons présenté et testé une première version de cette installation lors de trois expositions en Belgique, Italie et France. Les réactions parmi les participants ont varié entre la curiosité et l'amusement.},
  keywords = {audio similarity,depth sensors,digital musical instruments,information visualization,Interactive music systems and retrieval,multimedia browsers,user interaction and interfaces}
}

@inproceedings{LoopJamNIME2012,
  title = {{{LoopJam}}: Turning the Dance Floor into a Collaborative Instrumental Map},
  shorttitle = {{{LoopJam}}},
  booktitle = {12th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Frisson, Christian and Dupont, Stéphane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
  editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
  date = {2012-05-21/2012-05-23},
  series = {{{NIME}}'12},
  location = {{Ann Arbor, Michigan}},
  doi = {10.5281/zenodo.1178255},
  abstract = {This paper presents the LoopJam installation which allows participants to interact with a sound map using a 3D computer vision tracking system. The sound map results from similarity-based clustering of sounds. The playback of these sounds is controlled by the positions or gestures of participants tracked with a Kinect depth-sensing camera. The beat-inclined bodily movements of participants in the installation are mapped to the tempo of played sounds, while the playback speed is synchronized by default among all sounds. We presented and tested an early version of the installation to three exhibitions in Belgium, Italy and France. The reactions among participants ranged between curiosity and amusement.},
  keywords = {audio similarity,depth sensors,digital musical instruments,information visualization,Interactive music systems and retrieval,multimedia browsers,user interaction and interfaces}
}

@report{LoopJamNUMEDIART2011,
  title = {{{LoopJam}}: A Collaborative Musical Map on the Dance Floor},
  shorttitle = {{{LoopJamNUMEDIART2011}}},
  author = {Frisson, Christian and Dupont, Stéphane and Leroy, Julien and Moinet, Alexis and Ravet, Thierry and Siebert, Xavier},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  date = {2011-06},
  number = {2},
  pages = {37--40},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {digital musical instruments,information visualization,multimedia browsers}
}

@inproceedings{MakamCycleCompMusic2012,
  title = {Improving the {{Understanding}} of {{Turkish Makam Music}} through the {{MediaCycle Framework}}},
  shorttitle = {{{MakamCycle}}},
  booktitle = {2nd {{CompMusic Workshop}}},
  author = {Babacan, Onur and Frisson, Christian and Dutoit, Thierry},
  date = {2012-07-12/2012-07-13},
  series = {{{CompMusic}}'12},
  pages = {25--28},
  location = {{Istanbul, Turkey}},
  url = {http://compmusic.upf.edu/system/files/static_files/06-Onur-Babacan-et-al-2nd-CompMusic-Workshop-2012_0.pdf},
  keywords = {information visualization,multimedia browsers,music,music information retrieval}
}

@report{MakamCycleNUMEDIART2012,
  title = {{{MakamCycle}}: Improving the Understanding of {{Turkish Makam Music}} through the {{MediaCycle Framework}}},
  shorttitle = {{{MakamCycleNUMEDIART2012}}},
  author = {Frisson, Christian and Babacan, Onur and Dutoit, Thierry},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  date = {2012-06},
  number = {2},
  pages = {17--20},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers,music information retrieval}
}

@thesis{ManuelBolducSAT2023,
  type = {MMath},
  title = {Live Virtual Crowd Animation for Symphonic Concert Virtualization},
  shorttitle = {{{ManuelBolducSAT2023}}},
  author = {Bolduc, Manuel},
  date = {2023},
  institution = {{McGill University}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2022-interns/live-crowd-animation/}
}

@inproceedings{MapLooperNIME2021,
  title = {{{MapLooper}}: Live-Looping of Distributed Gesture-to-Sound Mappings},
  shorttitle = {{{MapLooper}}},
  booktitle = {21st {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Frisson, Christian and Bredholt, Mathias and Malloch, Joseph and Wanderley, Marcelo M.},
  date = {2021},
  series = {{{NIME}}'21},
  url = {https://nime.pubpub.org/pub/2pqbusk7/},
  keywords = {digital musical instruments,information visualization,mapping,sound synthesis}
}

@inproceedings{MappEMGNIME2022,
  title = {Feeling the {{Effort}} of {{Classical Musicians}} - {{A Pipeline}} from {{Electromyography}} to {{Smartphone Vibration}} for {{Live Music Performance}}},
  shorttitle = {{{MappEMG}}},
  booktitle = {22nd {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Verdugo, Felipe and Ceglia, Amedeo and Frisson, Christian and Burton, Alexandre and Begon, Mickael and Gibet, Sylvie and Wanderley, Marcelo M.},
  date = {2022},
  series = {{{NIME}}'22},
  url = {https://nime.pubpub.org/pub/kmn0rbyp/draft?access=1ga9wh1v},
  keywords = {digital musical instruments,information visualization,mapping,sound synthesis}
}

@inproceedings{MappingIntegrationNIME2023,
  title = {Addressing {{Barriers}} for {{Entry}} and {{Operation}} of a {{Distributed Signal Mapping Framework}}},
  shorttitle = {{{MappingIntegration}}},
  booktitle = {23rd {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Boettcher, Brady and Meneses, Eduardo A. L. and Frisson, Christian and Wanderley, Marcelo M. and Malloch, Joseph},
  date = {2023},
  series = {{{NIME}}'23}
}

@artwork{MashtaCycle,
  title = {{{MashtaCycle}}},
  shorttitle = {{{MashtaCycle}}},
  author = {Keyaerts, Gauthier},
  editora = {Frisson, Christian},
  editoratype = {collaborator},
  date = {2013},
  keywords = {digital musical instruments,multimedia browsers}
}

@inproceedings{MashtaCycleINTETAIN2013,
  title = {{{MashtaCycle}}: On-Stage Improvised Audio Collage by Content-Based Similarity and Gesture Recognition},
  shorttitle = {{{MashtaCycle}}},
  booktitle = {5th {{International Conference}} on {{Intelligent Technologies}} for {{Interactive Entertainment}}},
  author = {Frisson, Christian and Keyaerts, Gauthier and Grisard, Fabien and Dupont, Stéphane and Ravet, Thierry and Zajéga, François and Guerra, Laura Colmenares and Todoroff, Todor and Dutoit, Thierry},
  date = {2013-07-03/2013-07-05},
  series = {{{INTETAIN}}'13},
  publisher = {{Springer}},
  location = {{Mons, Belgium}},
  doi = {10.1007/978-3-319-03892-6_14},
  abstract = {In this paper we present the outline of a performance in-progress. It brings together the skilled musical practices from Belgian audio collagist Gauthier Keyaerts aka Very Mash'ta; and the realtime, content-based audio browsing capabilities of the AudioCycle and LoopJam applications developed by the remaining authors. The tool derived from AudioCycle named MashtaCycle aids the preparation of collections of stem audio loops before performances by extracting content-based features (for instance timbre) used for the positioning of these sounds on a 2D visual map. The tool becomes an embodied on-stage instrument, based on a user interface which uses a depth-sensing camera, and augmented with the public projection of the 2D map. The camera tracks the position of the artist within the sensing area to trigger sounds similarly to the LoopJam installation. It also senses gestures from the performer interpreted with the Full Body Interaction (FUBI) framework, allowing to apply sound effects based on bodily movements. MashtaCycle blurs the boundary between performance and preparation, navigation and improvisation, installations and concerts.},
  keywords = {audio collage,content-based similarity,depth cameras,digital audio effects,digital musical instruments,gesture recognition,Human-music interaction,multimedia browsers,PureData}
}

@thesis{MathiasBredholtMAMcGill2020,
  type = {MArts},
  title = {{{CIRMMT Distinguished Speaker Series Visualizations}}},
  shorttitle = {{{MathiasBredholtMAMcGill2020}}},
  author = {Bredholt, Mathias},
  editora = {Wanderley, Marcelo M. and Frisson, Christian},
  editoratype = {collaborator},
  date = {2020},
  institution = {{McGill University}},
  location = {{IDMIL}},
  keywords = {digital musical instruments,information visualization,mapping}
}

@thesis{MathiasBredholtMAMcGill2021,
  type = {MArts},
  title = {Live-Looping of Distributed Gesture-to-Sound Mappings},
  shorttitle = {{{MathiasBredholtMAMcGill2021}}},
  author = {Bredholt, Mathias},
  editora = {Wanderley, Marcelo M. and Meneses, Eduardo and Frisson, Christian and Michon, Romain},
  editoratype = {collaborator},
  date = {2021},
  institution = {{McGill University}},
  location = {{IDMIL}},
  url = {https://escholarship.mcgill.ca/concern/theses/gx41mp578},
  keywords = {digital musical instruments,information visualization,mapping}
}

@thesis{MathiasKirkegaardMAMcGill2021,
  type = {MArts},
  title = {Integrating 1-{{DoF Force Feedback Interactions In Self-Contained DMIs}}},
  shorttitle = {{{MathiasKirkegaardMAMcGill2021}}},
  author = {Kirkegaard, Mathias},
  editora = {Wanderley, Marcelo M. and Frisson, Christian and Pietrzak, Thomas and Paté, Arthur},
  editoratype = {collaborator},
  date = {2021},
  institution = {{McGill University}},
  location = {{IDMIL}},
  url = {https://escholarship.mcgill.ca/concern/theses/b5644x49k},
  keywords = {digital musical instruments,force-feedback,haptics,information visualization}
}

@thesis{MatthewWieseGSoC2022,
  type = {GSoC},
  title = {Face {{Tracking}} to {{Improve Accessibility}} and {{Interaction}} in the {{Metaverse}} with {{Satellite}}},
  shorttitle = {{{MatthewWieseGSoC2022}}},
  author = {Wiese, Matthew},
  editora = {Frisson, Christian and Durand, Emmanuel and Askari, Farzaneh},
  editoratype = {collaborator},
  date = {2022},
  institution = {{University of Colorado Boulder, USA}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2022-proposals/proposal-matthewwiese-satellite-face-tracking/},
  abstract = {The "metaverse" has seen exceptional interest in both tech and the wider market. However, metaverse software currently on the market, from Second Life to Horizon Worlds, either limit input to traditional input devices like mice and keyboards, or require expensive gear in the form of VR headsets and specialized controllers. In this project we aim at integrating interaction techniques that rely on human facial and body gestures, similar to these employed in high-fidelity telepresence systems like Google's Starline, that we will apply to a hybrid presence system and with low-cost hardware to enable accessibility and affordability. This GSoC project aims to perform much needed research and development into using face tracking as a viable means to improve the accessibility of 3D virtual spaces, much like those found in the metaverse. The result benefits all users, whether or not they have specific needs for accessibility: improved interaction vectors and empathy channels liberate the user to go become human. Face tracking software built upon open source technologies will be integrated with Satellite / Mozilla Hubs to provide this crucial feature, enabling a practical testbed for future innovation in hybrid telepresence interaction.}
}

@thesis{MaxwellGentiliMorinGSoC2023,
  type = {GSoC},
  title = {Audio to {{Haptic}} Interaction Design with {{ForceHost}} and {{Feelix}} Supporting {{DeformableHapticSurfaces}}},
  shorttitle = {{{MaxwellGentiliMorinGSoC2023}}},
  author = {Gentili-Morin, Maxwell},
  editora = {family=Oosterhout, given=Anke, prefix=van, useprefix=true and Frisson, Christian},
  editoratype = {collaborator},
  date = {2023},
  institution = {{McGill University, IDMIL}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-proposals/proposal-maxw3llgm-force-host-support-deformablehapticsurfaces-feelix/},
  abstract = {Haptics, as a multidisciplinary field, has applications within the Medical, Consumer and Entertainment domains, to name a few, and despite their widespread presence in our phones to provide the user with a vibrotactile feedback response (a subset of haptics), haptics have very minimally been explored by artists as a tool for authoring immersive arts. The proposed project aims to create low-cost affordable haptic devices with one or mutliple degrees of freedom and authoring tools for artists to implement into their artwork immersive haptics and interactive audio. In the near future, artists can then use the proposed toolkit to benefit from Haptic floors being deployed into immersive art spaces like the Dome at SAT. The authoring tools Feelix, a "haptic authoring tool developed to support the design and integration of force feedback and shape change in user interfaces", and ForceHost, a toolchain that uses the functional sound synthesis and processing programming language FAUST that compiles firmware for audio-haptic applications, will be extended for use with the DeformableHapticSurfaces, a work-in-progress and open-source toolkit for interactive multi-linear DoF deformable surfaces, to create and demo the proposed immersive haptic and audio interaction toolkit. Support for audio input will also be added into Feelix potentially with ForceHost as an input modality so that ForceHost developed tools can interface their audio with the Haptic floor.}
}

@unpublished{McGillMUMT620Fall2020Haptics,
  type = {Guest Lecture},
  title = {Gestural {{Control}} of {{Music}} (with {{Force Feedback Haptics}})},
  shorttitle = {{{McGillMUMT620Fall2020Haptics}}},
  author = {{Christian Frisson}},
  editora = {Wanderley, Marcelo M},
  editoratype = {collaborator},
  date = {2020-09-17},
  url = {https://idmil.gitlab.io/course-materials/mumt620/2020-fall-guest-lecture-force-feedback-frisson/#/start},
  abstract = {Presentation (50 min) of course complements in haptics.},
  eventtitle = {{{MUMT620 Fall}} 2020 {{Gestural Control}} of {{Sound Synthesis}}},
  venue = {{McGill University}},
  keywords = {haptics}
}

@unpublished{McGillMUMT620Fall2020InfoVis,
  type = {Guest Lecture},
  title = {Gestural {{Control}} of {{Music}} (with {{Information Visualization}})},
  shorttitle = {{{McGillMUMT620Fall2020InfoVis}}},
  author = {{Christian Frisson}},
  editora = {Wanderley, Marcelo M},
  editoratype = {collaborator},
  date = {2020-10-01},
  url = {https://idmil.gitlab.io/course-materials/mumt620/2020-fall-guest-lecture-infovis-frisson/},
  abstract = {Presentation (50 min) of course complements in information visualization.},
  eventtitle = {{{MUMT620 Fall}} 2020 {{Gestural Control}} of {{Sound Synthesis}}},
  venue = {{McGill University}},
  keywords = {information visualization}
}

@unpublished{McGillMUMT620Winter2019InfoVis,
  type = {Guest Lecture},
  title = {Information {{Visualisation}}},
  shorttitle = {{{McGillMUMT620Winter2019InfoVis}}},
  author = {{Christian Frisson}},
  editora = {Wanderley, Marcelo M},
  editoratype = {collaborator},
  date = {2019},
  abstract = {Presentation (2 x 50 min) of course complements and evaluation of student projects in information visualization.},
  eventtitle = {{{MUMT620 Winter}} 2019 {{Gestural Control}} of {{Sound Synthesis}}},
  venue = {{McGill University}},
  keywords = {information visualization}
}

@unpublished{MechaMediaDuinoFFM2016,
  type = {Invited Talk},
  title = {Force-{{Feedback}} ({{Rotary}}) {{Audio Browsing}}},
  shorttitle = {{{MechaMediaDuinoFFM2016}}},
  author = {Frisson, Christian},
  date = {2016},
  series = {{{FF}}\&{{M}}'16},
  url = {https://hal.inria.fr/hal-01429162},
  abstract = {A subset of not so new interfaces for musical expression have been traditionally employed in an artistic and scientific field related to and generative of computer music: physical/tangible controls for media browsing. Cyclic representations of time might have been the motivation for the use of rotary control for temporal media (audio and video). Rotary controls have been widely used by experts in audio edition and video montage even before their systems were computerized, with passive proprioceptive and kinesthetic feedback (on hands) limited by the physical controls during their design and fabrication. Why are there no cost-effective commercial devices for force-feedback rotary control widely available now for digital systems, with user-definable mappings, besides the upcoming Microsoft Surface Dial? Can we just make one from off-the-shelf and repurposed components? This talk will start with a short overview of past personal projects on tangible-to-force-feedback media browsing.  The core of the talk is to provide a log reporting hands-on attempts in replicating interaction techniques for force-feedback audio browsing from seminal papers, towards a "hello world" tutorial, using a recent low-cost opensource and openhardware servo motor project (MechaDuino) and a fork of a visual programming environment dedicated for audio/control dataflow (PurrData out of PureData) that had already been used for prototyping force-feedback and music applications.},
  eventtitle = {Force-{{Feedback}} \& {{Music Symposium}}},
  venue = {{Montreal, QC, Canada}},
  keywords = {audio browsing,force feedback,force-feedback user interfaces,haptics,jog wheel,media browsing,rotary,tangible user interfaces}
}

@report{MediaBlenderNUMEDIART2011,
  title = {{{MediaBlender}} : {{Interactive Multimedia Segmentation}}},
  shorttitle = {{{MediaBlenderNUMEDIART2011}}},
  author = {Dupont, Stéphane and Frisson, Christian and Urbain, Jérôme and Mahmoudi, Sidi and Siebert, Xavier},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  date = {2011-03},
  number = {1},
  pages = {1--6},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers}
}

@inproceedings{MediaCycleISEA2011,
  title = {Similarity in Media Content: Digital Art Perspectives},
  shorttitle = {{{MediaCycle}}},
  booktitle = {17th {{Symposium}} on {{Electronic Art}}},
  author = {Frisson, Christian and Dupont, Stéphane and Siebert, Xavier and Dutoit, Thierry},
  date = {2011-09-14/2011-09-21},
  series = {{{ISEA}}'11},
  location = {{Istanbul, Turkey}},
  url = {https://web.archive.org/web/20171029070517/https://isea2011.sabanciuniv.edu/paper/similarity-media-content-digital-art-perspectives},
  abstract = {This essay examines how media content navigation by similarity can foster new practices in digital arts, blurring the boundaries between composing/performing, curating/authoring, creating/interpreting. With MediaCycle, a framework for browsing media databases by similarity, we created several prototypes: a website for browsing dancers' identities through video recordings, a collaborative dancefloor for music creation.},
  keywords = {content-based similarity,digital art,information visualization,Media browsers,media content,multimedia browsers,new media arts,similarity}
}

@inproceedings{MediaCyclingTEI2013,
  title = {Designing {{Tangible}}/{{Free-form Applications}} for {{Navigation}} in {{Audio}}/{{Visual Collections}} (by {{Content-based Similarity}})},
  shorttitle = {{{MediaCycling}}},
  booktitle = {7th {{Conference}} on {{Tangible}}, {{Embedded}} \& {{Embodied Interaction}}},
  author = {Frisson, Christian},
  date = {2013-02-10/2013-02-13},
  series = {{{TEI}}'13},
  publisher = {{ACM}},
  location = {{Barcelona, Spain}},
  doi = {10.1145/2460625.2460686},
  abstract = {This paper focuses on one aspect of doctoral studies, within the last year of completion, consisting in designing applications for the navigation (by content-based similarity) in audio or video collections: the choice of tangible or free-form interfaces depending on use cases. One goal of this work is to determine which type of gestural interface suits best each chosen use case making use of navigation into media collections composed of audio or video elements, among: classifying sounds for electroacoustic music composition, derushing video, improvising instant music through an installation organizing and synchronizing audio loops. Prototype applications have been developed using the modular Media-Cycle framework for organization of media content by similarity. We conclude preliminarily that tangible interfaces are better-suited for focused expert tasks and free-form interfaces for multiple-user exploratory tasks, while a combination of both can create emergent practices.},
  isbn = {978-1-4503-1898-3},
  keywords = {free-form interfaces,information visualization,Interface design,multimedia browsers,multimedia content organization,tangible interfaces,tangible user interfaces}
}

@inproceedings{MediaHapticsIEEEICRA2019SoftHapticInteraction,
  title = {Towards {{Opensource Authoring Toolkits}} for {{Designing Soft}}+{{Stiff Haptic Interactions}}},
  shorttitle = {{{MediaHaptics}}},
  booktitle = {Workshop: {{Modeling}}, {{Design}} and {{Application}}},
  author = {Frisson, Christian and Gallacher, Colin and Wanderley, Marcelo M.},
  date = {2019-05},
  series = {{{IEEE ICRA}}'19 {{Soft Haptic Interaction}}},
  location = {{Montreal, Qc, Canada}},
  url = {https://frisson.re/MediaHapticsICRA2019SHI/},
  keywords = {force feedback,haptics,information visualization}
}

@unpublished{MediaMappingScriptingIEA2020,
  type = {Workshop Presentation},
  title = {Navigation Dans Des Collections Multimédia: De La Cartographie Vers l'écriture.},
  shorttitle = {{{MediaMappingScriptingIEA2020}}},
  author = {Frisson, Christian},
  date = {2020-11-19},
  series = {{{IEA}}'20},
  url = {https://frisson.re/MediaMappingScriptingIEA2020},
  abstract = {Nous partageons nos perspectives sur la navigation dans des collections de contenu multimédia, d'abord par la cartographie (ou mise en correspondance) entre caractéristiques des collections et des interactions; puis par le parcours de cheminements (ou séquences) que l'on pourrait pré-écrire.},
  eventtitle = {{{IEA Workshop}}: {{Écriture}} Du Temps et de l'interaction ({{CNRS-Inria LaBRI}} + {{McGill IDMIL}})},
  venue = {{Montreal, Qc, Canada + Bordeaux, France}},
  keywords = {information visualization,multimedia browsers}
}

@report{MedianeumNUMEDIART2012,
  title = {Medianeum: Crafting Interactive Timelines from Multimedia Content},
  shorttitle = {{{MedianeumNUMEDIART2012}}},
  author = {Zajéga, François and Picard, Cécile and René, Julie and Puleo, Antonin and Decuypere, Justine and Frisson, Christian and Ravet, Thierry and Mancas, Matei},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  date = {2012-06},
  number = {2},
  pages = {1--7},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers,timeline}
}

@inproceedings{MediaSurfacesACMITS2013CmIS,
  title = {Designing Artfully-Mediated Interactive Surfaces Organizing Media Collections},
  shorttitle = {{{MediaSurfaces}}},
  booktitle = {Interactive {{Tabletops}} and {{Surfaces Conf}}., {{Collaboration}} Meets {{Interactive Surfaces Workshop}}},
  author = {Frisson, Christian and Schayes, Eric and Uyttenhove, Simon and {Stéphane Dupont} and Giot, Rudi and Dutoit, Thierry},
  date = {2013-10-06},
  series = {{{ACM ITS}}'13 {{CmIS}}},
  location = {{St Andrews, Scotland, UK}},
  keywords = {interactive surfaces,multimedia browsers}
}

@artwork{MetaCrane,
  title = {Méta-{{Crâne}}},
  shorttitle = {{{MetaCrane}}},
  author = {Israel, Thomas},
  editora = {Frisson, Christian},
  editoratype = {collaborator},
  date = {2009},
  url = {http://www.thomasisrael.be/pf/meta-crane/},
  keywords = {multimedia browsers}
}

@inproceedings{MetropolitanViewsMMM2014Mediadrom,
  title = {Scenarizing {{Metropolitan Views}}: {{FlanoGraphing}} the {{Urban Spaces}}},
  shorttitle = {Metropolitan {{Views}}},
  booktitle = {20th {{International Conference}} on {{MultiMedia Modeling}}, {{Mediadrom}} Special Session},
  author = {Jacobs, Bénédicte and Jacobs, Laure-Anne and Frisson, Christian and Yvart, Willy and Dutoit, Thierry and Leleu-Merviel, Sylvie},
  date = {2014-01-07/2014-01-10},
  series = {{{MMM}}'14 {{Mediadrom}}},
  publisher = {{Springer}},
  location = {{Dublin, Ireland}},
  doi = {10.1007/978-3-319-04117-9_2},
  abstract = {The recent decade has seen a rapid evolution in the field of digital media. Mobile devices are now being integrated into every aspect of urban life. GPS, sensor technologies and augmented reality have transformed the new generation of mobile devices from a communication and information platform into a navigational tool, fostering new ways of perceiving reality and image building. Touch sensor technology has changed the screen into a joint input and display device. In this paper we present the FlanoGraph, an application for smartphones and tablets designed to take benefit of the changes induced by mobile devices. We first briefly outline the conceptual background, evoking the work of some researchers in the fields of 'Non Representational Theory', mobile media, and computational data processing. We then present and describe the FlanoGraph through a set of use cases. Finally, we conclude discussing some techniques necessary for the development of the application.},
  isbn = {978-3-319-04116-2},
  keywords = {abstracting technologies,data visualization,database management,FlanoGraph,gestural interaction,GPS,information retrieval,information visualization,navigation,sensing technologies,summarizing technologies,timeline,user interface design}
}

@unpublished{MobileImmersiveSpacesGI2022,
  title = {Creating {{Spontateous}} and {{Mobile Immersive Spaces}} Using {{FLOSS}}},
  shorttitle = {{{MobileImmersiveSpacesGI2022}}},
  author = {Durand, Emmanuel and Frisson, Christian and Seta, Michał},
  date = {2022},
  series = {{{GI}}'22},
  url = {https://gitlab.com/sat-metalab/workshops/GI2022-workshop/},
  abstract = {This workshop is aimed at artists, designers, content creators and other creatives who are interested in creating immersive spaces using low-cost and lightweight equipment. We will be using off-the-shelf hardware and open-source software to create interactive and immersive experiences, from scratch, in impromptu spaces. We will present the tools and the process for deploying immersive experiences: characterizing the available space, setting up projection mapping, calibrating video projectors and sound equipment, setting up cameras for real-time pose tracking and estimation, and employing a software pipeline to glue all components together. The workshop is based on using free/libre open-source software as much as possible, democratizing what's possible to do with computer vision, computer graphics and spatialized audio technologies that are still marginalized and on the fringes of creative activities. We will be exploring production pipelines that illustrate the use of software developed by the Metalab in tandem with existing tools that have been seeing wider adoption in recent years. For clarity, we present the proposed toolset in two groups: software developed by the Metalab and third-party software. Metalab software: LivePose - for interpreting live pose estimation using machine learning Splash - for video mapping SATIE - for sound rendering and spatialization Other possible software: Godot Game Engine - live 3D renderer for interactive content Chataigne - show control and "glue" for data mapping Ossia Score - show control and "glue" for data mapping The above software can run on low-cost, low-energy hardware such as Raspberry Pi and NVIDIA Jetson single board computers. We believe that putting such hardware and open-source software into creative hands can bring awe and wonder as much, if not more, than the technological bloat that we experience today. The call for participation to this workshop will emphasize first and foremost the collaboration, sharing and accessibility aspects from where an art piece can emerge from. Collaboration and sharing aspects are brought firstly by the creation of a single interactive and immersive experience, all participants being encouraged to come with ideas and content and to be open to the suggestions of other participants. Sharing and accessibility are brought by the use of technologies which are easy to obtain. In addition to cameras, videoprojectors and audio speakers, the workshop makes use of free software as well as low-cost hardware. Some of the software is already well known in the community (Godot, Chataigne), whereas our own software is not as much but it addresses use cases which are either not possible with other software or involve costly solutions. Plan of the workshop Introduction to the challenges of creating immersive artworks Summary overview of different solutions (software/hardware) to address the challenges Introduction to the Metalab software/hardware ecosystem Implementation of a simple immersive (audio-visual and interactive) installation with attendees participation Discussion Requirements a suitably sized room for the number of participants with controlled lighting (and away from windows). Ideally a dedicated corner of room to demonstrate setting up a video mapping onto non-planar surfaces controlled lighting or the ability to dim or switch off the lights to demonstrate video projections freedom to play sounds without disturbing others},
  eventtitle = {48th {{Graphics Interface Conference}}},
  venue = {{Montreal, QC, Canada}}
}

@report{MoViNUMEDIART2010,
  title = {{{MoVi}}: {{MediaCycle Audio}} and {{Visualization}} Improvements},
  shorttitle = {{{MoViNUMEDIART2010}}},
  author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Tardieu, Damien},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2010-03},
  number = {1},
  pages = {5--8},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers}
}

@unpublished{MulsemediaMaterialsCUTE2022,
  type = {Invited Talk},
  title = {Discover, Ideate, and Create Mulsemedia Materials},
  shorttitle = {{{MulsemediaMaterialsCUTE2022}}},
  author = {Frisson, Christian},
  date = {2022},
  series = {{{CUTE}}'22},
  url = {https://gitlab.com/sat-mtl/metalab/workshops/CUTE2022-workshop},
  eventtitle = {{{CUlture}} \& {{TEchnology}}},
  venue = {{Mons, Belgium}},
  keywords = {audio browsing,force feedback,force-feedback user interfaces,haptics,jog wheel,media browsing,rotary,tangible user interfaces}
}

@report{MultiMediaCycleNUMEDIART2009,
  title = {{{MultiMediaCycle}}: {{Consolidating}} the {{HyForge Framework}} towards {{Improved Scalability}} and {{Usability}}},
  shorttitle = {{{MultiMediaCycleNUMEDIART2009}}},
  author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Tardieu, Damien},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2009-12},
  number = {2},
  pages = {113--117},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,multimedia browsers}
}

@inproceedings{MultimodalGuitareNTERFACE2009,
  title = {Multimodal {{Guitar}}: {{Performance Toolbox}} and {{Study Workbench}}},
  shorttitle = {Multimodal {{Guitar}}},
  booktitle = {5th {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Frisson, Christian and Reboursière, Loïc and Chu, Wen-Yang and Lähdeoja, Otso and Mills III, John Anderson and Picard, Cécile and Shen, Ao and Todoroff, Todor},
  date = {2009-07-13/2009-08-08},
  series = {{{eNTERFACE}}'09},
  location = {{Genova, Italy}},
  abstract = {This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. We investigate two axes, 1) “A gestural/polyphonic sensing/processing toolbox to augment guitar performances”, and 2) “An interactive guitar score following environment for adaptive learning”. These approaches share quite similar technological challenges (sensing, analysis, processing, synthesis and interaction methods) and dissemination intentions (community-based, low-cost, open-source whenever possible), while leading to different applications (respectively artistic and educational), still targeted towards experienced players and beginners.},
  keywords = {audio synthesis,Audio- and polyphonic multi-pitch transcription,digital audio effects,digital musical instruments,finger tracking,guitar score following,multimodal fusion,multimodal interaction and gestural sensing,particle filtering}
}

@unpublished{MultimodalGuitarJEIGE2009,
  type = {Workshop Presentation},
  title = {Objectifs Du Projet {{Multimodal Guitar}}},
  shorttitle = {{{MultimodalGuitarJEIGE2009}}},
  author = {Frisson, Christian and Reboursière, Loïc},
  date = {2009-05-18/2009-05-19},
  series = {{{JEIGE}}'09},
  eventtitle = {Journées d'{{Étude Identités}} de La {{Guitare Électrique}}},
  venue = {{Maison des Sciences de l'Homme Paris-Nord, Saint Denis, France}},
  keywords = {digital musical instruments,guitar,multimodal,PureData}
}

@inproceedings{MultimodalGuitarNIME2010,
  title = {{{MultimodalGuitar}}: A {{Toolbox}} for {{Augmented Guitar Performances}}},
  shorttitle = {{{MultimodalGuitar}}},
  booktitle = {10th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Reboursière, Loïc and Frisson, Christian and Lähdeoja, Otso and Mills III, John Anderson and Picard, Cécile and Todoroff, Todor},
  date = {2010-06-15/2010-06-18},
  series = {{{NIME}}'10},
  location = {{Sydney, Australia}},
  doi = {10.5281/zenodo.1177881},
  abstract = {This project aims at studying how recent interactive and interactions technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed and developed a freely-available toolbox for multimodal guitar performances, compliant with the PureData and Max/MSP modular environments for a more widespread use, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing, modal synthesis, infinite sustain, rearranged looping and “smart” harmonizing.},
  isbn = {978-0-646-53482-4},
  keywords = {audio synthesis,Augmented guitar,digital audio effects,digital musical instruments,gestural sensing,hexaphonic guitar,multimodal interaction,polyphonic transcription,PureData}
}

@report{MultimodalGuitarNUMEDIART2009,
  title = {Multimodal {{Guitar}}: {{Performance Toolbox}} and {{Study Workbench}}},
  shorttitle = {{{MultimodalGuitarNUMEDIART2009}}},
  author = {Frisson, Christian and Reboursière, Loïc and Chu, Wen-Yang and Lähdeoja, Otso and III, John Anderson Mills and Picard, Cécile and Shen, Ao and Todoroff, Todor},
  editora = {Dutoit, Thierry and Macq, Benoît},
  editoratype = {collaborator},
  date = {2009-09},
  number = {3},
  pages = {67--84},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {digital musical instruments,PureData}
}

@thesis{OnurFerhatPhDUAB2017,
  type = {phdthesis},
  title = {A {{Gaze Estimation Method}} and {{System}} for {{Natural Light Cameras}}.},
  shorttitle = {{{OnurFerhatPhDUAB2017}}},
  author = {Ferhat, Onur},
  editora = {Vilariño, Fernando and Villanueva, Arantzazu and Karatzas, Dimosthenis and Parraga, Carlos Alejandro and Gomez Bigorda, Luis and Super, Hans},
  editoratype = {collaborator},
  date = {2017},
  institution = {{Universitat Autònoma Barcelona}},
  location = {{Inria, France}},
  keywords = {computer vision,eye tracking}
}

@unpublished{OpenForceFeedbackHAID2019,
  type = {Workshop Presentation},
  title = {Open Technologies for Force-Feedback in Artistic Creation},
  shorttitle = {{{OpenForceFeedbackHAID2019}}},
  author = {Sinclair, Stephen and Leonard, James and Villeneuve, Jérôme and Frisson, Christian},
  date = {2019-03},
  series = {{{HAID}}'19},
  url = {https://frisson.re/OpenForceFeedbackHAID2019/},
  eventtitle = {9th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  venue = {{Lille, France}},
  keywords = {digital musical instruments,force feedback,haptics,information visualization,multimedia browsers}
}

@thesis{PedroAndradeFerreiraSobrinhoGSoC2023,
  type = {GSoC},
  title = {Building {{Immersive Learning Experiences}}: {{Explorable}} Explanations for Teaching Digital Arts Concepts in Hybrid Telepresence Campus},
  shorttitle = {{{PedroAndradeFerreiraSobrinhoGSoC2023}}},
  author = {Andrade Ferreira Sobrinho, Pedro},
  editora = {Fardon, Rochana and Frisson, Christian},
  editoratype = {collaborator},
  date = {2023},
  institution = {{State University of Campinas in Brazil + Telecom Paris}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-proposals/proposal-pedroansa1-hybrid-telepresence-campus/},
  abstract = {The proposed project aims to support teaching in hybrid telepresence settings by developing and embedding examplar explorable explanations into the Satellite hub, an immersive 3D social web environment developed at the Society for Arts and Technology (SAT) using Mozilla Hubs. Explorable explanations are interactive documents that use visualizations, animations, and simulations to help learners understand complex concepts. The project will integrate explorable explanations in Mozilla Hubs used for Satellite by adapting Spoke to defining dynamic assets with interactivity. This integration will allow for the creation of immersive and interactive educational environments that cater to diverse abilities in learning and allow learners to play with parameters to deepen their understanding of the concepts. The project seeks to create synergy by interconnecting various cultural and artistic contents in the Satellite hub and embedding explorable explanations to enhance the learning experience. The explorable explanations will be designed to cater to a broad audience and facilitate access to learning. The project will involve prototyping examplar explorable explanations and embedding them in Satellite / Mozilla Hubs, leveraging the capabilities of the platform to provide an immersive learning experience. The aim is to enhance the teaching of digital arts concepts in hybrid telepresence campus settings and promote interactivity and innovation in teaching, leveraging emerging technologies to enhance the learning experience. Ultimately, the project will contribute to promoting interactivity and innovation in teaching, leveraging emerging technologies to enhance the learning experience.}
}

@inproceedings{PhysioContenteNTERFACE2007,
  title = {Audiovisual {{Content Generation Controlled}} by {{Physiological Signals}} for {{Clinical}} and {{Artistic Applications}}},
  shorttitle = {{{PhysioContent}}},
  booktitle = {3rd {{Summer School}} on {{Multimodal Interfaces}}},
  author = {Benovoy, Mitchel and Brouse, Andrew and Corcoran, Thomas Greg and Drayson, Hannah and Erkut, Cumhur and Filatriau, Jean-Julien and Frisson, Christian and Gundogdu, Umut and Knapp, Ben and Lehembre, Rémy and Mühl, Christian and Pérez, Miguel Angel Ortiz and Sayin, Alaattin and Soleymani, Mohammad and Tahiroğlu, Koray},
  date = {2007-07-16/2007-08-10},
  series = {{{eNTERFACE}}'07},
  pages = {103--116},
  location = {{Istanbul, Turkey}},
  url = {http://www.cmpe.boun.edu.tr/enterface07/outputs/final/p8report.pdf},
  abstract = {While an extensive palette of sound and visual generation techniques have been developed during the era of digital signal processing, the design of innovative virtual instruments has come to dramatic fruition over the last decade. The use of measured biological signals to drive these instruments proposes some new and powerful tools for clinical, scientific and artistic applications. Over the period of one month - during the eNTERFACE’07 summer workshop in  ̇Istanbul, Turkey - researchers from the fields of human-computer interfaces, sound synthesis and new media art worked together towards this common goal. A framework for auditory display and bio-musical applications was established upon which were based different experimental prototypes. Diverse methods for the analysis of measured physiological signals and of mapping the extracted parameters to sound and visual synthesis processes were explored. Biologically-driven musical instruments and data displays for clinical and medical purposes were built. From this have emerged some worthwhile perspectives on future research. This report summarises the results of that project.},
  keywords = {Auditory display,Biologically-augmented performances,Biosignals,Brain-computer interfaces,digital musical instruments,Interactive arts,Multimodal interfaces,Sonification}
}

@thesis{PierreTalbotBEngMcGill2020,
  type = {BEng},
  title = {Analysis and Processing of a Vibrating String on Embedded Platforms ({{Raspberry Pi}})},
  shorttitle = {{{PierreTalbotBEngMcGill2020}}},
  author = {Talbot, Pierre},
  date = {2020},
  institution = {{McGill University}},
  location = {{IDMIL}}
}

@thesis{PierreTalbotBEngMcGill2021,
  type = {BEng},
  title = {{{RotaRep}}: Replication of {{TorqueTuner}}, Self-Contained Module for Rotary Haptic Force Feedback in Digital Musical Instruments},
  shorttitle = {{{PierreTalbotBEngMcGill2021}}},
  author = {Talbot, Pierre},
  date = {2021},
  institution = {{McGill University}},
  location = {{IDMIL}}
}

@unpublished{PlancherHaptiqueAcfas2023,
  title = {L’écosystème Créatif Autour Du {{Plancher Haptique}} de La {{Société}} Des Arts Technologiques.},
  shorttitle = {{{PlancherHaptiqueAcfas2023}}},
  author = {Frisson, Christian},
  date = {2023-05-08},
  series = {Acfas'23},
  url = {https://www.acfas.ca/evenements/congres/programme-preliminaire/600/624},
  abstract = {Nous allons vous présenter l’écosystème créatif autour du Plancher Haptique (PH): un affichage pour le sens du toucher créé à la Société des arts technologiques (SAT), une organisation à but non lucratif dédiée au développement et au soutien de la culture numérique. Le PH et ses outils d’auteurisation permettent de concevoir et de diffuser des stimulations vibratoires et kinesthésiques pour augmenter l’immersion des personnes participant à des installations et performances d’arts numériques. Une singularité du PH réside dans sa création à travers un terrain de recherche intersectorielle et interdisciplinaire, liant les milieux du divertissement, génie et création; et associant des experts en art, science et technologies. Nous allons vous illustrer cette singularité en vous présentant: - les départements d’innovation de la SAT dont le Metalab et la Valorisation dans lesquels le PH est incubé - la collaboration avec le CTA de l’Université de Sherbrooke pour la caractérisation mécanique et l’industrialisation du PH - deux résidences artistiques de l’été 2022 focalisées sur le PH: feuloured par Michał Seta et Ressenti par Samuel Vignola et son équipe - les pistes de deux stages de 2022-2023 par Valentine Auphan (2nd cycle en conception graphique à l’École nationale supérieure de Arts Décoratifs) qui conçoit des visualisations interactives pour le diagnostic du PH et la composition avec le PH, et par Raphaël Suzor-Schröder (1er cycle en génie mécanique à l’Université McGill) qui crée une version réduite transportable du PH pour en explorer les espaces de travail mécanique et de conception d’interactions.},
  eventtitle = {90e {{Congrès}} de l'{{Acfas}}, {{Section}} 600 - {{Colloques}} Multisectoriels, {{Colloque}} 624 - {{R}}\&{{D}} et {{R}}\&{{C}} : {{Les}} Formes de Collaborations Intersectorielles Dans Les Industries Créatives},
  venue = {{Montréal, QC, Canada}}
}

@unpublished{PolyMTLINF8808Winter2021EDI,
  type = {Guest Lecture},
  title = {Visualisation d'information ↔ {{Equité}}, Diversité et Inclusion},
  shorttitle = {{{PolyMTLINF8808Winter2021EDI}}},
  author = {{Christian Frisson}},
  editora = {Hurtut, Thomas},
  editoratype = {collaborator},
  date = {2021-02-25},
  url = {https://frisson.re/2021-PolyMTL-INF8808-EDI/},
  abstract = {Presentation (February 25th, 50 min) on Equity, Diversity and Inclusion ↔ (in/with) InfoVis.},
  eventtitle = {{{INF8808 Winter}} 2021 {{Data Visualization}}},
  venue = {{Polytechnique Montréal}},
  keywords = {information visualization}
}

@inproceedings{PrintgetsHAID2020,
  title = {Printgets: An {{Open-Source Toolbox}} for {{Designing Vibrotactile Widgets}} with {{Industrial-Grade Printed Actuators}} and {{Sensors}}},
  shorttitle = {Printgets},
  booktitle = {10th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  author = {Frisson, Christian and Decaudin, Julien and Sanz-Lopez, Mario and Pietrzak, Thomas},
  date = {2020-08-21},
  series = {{{HAID}}'20},
  url = {https://hal.archives-ouvertes.fr/hal-02901202},
  urldate = {2020-09-09},
  abstract = {New technologies for printing sensors and actuators combine the flexibility of interface layouts of touchscreens with localized vibrotactile feedback, but their fabrication still requires industrial-grade facilities. Until these technologies become easily replicable, interaction designers need material for ideation. We propose an open-source hardware and software toolbox providing maker-grade tools for iterative design of vibrotactile widgets with industrial-grade printed sensors and actuators. Our hardware toolbox provides a mechanical structure to clamp and stretch printed sheets, and electronic boards to drive sensors and actuators. Our software toolbox expands the design space of haptic interaction techniques by reusing the wide palette of available audio processing algorithms to generate real-time vibrotactile signals. We validate our toolbox with the implementation of three exemplar interface elements with tactile feedback: buttons, sliders, touchpads.},
  eventtitle = {{{HAID}} 2020 - {{International Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  langid = {english},
  keywords = {3d printing,haptics,piezoelectric ink,printed actuators,printed electronics,printed sensors,PureData,vibrotactile}
}

@report{PrintgetsHAPPINESS2017,
  title = {Guidelines for {{Next Generation Haptic Demonstrator}}},
  shorttitle = {{{PrintgetsHAPPINESS2017}}},
  author = {Frisson, Christian and Decaudin, Julien and Lopez, Mario Sanz and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Latour, Antoine},
  date = {2017-12},
  number = {D.17},
  institution = {{HAPPINESS EU H2020 645145}},
  url = {https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5b760277f&appId=PPGMS},
  keywords = {haptics,PureData}
}

@thesis{RaphaelSuzorSchroderSAT2022,
  type = {BEng},
  title = {Development of an Open-Source Toolkit for Creating Deformable Haptic Surfaces},
  shorttitle = {{{RaphaelSuzorSchroderSAT2022}}},
  author = {Suzor-Schröder, Raphaël},
  date = {2022},
  institution = {{McGill University}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2021-09-interns/deformable-haptic-surfaces/}
}

@thesis{RemyLabbeMScUCLouvain2009,
  type = {MSc},
  title = {Sound Spatialisation through Multi-Camera Analysis of Gestures},
  shorttitle = {{{RemyLabbeMScUCLouvain2009}}},
  author = {Labbé, Rémy},
  editora = {Macq, Benoît and Filatriau, Jean-Julien and Frisson, Christian and Van Brussel, Christian},
  editoratype = {collaborator},
  date = {2009},
  institution = {{Université catholique de Louvain, Belgium}},
  location = {{Université catholique de Louvain, Belgium}},
  keywords = {computer vision,digital musical instruments,PureData,sound spatialization}
}

@software{RepHap,
  title = {{{RepHap}}: An Open Source Platform for Benchmarking Haptic Devices Leveraging the {{Robot Operating System}} Ecosystem},
  shorttitle = {{{RepHap}}},
  author = {Frisson, Christian},
  date = {2019/2020},
  url = {https://github.com/RepHap},
  keywords = {3d printing,force feedback,haptics,robotics}
}

@inproceedings{RepHapHAPTICS2020WIP,
  title = {{{RepHap}}: Towards an Open Source Platform for Benchmarking Haptic Devices Leveraging the {{Robot Operating System}} Ecosystem},
  shorttitle = {{{RepHap}}},
  booktitle = {Haptics {{Symposium}}, {{Works}} in {{Progress}}},
  author = {Frisson, Christian and Delbos, Benjamin and Désourdy, Félix and Ding, Steve and Wanderley, Marcelo M. and Lévesque, Vincent and Gallacher, Colin},
  date = {2020},
  series = {{{HAPTICS}}'20 {{WIP}}},
  publisher = {{IEEE}},
  location = {{Washington, DC, USA}},
  keywords = {3d printing,force feedback,haptics,robotics}
}

@report{RT-MediaCycleNUMEDIART2011,
  title = {{{RT-MediaCycle}}: {{Towards}} a Real-Time Use of {{MediaCycle}} in Performances and Video Installations},
  shorttitle = {{{RT-MediaCycleNUMEDIART2011}}},
  author = {Siebert, Xavier and Dupont, Stéphane and Frisson, Christian and Delcourt, Bernard},
  editora = {Dutoit, Thierry},
  editoratype = {collaborator},
  date = {2011-09},
  number = {3},
  pages = {55--58},
  institution = {{numediart Research Program on Digital Art Technologies}},
  keywords = {information visualization,PureData}
}

@inproceedings{SketchingImmersivePipelinesSIGGRAPH2023,
  title = {Sketching {{Pipelines}} for {{Ephemeral Immersive Spaces}}},
  shorttitle = {{{SketchingImmersivePipelines}}},
  booktitle = {{{ACM SIGGRAPH}} 2023 {{Labs Hands-On Class}}},
  author = {Frisson, Christian and Meneses, Eduardo A. L. and Seta, Michał and Durand, Emmanuel},
  date = {2023},
  series = {{{SIGGRAPH}}'23},
  publisher = {{ACM}}
}

@article{SofaModalJASP2010,
  title = {Advances in {{Modal Analysis Using}} a {{Robust}} and {{Multiscale Method}}},
  shorttitle = {{{SofaModal}}},
  author = {Picard, Cécile and Frisson, Christian and Faure, François and Drettakis, George and Kry, Paul G.},
  date = {2010-02},
  journaltitle = {EURASIP J. Adv. Signal Process},
  series = {{{JASP}}'10},
  volume = {2010},
  issn = {1110-8657},
  doi = {10.1155/2010/392782},
  abstract = {This paper presents a new approach to modal synthesis for rendering sounds of virtual objects. We propose a generic method that preserves sound variety across the surface of an object at different scales of resolution and for a variety of complex geometries. The technique performs automatic voxelization of a surface model and automatic tuning of the parameters of hexahedral finite elements, based on the distribution of material in each cell. The voxelization is performed using a sparse regular grid embedding of the object, which permits the construction of plausible lower resolution approximations of the modal model. We can compute the audible impulse response of a variety of objects. Our solution is robust and can handle nonmanifold geometries that include both volumetric and surface parts. We present a system which allows us to manipulate and tune sounding objects in an appropriate way for games, training simulations, and other interactive virtual environments.},
  keywords = {audio synthesis,finite element modeling,modal analysis,modal synthesis,PureData}
}

@inproceedings{SonixCycleAM2016,
  title = {A {{Semantic}} and {{Content-Based Search User Interface}} for {{Browsing Large Collections}} of {{Foley Sounds}}},
  shorttitle = {{{SonixCycle}}},
  booktitle = {11th {{AudioMostly Conference}} on {{Interaction}} with {{Sound}}},
  author = {Urbain, Gabriel and Frisson, Christian and Moinet, Alexis and Dutoit, Thierry},
  date = {2016},
  series = {{{AM}} '16},
  pages = {272--277},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2986416.2986436},
  url = {http://doi.acm.org/10.1145/2986416.2986436},
  urldate = {2016-11-12},
  abstract = {Sound designers select the sounds they use among massive collections of recordings. They usually rely on text-based queries to narrow down a subset from these collections when looking for specific content. However, when it comes to unknown collections, this approach can fail to precisely retrieve files according to their content. We investigate an audio search engine that associates content-based features and semantic meta-data using Apache Solr deployed in a fully integrated server architecture. In order to facilitate the task of browsing the sounds, we also propose a search user interface in which the user can perform both text-based queries and visual browsing in a window where sounds are organized according to their audio features. A preliminary evaluation of the performances helped to optimize the parameters of the system.},
  isbn = {978-1-4503-4822-5},
  keywords = {information visualization,multimedia browsers,multimedia information retrieval,search user interfaces,sound effects}
}

@patent{SonixCyclePatentWO2017,
  type = {patent},
  title = {Audio Search User Interface},
  shorttitle = {{{SonixCyclePatentWO2020}}},
  author = {Urbain, Gabriel and Moinet, Alexis and Frisson, Christian},
  holder = {{Universite De Mons}},
  date = {2017-09-21},
  number = {WO2017158159A1},
  location = {{WO}},
  url = {https://patents.google.com/patent/WO2017158159A1/},
  urldate = {2020-12-18},
  langid = {english},
  keywords = {audio,descriptor,file,index,information visualization,perceptual}
}

@inproceedings{SplashColorimetryACMVRST2022,
  title = {Colorimetry {{Evaluation}} for {{Video Mapping Rendering}}},
  shorttitle = {{{SplashColorimetry}}},
  booktitle = {28th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Décorps, Eva and Frisson, Christian and Durand, Emmanuel},
  date = {2022},
  series = {{{ACM VRST}}'22},
  doi = {10.1145/3562939.3565684}
}

@inproceedings{StereoHapticsSIGGRAPH2016,
  type = {Tutorial},
  title = {Stereohaptics: {{A Haptic Interaction Toolkit}} for {{Tangible Virtual Experiences}}},
  shorttitle = {{{StereoHaptics}}},
  booktitle = {{{ACM SIGGRAPH}} 2016 {{Studio}}},
  author = {Israr, Ali and Zhao, Siyan and McIntosh, Kyna and Schwemler, Zachary and Fritz, Adam and Mars, John and Bedford, Job and Frisson, Christian and Huerta, Ivan and Kosek, Maggie and Koniaris, Babis and Mitchell, Kenny},
  date = {2016},
  series = {{{SIGGRAPH}}'16},
  publisher = {{ACM}},
  location = {{Anaheim, California, USA}},
  doi = {10.1145/2929484.2970273},
  abstract = {With a recent rise in the availability of affordable head mounted gear sets, various sensory stimulations (e.g., visual, auditory and haptics) are integrated to provide seamlessly embodied virtual experience in areas such as education, entertainment, therapy and social interactions. Currently, there is an abundance of available toolkits and application programming interfaces (APIs) for generating the visual and audio content. However, such richness in hardware technologies and software tools is missing in designing haptic experiences. Current solutions to integrate haptic effects are limited due to: i) a user's rigid adaptation to new hardware and software technologies, ii) limited scalability of the existing tools to incorporate haptic hardware and applications, iii) inflexible authoring capabilities, iv) missing infrastructure for storing, playback and distribution, and v) and unreliable hardware for long term usage.},
  isbn = {978-1-4503-4373-2},
  keywords = {audio signal processing,Haptics,PureData,vibrotactile}
}

@inproceedings{TangibleHaystackTEI2014,
  title = {Tangible {{Needle}}, {{Digital Haystack}}: {{Tangible Interfaces}} for {{Reusing Media Content Organized}} by {{Similarity}}},
  shorttitle = {{{TangibleHaystack}}},
  booktitle = {8th {{Conference}} on {{Tangible}}, {{Embedded}} \& {{Embodied Interaction}}},
  author = {Frisson, Christian and Rocca, François and Dupont, Stéphane and Dutoit, Thierry and Grobet, Damien and Giot, Rudi and El Brouzi, Mohammed and Bouaziz, Samir and Yvart, Willy and Merviel, Sylvie},
  date = {2014},
  series = {{{TEI}}'14},
  publisher = {{ACM}},
  location = {{Munich, Germany}},
  doi = {10.1145/2540930.2540983},
  abstract = {This paper presents the design process of a desk-set tangible user interface for the navigation and manipulation of media content organized by content-based similarity with off-the-shelf/flea market devices. For intra-media navigation, a refurbished portable vinyl player has its inside mechanics replaced by a webcam monitoring circular gray code analyzed through computer vision for position/speed tracking. For inter-media navigation, a 3D force-feedback controller is mounted in upright position on a truss with cell clamps, repurposed as trackpad. For media recomposition, motorized faders recall the effect presets of the closest/last selected media item.},
  isbn = {978-1-4503-2635-3},
  keywords = {content-based similarity,contextual inquiry,force feedback,force-feedback user interfaces,haptics,information visualization,known-item search,multimedia browsers,tangible interfaces,tangible user interfaces}
}

@thesis{TarekYasserGSoC2023,
  type = {GSoC},
  title = {Replace {{OpenGL}} by a Multi-{{API}} Rendering Library in {{Splash}}},
  shorttitle = {{{TarekYasserGSoC2023}}},
  author = {Yasser, Tarek},
  editora = {Durand, Emmanuel and Celerier, Jean-Michaël and Frisson, Christian},
  editoratype = {collaborator},
  date = {2023},
  institution = {{Cairo University, Egypt}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2023-proposals/proposal-knockerpulsar-replace-opengl-by-a-multi-api-rendering-library-in-splash/},
  abstract = {The Splash projection mapping software allows for controlling multiple videoprojectors together to build a single projection. It is able to adapt to virtually any real geometry, as long as the surface is diffuse. Its rendering engine is built using the OpenGL API. It currently runs on platforms capable of runing Linux and handling an OpenGL 4.5 context, and has been tested successfully on x86\_64 (with NVIDIA, AMD and Intel graphic cards) and aarch64 (with NVIDIA graphic cards) However to be able to a) optimize it further and b) support more platforms (for example Raspberry Pi), it would be interesting to support more graphics API. To do this it is envisionned to replace direct use of OpenGL with an intermediate, multi-API rendering library. For now bgfx is considered.}
}

@artwork{TheListeningRoom,
  title = {The {{Listening Room}}},
  shorttitle = {{{TheListeningRoom}}},
  author = {Frisson, Christian},
  date = {2012},
  keywords = {digital musical instruments,multimedia browsers}
}

@thesis{TinaHuynhBScUofC2018,
  type = {BSc},
  title = {Discovering the {{Data Mapping}} of an {{Unfamiliar Visualization}}},
  shorttitle = {{{TinaHuynhBScUofC2018}}},
  author = {Huynh, Tina},
  date = {2018},
  institution = {{University of Calgary}},
  location = {{iLab}}
}

@inproceedings{TorqueTunerHAID2022,
  title = {{{TorqueTuner}}: A {{Case Study}} for {{Sustainable Haptic Development}}},
  shorttitle = {{{TorqueTuner}}},
  booktitle = {11th {{Intl}}. {{Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  author = {Niyonsenga, Albert-Ngabo and Frisson, Christian and Wanderley, Marcelo M.},
  date = {2022},
  series = {{{HAID}}'22},
  eventtitle = {{{HAID}} 2022 - {{International Workshop}} on {{Haptic}} and {{Audio Interaction Design}}},
  langid = {english},
  keywords = {3d printing,haptics,piezoelectric ink,printed actuators,printed electronics,printed sensors,PureData,vibrotactile}
}

@inproceedings{TorqueTunerNIME2020,
  title = {{{TorqueTuner}}: {{A}} Self Contained Module for Designing Rotary Haptic Force Feedback for Digital Musical Instruments},
  shorttitle = {{{TorqueTuner}}},
  booktitle = {20th {{Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Kirkegaard, Mathias and Bredholt, Mathias and Frisson, Christian and Wanderley, Marcelo M.},
  date = {2020-07-21/2020-07-25},
  series = {{{NIME}}'20},
  location = {{Birmihgham, UK}},
  doi = {10.5281/zenodo.4813359},
  url = {https://www.nime.org/proceedings/2020/nime2020_paper52.pdf},
  keywords = {3d printing,digital musical instruments,force feedback,haptics,information visualization,mapping,sound synthesis}
}

@thesis{ValentineAuphanSAT2022,
  type = {MArts},
  title = {{{UI}}/{{UX}} Design for Authoring Haptic Floor Interactions},
  shorttitle = {{{ValentineAuphanSAT2022}}},
  author = {Auphan, Valentine},
  date = {2022},
  institution = {{École nationale supérieure des Arts Décoratifs}},
  location = {{SAT, Metalab}}
}

@report{VibroTactileWidgetsHAPPINESS2016,
  title = {Interaction Techniques Leveraging Haptic Feedback on New Interactive Surfaces (Intermediate)},
  shorttitle = {{{VibroTactileWidgetsHAPPINESS2016}}},
  author = {Frisson, Christian and Decaudin, Julien and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Gaffary, Yoren and Lecuyer, Anatole and Latour, Antoine},
  date = {2016-12},
  number = {D2.3},
  institution = {{HAPPINESS EU H2020 645145}},
  keywords = {haptics,PureData}
}

@report{VibroTactileWidgetsHAPPINESS2017,
  title = {Interaction Techniques Leveraging Haptic Feedback on New Interactive Surfaces (Final)},
  shorttitle = {{{VibroTactileWidgetsHAPPINESS2017}}},
  author = {Frisson, Christian and Decaudin, Julien and Lopez, Mario Sanz and Pietrzak, Thomas and Ng, Alexander and Brewster, Stephen and Gaffary, Yoren and Lecuyer, Anatole and Latour, Antoine},
  date = {2017-12},
  number = {D2.4},
  institution = {{HAPPINESS EU H2020 645145}},
  keywords = {haptics,PureData}
}

@inproceedings{VibroTactileWidgetsUIST2017Adjunct,
  title = {Designing {{Vibrotactile Widgets}} with {{Printed Actuators}} and {{Sensors}}},
  shorttitle = {{{VibroTactileWidgets}}},
  booktitle = {30th {{Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Frisson, Christian and Decaudin, Julien and Pietrzak, Thomas and Ng, Alexander and Poncet, Pauline and Casset, Fabrice and Latour, Antoine and Brewster, Stephen A.},
  date = {2017},
  series = {{{UIST}}'17 {{Adjunct}}},
  pages = {11--13},
  publisher = {{ACM}},
  location = {{Quebec, Qc, Canada}},
  doi = {10.1145/3131785.3131800},
  abstract = {Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.},
  isbn = {978-1-4503-5419-6},
  keywords = {3d printing,haptics,piezoelectric ink,printed actuators,printed electronics,printed sensors,PureData,thin-film actuators,vibrotactile,widgets}
}

@thesis{VictorRiosSAT2022,
  type = {MEng},
  title = {Integration of Pose Estimation and Tracking for Multiperson Videos},
  shorttitle = {{{VictorRiosSAT2022}}},
  author = {Rios, Victor},
  date = {2023},
  institution = {{Ecole de Technologie Supérieure}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2022-ideas/idea-integration-of-pose-estimation-and-tracking/}
}

@article{VideoBrowserShowdownIJMIR2014,
  title = {The {{Video Browser Showdown}}: A Live Evaluation of Interactive Video Search Tools},
  shorttitle = {{{VideoBrowserShowdown}}},
  author = {Schoeffmann, Klaus and Ahlström, David and Bailer, Werner and Cobârzan, Claudiu and Hopfgartner, Frank and McGuinness, Kevin and Gurrin, Cathal and Frisson, Christian and Le, Duy-Dinh and Fabro, Manfred and Bai, Hongliang and Weiss, Wolfgang},
  date = {2014},
  journaltitle = {International Journal of Multimedia Information Retrieval},
  series = {{{IJMIR}}'14},
  volume = {3},
  number = {2},
  pages = {1--15},
  doi = {10.1007/s13735-013-0050-8},
  abstract = {The Video Browser Showdown evaluates the performance of exploratory video search tools on a common data set in a common environment and in presence of the audience. The main goal of this competition is to enable researchers in the field of interactive video search to directly compare their tools at work. In this paper, we present results from the second Video Browser Showdown (VBS2013) and describe and evaluate the tools of all participating teams in detail. The evaluation results give insights on how exploratory video search tools are used and how they perform in direct comparison. Moreover, we compare the achieved performance to results from another user study where 16 participants employed a standard video player to complete the same tasks as performed in VBS2013. This comparison shows that the sophisticated tools enable better performance in general, but for some tasks common video players provide similar performance and could even outperform the expert tools. Our results highlight the need for further improvement of professional tools for interactive search in videos.},
  keywords = {Exploratory search,information visualization,multimedia browsers,Video browsing,Video retrieval,Video search}
}

@inproceedings{VideoCycleMMM2013VBS,
  title = {{{VideoCycle}}: {{User-Friendly Navigation}} by {{Similarity}} in {{Video Databases}}},
  shorttitle = {{{VideoCycle}}},
  booktitle = {19th {{International Conference}} on {{MultiMedia Modeling}}, {{Video Browser Showdown}} Special Session},
  author = {Frisson, Christian and Dupont, Stéphane and Moinet, Alexis and Picard, Cécile and Ravet, Thierry and Siebert, Xavier and Dutoit, Thierry},
  date = {2013-01-07/2013-01-09},
  series = {{{MMM}}'13 {{VBS}}},
  pages = {550--553},
  publisher = {{Springer}},
  location = {{Huangshan, China}},
  doi = {10.1007/978-3-642-35728-2_66},
  abstract = {VideoCycle is a candidate application for this second Video Browser Showdown challenge. VideoCycle allows interactive intra-video and inter-shot navigation with dedicated gestural controllers. MediaCycle, the framework it is built upon, provides media organization by similarity, with a modular architecture enabling most of its workflow to be performed by plugins: feature extraction, clustering, segmentation, summarization, intra-media and inter-segment visualization. MediaCycle focuses on user experience with user interfaces that can be tailored to specific use cases.},
  keywords = {clustering,feature extraction,information visualization,jog wheel,known-item search,Media browsers,multimedia browsers,multimedia information retrieval,timeline}
}

@unpublished{VideodromeINTETAIN2013,
  type = {Workshop Presentation},
  title = {Videodrome: A {{Timeline}} of {{Linked Media Art}}/{{Science}}/{{Technology Events}}},
  shorttitle = {{{VideodromeINTETAIN2013}}},
  author = {Frisson, Christian},
  date = {2013-07-03/2013-07-05},
  series = {{{INTETAIN}}'13},
  url = {https://cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=1NSouRUMxc2KJ5AowT463ae7rOULoTwuSVR6__ZNISlw&font=Default&lang=en&initial_zoom=2&height=650},
  eventtitle = {5th {{Intl}}. {{Conf}}. on {{Intelligent Technologies}} for {{Interactive Entertainment}}, {{LinkedTV Demo Session}}},
  venue = {{Mons, Belgium}},
  keywords = {linked tv,multimedia browsers,timeline}
}

@inproceedings{WebAudioHapticsWAC2016,
  title = {{{WebAudioHaptics}}: {{Tutorial}} on {{Haptics}} with {{Web Audio}}},
  shorttitle = {{{WebAudioHaptics}}},
  booktitle = {2nd {{Web Audio Conference}}},
  author = {Frisson, Christian and Pietrzak, Thomas and Zhao, Siyan and Israr, Ali},
  date = {2016},
  series = {{{WAC}}'16},
  location = {{Atlanta, Georgia, USA}},
  url = {https://WebAudioHaptics.github.io},
  abstract = {The Web Audio Haptics WAC Tutorial 2016 will explore how to create meaningful haptic content that engages different areas of the body using off-the-shelf hardware and open source software running on a web browser using Web Audio technologies. Participants will 1) learn the basic theories of tactile illusions; 2) get an overview on actuators and sensors; 3) explore tactile illusions using web-based audio tools and a box connecting actuators and sensors to their computer audio I/O; and 4) ideate use cases in groups. Tutorial material will remain available from: https://github.com/WebAudioHaptics},
  keywords = {audio signal processing,Haptics,PureData,vibrotactile,Web Audio}
}

@thesis{YashRajGSoC2022,
  type = {GSoC},
  title = {Create a {{WebUI}} for Data Mapping and Software Control in Embedded Systems},
  shorttitle = {{{YashRajGSoC2022}}},
  author = {Raj, Yash},
  editora = {Meneses, Eduardo and Frisson, Christian and Valentin, Laurent},
  editoratype = {collaborator},
  date = {2022},
  institution = {{Indian Institute of Technology (BHU), India}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/collaborations/google-summer-of-code/posts/2022-proposals/proposal-tiger-yash-webui-for-data-mapping-and-software-control/},
  abstract = {Open Source tools, such as those developed by SAT, have been widely used for a variety of applications by academics and independent users. The tools benefit users a lot and make their work easier, more efficient, and less stressful, but most of them need lengthy setups and installations, which are not very user-friendly. This GSoC project attempts to integrate tools like SATIE and LivePose into a bespoke WebUI and make them accessible. The UI will be responsible for software configuration, deploying an audio synthesizer on the back-end, etc. The WebUI will be capable of deploying the tools (start/stop LivePose and SATIE, control Jack Audio parameters, and request reboot/service restart) and basically remotely launching (and possibly integrating and mapping) SAT tools in embedded systems based on the Raspberry Pi generic computer running Linux. Any program or product's goal is to provide users with a rich and interactive experience; similarly, our WebUI will serve as an entry point to those tools with simple customizations that will make users' life much easier.}
}

@thesis{YassinAkbibSAT2023,
  type = {MEng},
  title = {Development of a Plugin for a Bibliography Management Tool to Facilitate Exploring Trends},
  shorttitle = {{{YassinAkbibSAT2023}}},
  author = {Akbib, Yassin},
  date = {2023},
  institution = {{Institut Supérieur Industriel de Bruxelles}},
  location = {{SAT, Metalab}},
  url = {https://sat-mtl.gitlab.io/metalab/lab/posts/2022-interns/zotero-infovis-trends-plugin/}
}
