% This file was created with JabRef 2.8b.
% Encoding: ISO8859_1

@inproceedings{VibroTactileWidgetsUIST2017,
 author = {Christian Frisson and Julien Decaudin and Thomas Pietrzak and Alex Ng and Pauline Poncet and Fabrice Casset and Antoine Latour and Stephen Brewster},
 title = {Designing Vibrotactile Widgets with Printed Actuators and Sensors},
 booktitle = {30th Symposium on User Interface Software and Technology},
 series = {UIST'17 Adjunct},
 year = {2017},
 location = {Quebec, Canada},
 publisher = {ACM},
 numpages = {3},
 doi = {10.1145/3131785.3131800},
 keywords = {Vibrotactile feedback, printed electronics, printed actuators, printed sensors, widgets, piezoelectric ink, thin-film actuators},
 abstract = {Physical controls are fabricated through complicated assembly of parts requiring expensive machinery and are prone to mechanical wear. One solution is to embed controls directly in interactive surfaces, but the proprioceptive part of gestural interaction that makes physical controls discoverable and usable solely by hand gestures is lost and has to be compensated, by vibrotactile feedback for instance. Vibrotactile actuators face the same aforementioned issues as for physical controls. We propose printed vibrotactile actuators and sensors. They are printed on plastic sheets, with piezoelectric ink for actuation, and with silver ink for conductive elements, such as wires and capacitive sensors. These printed actuators and sensors make it possible to design vibrotactile widgets on curved surfaces, without complicated mechanical assembly.}
} 

@misc{MechaMediaDuinoFFM2016,
 author = {Christian Frisson},
 title = {Force-Feedback (Rotary) Audio Browsing},
 booktitle = {Force-Feedback \& Music Symposium},
 series = {FF\&M'16},
 year = {2016},
 location = {Montreal, Canada},
 numpages = {0},
 keywords = {Force-feedback user interfaces, haptics, tangible user interfaces, rotary, jog wheel, media browsing, audio browsing},
 abstract = {A subset of not so new interfaces for musical expression have been traditionally employed in an artistic and scientific field related to and generative of computer music: physical/tangible controls for media browsing. Cyclic representations of time might have been the motivation for the use of rotary control for temporal media (audio and video). Rotary controls have been widely used by experts in audio edition and video montage even before their systems were computerized, with passive proprioceptive and kinesthetic feedback (on hands) limited by the physical controls during their design and fabrication. Why are there no cost-effective commercial devices for force-feedback rotary control widely available now for digital systems, with user-definable mappings, besides the upcoming Microsoft Surface Dial? Can we just make one from off-the-shelf and repurposed components? This talk will start with a short overview of past personal projects on tangible-to-force-feedback media browsing. 
 
The core of the talk is to provide a log reporting hands-on attempts in replicating interaction techniques for force-feedback audio browsing from seminal papers, towards a "hello world" tutorial, using a recent low-cost opensource and openhardware servo motor project (MechaDuino) and a fork of a visual programming environment dedicated for audio/control dataflow (PurrData out of PureData) that had already been used for prototyping force-feedback and music applications.},
 url = {http://www.cirmmt.org/activities/workshops/research/ffedback_music},
 slides = {https://github.com/ChristianFrisson/MechaMediaDuino}
} 

@inproceedings{SonixCycleAudioMostly2016,
 author = {Gabriel Urbain and Christian Frisson and Alexis Moinet and Thierry Dutoit},
 title = {A Semantic and Content-Based Search User Interface for Browsing Large Collections of Foley Sounds},
 booktitle = {11th AudioMostly Conference on Interaction with Sound},
 series = {AM'16},
 year = {2016},
 location = {New York, USA},
 publisher = {ACM},
 numpages = {6},
 doi = {10.1145/2986416.2986436},
 keywords = {Search user interfaces, multimedia information retrieval, information visualization},
 abstract = {Sound designers select the sounds they use among massive collections of recordings. They usually rely on text-based queries to narrow down a subset from these collections when looking for specific content. However, when it comes to unknown collections, this approach can fail to precisely retrieve files according to their content.
We investigate an audio search engine that associates content-based features and semantic meta-data using Apache Solr deployed in a fully integrated server architecture. In order to facilitate the task of browsing the sounds, we also propose a search user interface in which the user can perform both text-based queries and visual browsing in a window where sounds are organized according to their audio features. A preliminary evaluation of the performances helped to optimize the parameters of the system.}
} 

@inproceedings{ComixTripDIS2016LBW,
 author = {Alexis Rochette and Cédric Goossens and Rudi Giot and Christian Frisson},
 title = {ComixTrip: Reading Comic Books with Text Sequenced through Gaze Tracking},
 booktitle = {Conference on Designing Interactive Systems},
 series = {DIS'16 PWiP},
 year = {2016},
 location = {Brisbane, Australia},
 numpages = {6},
 publisher = {ACM},
 address = {New York, NY, USA},
 doi = {10.1145/2908805.2909405},
 keywords = {Comic book, Eye tracking, Reading time},
 abstract = {Paper-based comic books are rendered on a static medium, where time can alternatively be sequenced through space. People usually prefer to read comic books at their pace. When comic books are digital, their medium becomes dynamic and interactive, how can the readers' experience be redesigned? We present ComixTrip, a system with which people can read digital comic books with sequential media cues responsively adapted to their reading pace. Our system relies on low-cost eye tracking both for measuring how people read; and for sequencing speech balloons semi-automatically: once a balloon is read, the next one is displayed. We ran a study to analyse how people read text paragraphs with diverse spatial layouts by tracking their gaze. Our preliminary results show that we may accurately track when people have read balloons in comic books. Our system needs to be improved regarding inter- and intra-person reading speed variations.}
} 

@inproceedings{InspectorWidgetCHI2016LBW,
 author = {Christian Frisson and Sylvain Malacria and Gilles Bailly and Thierry Dutoit},
 title = {InspectorWidget: a System to Analyze Users Behaviors in Their Applications},
 booktitle = {33rd Conference Extended Abstracts on Human Factors in Computing Systems},
 series = {CHI'16 EA},
 year = {2016},
 location = {San Jose, California, USA},
 numpages = {6},
 publisher = {ACM},
 address = {New York, NY, USA},
 doi = {10.1145/2851581.2892388},
 keywords = {Logging, automatic annotation, computer vision},
 abstract = {We propose InspectorWidget, an opensource application to track and analyze users' behaviors in interactive software. The key contributions of our application are: 1) it works with closed applications that do not provide source code nor scripting capabilities; 2) it covers the whole pipeline of software analysis from logging input events to visual statistics through browsing and programmable annotation; 3) it allows post-recording logging; and 4) it does not require programming skills. To achieve this, InspectorWidget combines low-level event logging (e.g. mouse and keyboard events) and high-level screen features (e.g. interface widgets) captured though computer vision techniques. InspectorWidget benefits end users, usability experts and HCI researchers.},
 code = {http://github.com/InspectorWidget}
} 

@inproceedings{StereoHapticsSIGGRAPH2016,
 author = {Israr, Ali and Zhao, Siyan and McIntosh, Kyna and Schwemler, Zachary and Fritz, Adam and Mars, John and Bedford, Job and Frisson, Christian and Huerta, Ivan and Kosek, Maggie and Koniaris, Babis and Mitchell, Kenny},
 title = {Stereohaptics: A Haptic Interaction Toolkit for Tangible Virtual Experiences},
 booktitle = {ACM SIGGRAPH 2016 Studio},
 series = {SIGGRAPH'16},
 year = {2016},
 location = {Anaheim, California},
 pages = {13:1--13:57},
 doi = {10.1145/2929484.2970273},
 publisher = {ACM},
 keywords = {Haptics, toolkit, vibrotactile, audio signal processing, PureData},
 abstract = {With a recent rise in the availability of affordable head mounted gear sets, various sensory stimulations (e.g., visual, auditory and haptics) are integrated to provide seamlessly embodied virtual experience in areas such as education, entertainment, therapy and social interactions. Currently, there is an abundance of available toolkits and application programming interfaces (APIs) for generating the visual and audio content. However, such richness in hardware technologies and software tools is missing in designing haptic experiences. Current solutions to integrate haptic effects are limited due to: i) a user's rigid adaptation to new hardware and software technologies, ii) limited scalability of the existing tools to incorporate haptic hardware and applications, iii) inflexible authoring capabilities, iv) missing infrastructure for storing, playback and distribution, and v) and unreliable hardware for long term usage.
We propose “Stereohaptics”, a framework to create, record, modify, and playback rich and dynamic haptic media using audio based tools. These tools are well established, mainstream and familiar to a large population in entertainment, design, academic, and the DIY communities, and already available for sound synthesis, recording, and playback. We tune these audio-based tools to create haptic media on user's bodies, distribute it to multiple slave units, and share it over the Internet. Applicable to the framework, we introduce a toolkit, the Stereohaptics toolkit, which uses off-the-shelf speaker technologies (electromagnetics, piezoelectric, electrostatic) and audio software tools to generate and embed haptic media in a variety of multisensory settings. This way, designers, artists, students and other professionals who are already familiar with sound production processes can utilize their skills to contribute towards designing haptics experiences. Moreover, using the audio infrastructure, application designers and software developers can create new applications and distribute haptic content to everyday users on mobile devices, computers, toys, game controllers and so on.}
} 

@inproceedings{WebAudioHapticsWAC2016,
 author = {Christian Frisson and Thomas Pietrzak and Siyan Zhao and Ali Israr},
 title = {WebAudioHaptics: Tutorial on Haptics with Web Audio},
 booktitle = {2nd Web Audio Conference},
 series = {WAC'16},
 year = {2016},
 location = {Atlanta, Georgia, USA},
 numpages = {1},
 keywords = {Haptics, toolkit, vibrotactile, audio signal processing, Web Audio, PureData},
 abstract = {The Web Audio Haptics WAC Tutorial 2016 will explore how to create meaningful haptic content that engages different areas of the body using off-the-shelf hardware and open source software running on a web browser using Web Audio technologies. Participants will 1) learn the basic theories of tactile illusions; 2) get an overview on actuators and sensors; 3) explore tactile illusions using web-based audio tools and a box connecting actuators and sensors to their computer audio I/O; and 4) ideate use cases in groups. Tutorial material will remain available from: \url{https://github.com/WebAudioHaptics}},
 slides = {https://WebAudioHaptics.github.io},
 code = {https://github.com/WebAudioHaptics}
}

@inproceedings{InfoPhysTEI2016,
 author = {Frisson, Christian and Dumas, Bruno},
 title = {InfoPhys: Direct Manipulation of Information Visualisation through a Force-Feedback
Pointing Device},
 booktitle = {10th Conference on Tangible, Embedded \& Embodied Interaction},
 series = {TEI'16},
 year = {2016},
 location = {Eindhoven, Netherlands},
 numpages = {6},
 doi = {10.1145/2839462.2856545},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Tangible user interfaces, force-feedback user interfaces, physical visualisation, information visualization, direct manipulation, haptics},
 abstract = {Information visualisation is the transformation of abstract data into visual, interactive representations. In this paper we present InfoPhys, a device that enables the direct, tangible manipulation of visualisations. InfoPhys makes use of a force-feedback pointing device to simulate haptic feedback while the user explores visualisations projected on top of the device. We present a use case illustrating the trends in ten years of TEI proceedings and how InfoPhys allows users to feel and manipulate these trends. The technical and software aspects of our prototype are presented, and promising improvements and future work opened by InfoPhys are then discussed.},
 code = {https://github.com/ChristianFrisson/InfoPhys}
} 

@phdthesis{TangibleMediaBrowsingPhD2015,
  TITLE = {Designing interaction for browsing media collections (by similarity)},
  author = {Frisson, Christian},
  URL = {https://tel.archives-ouvertes.fr/tel-01570858},
  SCHOOL = {{Universit{\'e} de Mons, Belgique}},
  YEAR = {2015},
  MONTH = Feb,
  KEYWORDS = {Human computer interaction HCI, Information visualisation, Information visualization, Multimedia information retrieval, Music information retrieval, Music information retrieval MIR, Tangible user interface, Tangible user interface TUI, Tangible user interfaces TUIs, Force-feedback, Haptic UIs, Similarity, Browsing, Search user interfaces, Glyph Design, Haptics},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-01570858/file/ChristianFrisson-PhD.pdf},
  HAL_ID = {tel-01570858},
  HAL_VERSION = {v1},
  abstract = {Sound designers source sounds in massive and heavily tagged collections. When searching for media content, once queries are filtered by keywords, hundreds of items are left to be reviewed. How can we present these results efficiently?

This doctoral work aims at improving the usability of browsers of media collections by blending techniques from multimedia information retrieval (MIR) and human-computer interaction (HCI).
We produced an in-depth state-of-the-art on media browsers. 
We overviewed HCI and MIR techniques that support our work: organization by content-based similarity (MIR), information visualization and gestural interaction (HCI). 
We developed the MediaCycle framework for organization by content-based similarity and the DeviceCycle toolbox for rapid prototyping of gestural interaction, both facilitated the design of several media browsers. 
We evaluated the usability of some of our media browsers.

Our main contribution is AudioMetro, an interactive visualization of sound collections. Sounds are represented by content-based glyphs, mapping perceptual sharpness (audio) to brightness and contour (visual). These glyphs are positioned in a starfield display using Student t-distributed Stochastic Neighbor Embedding (tSNE) for dimension reduction, then a proximity grid optimized for preserving direct neighbors. Known-item search evaluation shows that our technique significantly outperforms a grid of sounds represented by dots and ordered by filename.}
}

@InProceedings{AudioMetroAM2014,
  title = {AudioMetro: directing search for sound designers through content-based cues},
  author = {Christian Frisson and Stéphane Dupont and Willy Yvart and Nicolas Riche and Xavier Siebert and Thierry Dutoit},
  booktitle = {9th AudioMostly Conference on Interaction with Sound},
  year = {2014},
  series = {AM'14},
  location = {Aalborg, Denmark},
  month = {October 1-3},
  publisher = {ACM},
  doi = {10.1145/2636879.2636880},
  keywords = {Media browsers, sound effects, visual variables, music information retrieval, content-based similarity, known-item search, dimension reduction, tSNE, Student-t Stochastic Neighbor Embedding, proximity grids},
  abstract = {Sound designers source sounds in massive collections, heavily tagged by themselves and sound librarians. For each query, once successive keywords attained a limit to filter down the results, hundreds of sounds are left to be reviewed. AudioMetro combines a new content-based information visualization technique with instant audio feedback to facilitate this part of their workflow. We show through user evaluations by known-item search in collections of textural sounds that a default grid layout ordered by filename unexpectedly outperforms content-based similarity layouts resulting from a recent dimension reduction technique (Student-t Stochastic Neighbor Embedding), even when complemented with content-based glyphs that emphasize local neighborhoods and cue perceptual features. We propose a solution borrowed from image browsing: a proximity grid, whose density we optimize for nearest neighborhood preservation among the closest cells. Not only does it remove overlap but we show through a subsequent user evaluation that it also helps to direct the search. We based our experiments on an open dataset (the OLPC sound library) for replicability.}
}

@InProceedings{AudioMetroISMIR2014,
  title = {A proximity grid optimization method to improve audio search for sound design},
  author = {Christian Frisson and Stéphane Dupont and Willy Yvart and Nicolas Riche and Xavier Siebert and Thierry Dutoit},
  booktitle = {15th International Symposium on Music Information Retrieval},
  series = {ISMIR'14},
  year = {2014},
  location = {Taipei, Taiwan},
  month = {October 27-31},
  keywords = {Media browsers, sound effects, visual variables, music information retrieval, content-based similarity, known-item search, dimension reduction, tSNE, Student-t Stochastic Neighbor Embedding},
  abstract = {Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest
neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques.}
}

@INPROCEEDINGS{TangibleHaystackTEI2014,
 author = {Frisson, Christian and Rocca, François and Dupont, St{\'e}phane and Dutoit, Thierry and Grobet, Damien and Giot, Rudi and El Brouzi, Mohammed and Bouaziz, Samir and Yvart, Willy and Merviel, Sylvie},
 title = {Tangible Needle, Digital Haystack: Tangible Interfaces for Reusing Media Content Organized by Similarity},
 booktitle = {8th Conference on Tangible, Embedded \& Embodied Interaction},
 series = {TEI'14},
 year = {2014},
 location = {Munich, Germany},
 pages = {37--38},
 numpages = {2},
 doi = {10.1145/2540930.2540983},
 acmid = {2540983},
 publisher = {ACM},
 keywords = {Tangible user interfaces, content-based similarity, contextual inquiry, known-item search, force-feedback user interfaces},
 abstract = {This paper presents the design process of a desk-set tangible user interface for the navigation and manipulation of media content organized by content-based similarity with off-the-shelf/flea market devices. For intra-media navigation, a refurbished portable vinyl player has its inside mechanics replaced by a webcam monitoring circular gray code analyzed through computer vision for position/speed tracking. For inter-media navigation, a repurposed 3D force-feedback controller is mounted in upright position on a truss with cell clamps. For media recomposition, motorized faders recall the effect presets of the closest/last selected media item.}
} 


@INPROCEEDINGS{BaltazarsMMM2014Mediadrom,
  author = {Aurélie Baltazar and Pascal Baltazar and Christian Frisson},
  title = {An interactive device for exploring thematically sorted art-works},
  booktitle = {20th International Conference on MultiMedia Modeling, Mediadrom special session},
  series = {MMM'14}, 
  year = {2014},
  address = {Dublin, Ireland},
  month = {January 7-10},
  publisher = {Springer},
  doi = {10.1007/978-3-319-04117-9_4},
  keywords = {hypermedia browser, multimedia annotation, interactive installation, plastic theater},
  abstract = {This Mediadrom artful post-TV scenario consists in sketching the user interface of an interactive media content browsing system for exploring thematically sorted artworks, from the art field of plastic theater, merging art pieces at the intersection of the visual and the performing arts. Combining a touchscreen and an hypermedia browser of image and video content with expert annotations, this system can be installed in venues such as museum and media libraries, and performance spaces as satellite installation to plastic theater performances.}
}

@INPROCEEDINGS{MetropolitanViewsMMM2014Mediadrom,
  author = {Bénédicte Jacobs and Laure-Anne Jacobs and Christian Frisson and Willy Yvart and Thierry Dutoit and Sylvie Leleu-Merviel},
  title = {Scenarizing Metropolitan Views: FlanoGraphing the urban spaces},
  booktitle = {20th International Conference on MultiMedia Modeling, Mediadrom special session},
  series = {MMM'14}, 
  year = {2014},
  address = {Dublin, Ireland},
  month = {January 7-10},
  publisher = {Springer},
  doi = {10.1007/978-3-319-04117-9_2},
  keywords = {FlanoGraph, GPS, sensing technologies, database management, information retrieval, abstracting technologies, summarizing technologies, navigation, user interface design, gestural interaction, data visualization, timeline},
  abstract = {The recent decade has seen a rapid evolution in the field of digital media. Mobile devices are now being integrated into every aspect of urban life. GPS, sensor technologies and augmented reality have transformed the new generation of mobile devices from a communication and information platform into a navigational tool, fostering new ways of perceiving reality and image building. Touch sensor technology has changed the screen into a joint input and display device. In this paper we present the FlanoGraph, an application for smartphones and tablets designed to take benefit of the changes induced by mobile devices. We first briefly outline the conceptual background, evoking the work of some researchers in the fields of ‘Non Representational Theory', mobile media, and computational data processing. We then present and describe the FlanoGraph through a set of use cases. Finally, we conclude discussing some techniques necessary for the development of the application.}
}

@INPROCEEDINGS{CADastreExquisseMMM2014Mediadrom,
  author = {Cédric Sabato and Aurélien Giraudet and Virginie Delattre and Yves Desnos and Christian Frisson and Rudi Giot and Willy Yvart and François Rocca and Stéphane Dupont and Guy Vandem Bemden and Sylvie Leleu-Merviel and Thierry Dutoit},
  title = {Scenarizing CADastre Exquisse: a crossover between snoezeling in hospitals/domes, and authoring/experiencing soundful comic strips},
  booktitle = {20th International Conference on MultiMedia Modeling, Mediadrom special session},
  series = {MMM'14}, 
  year = {2014},
  address = {Dublin, Ireland},
  month = {January 7-10},
  publisher = {Springer},
  doi = {10.1007/978-3-319-04117-9_3},
  keywords = {virtual environments, collaborative media authoring, interactive comic strips, snoezelen},
  abstract = {This paper aims at providing scenarios for the design of authoring and experiencing environments for interactive soundful comic strips. One setting would be a virtual immersive environment made of a dome including spherical projection, surround sound, where visitors comfortably lying down on an interactive mattress can appreciate exquisite corpses floating on the ceiling of the dome, animated, with sound, dependent of the overall behavior of the visitors. On tabletops, creators can generate comic-strip-like creatures by collage or sketching, and associate audiovisual behaviors and soundscapes to these. This creation system will be used in hospitals towards a living lab comforting patients in accepting their health trip. Both settings are inspired by snoezelen methods. These crossover scenarios associate a project by L'Art-Ch ́etype retained to be featured for Mons 2015 EU Capital of Culture and other partners aiming at designing an environment for experiencing/authoring interactive comic-strips augmented with sound.}
}

@article{VideoBrowserShowdownIJMIR2014,
	author = {Schoeffmann, Klaus and Ahlstr\"{o}m, David and Bailer, Werner and Cob\^{a}rzan, Claudiu and Hopfgartner, Frank and McGuinness, Kevin and Gurrin, Cathal and Frisson, Christian and Le, Duy-Dinh and Fabro, Manfred and Bai, Hongliang and Weiss, Wolfgang},
	doi = {10.1007/s13735-013-0050-8},
	journal = {International Journal of Multimedia Information Retrieval},
	keywords = {Video browsing; Video search; Video retrieval; Exploratory search},
	number = {2},
	pages = {1-15},
	publisher = {Springer},
	series = {IJMIR'14 3:2},
	title = {The Video Browser Showdown: a live evaluation of interactive video search tools},
	volume = {3},
	Year = {2014},
    keywords = {Video browsing, Video search, Video retrieval, Exploratory search},
    abstract = {The Video Browser Showdown evaluates the performance of exploratory video search tools on a common data set in a common environment and in presence of the audience. The main goal of this competition is to enable researchers in the field of interactive video search to directly compare their tools at work. In this paper, we present results from the second Video Browser Showdown (VBS2013) and describe and evaluate the tools of all participating teams in detail. The evaluation results give insights on how exploratory video search tools are used and how they perform in direct comparison. Moreover, we compare the achieved performance to results from another user study where 16 participants employed a standard video player to complete the same tasks as performed in VBS2013. This comparison shows that the sophisticated tools enable better performance in general, but for some tasks common video players provide similar performance and could even outperform the expert tools. Our results highlight the need for further improvement of professional tools for interactive search in videos.}
}

@inproceedings{LavaAMPUIST2013SIC,
 author = {Christian Frisson and Eric Schayes},
 title = {LavaAMP: surrounding the beats in music tracks by streaming colorful blobs},
 booktitle = {26th Symposium on User Interface Software and Technology, Student Innovation Contest},
 series = {UIST'13 \href{https://uist.acm.org/uist2013/contest.php}{SIC}},
 year = {2013},
 location = {St. Andrews, Scotland, UK},
 month = {October 8–11},
 publisher = {ACM},
 keywords = {Media browser, liquid display, music information retrieval, beat tracking, Microsoft Pumpspark, fountain development kit, PureData},
 abstract = {LavaAMP surrounds the beats in music tracks by streaming colorful blobs. LavaAMP is inspired from vacuum tube amps and lava lamps. Each onset detected in music tracks would trigger an pump impulse in one bottle with cyclic permuting order, creating a flow of colored blobs.}
} 

@inproceedings{MashtaCycleINTETAIN2013,
  title = {{M}ashta{C}ycle: on-stage improvised audio collage by content-based similarity and gesture recognition},
  author = {Frisson, Christian and Keyaerts, Gauthier and Grisard, Fabien and Stéphane Dupont and Ravet, Thierry and François Zajéga and Laura Colmenares~Guerra and Todoroff, Todor and Dutoit, Thierry},
  booktitle = {Proceedings of the 5th International Conference on Intelligent Technologies for Interactive Entertainment},
  series = {INTETAIN'13},
  year = {2013},
  location = {Mons, Belgium},
  month = {July 3-5},
  doi = {10.1007/978-3-319-03892-6_14},
  keywords = {Human-music interaction, audio collage, content-based similarity, gesture recognition, depth cameras, digital audio effects},
  abstract = {In this paper we present the outline of a performance in-progress. It brings together the skilled musical practices from Belgian audio collagist Gauthier Keyaerts aka Very Mash'ta; and the realtime, content-based audio browsing capabilities of the AudioCycle and LoopJam applications developed by the remaining authors. The tool derived from AudioCycle named MashtaCycle aids the preparation of collections of stem audio loops before performances by extracting content-based features (for instance timbre) used for the positioning of these sounds on a 2D visual map. The tool becomes an embodied on-stage instrument, based on a user interface which uses a depth-sensing camera, and augmented with the public projection of the 2D map. The camera tracks the position of the artist within the sensing area to trigger sounds similarly to the LoopJam installation. It also senses gestures from the performer interpreted with the Full Body Interaction (FUBI) framework, allowing to apply sound effects based on bodily movements. MashtaCycle blurs the boundary between performance and preparation, navigation and improvisation, installations and concerts.}
}

@INPROCEEDINGS{AudioCycleICME2013,
  author = {Stéphane Dupont and Thierry Ravet and Cécile Picard-Limpens and Christian Frisson},
  title = {Nonlinear dimensionality reduction approaches applied to music and
	textural sounds},
  booktitle = {14th International Conference on Multimedia and Expo},
  series = {ICME'13},
  year = {2013},
  address = {San Jose, USA},
  month = {July 15-19},
  doi = {10.1109/ICME.2013.6607550},
  publisher = {IEEE},
  keywords = {Dimensionality reduction, multimedia information retrieval, audio and music analysis},
  abstract = {Recently, various dimensionality reduction approaches have been proposed as alternatives to PCA or LDA. These improved approaches do not rely on a linearity assumption, and are hence capable of discovering more complex embeddings within different regions of the data sets. Despite their success on artificial datasets, it is not straightforward to predict which technique is the most appropriate for a given real dataset. In this paper, we empirically evaluate recent techniques on two real audio use cases: musical instrument loops used in music production and sound effects used in sound editing. ISOMAP and t-SNE are being compared to PCA in a visualization problem, where we end up with a two-dimensional view. Various evaluation measures are used: classification performance, as well as trustworthiness/continuity assessing the preservation of neighborhoods. Although PCA and ISOMAP can yield good continuity performance even locally (samples in the original space remain close-by in the low-dimensional one), they fail to preserve the structure of the data well enough to ensure that distinct subgroups remain separate in the visualization. We show that t-SNE presents the best performance, and can even be beneficial as a pre-processing stage for improving classification when the amount of labeled data is low.}

}

@inproceedings{VideoCycleMMMVBS2013,
  author    = {Christian Frisson and St{é}phane Dupont and Alexis Moinet and C{é}cile Picard{-}Limpens and Thierry Ravet and Xavier Siebert and Thierry Dutoit},
  title     = {VideoCycle: User-Friendly Navigation by Similarity in Video Databases},
  booktitle = {19th International Conference on MultiMedia Modeling, Video Browser Showdown special session},
  address = {Huangshan, China}, 
  month = {January 7-9},
  series = {MMM'13},
  publisher = {Springer},
  pages     = {550--553},
  year      = {2013},
  crossref  = {DBLP:conf/mmm/2013-2},
  doi = {10.1007/978-3-642-35728-2_66},
  keywords = {Media browsers, multimedia information retrieval, known-item search, timeline, jog wheel, clustering, feature extraction},
  abstract = {VideoCycle is a candidate application for this second Video Browser Showdown challenge. VideoCycle allows interactive intra-video and inter-shot navigation with dedicated gestural controllers. MediaCycle, the framework it is built upon, provides media organization by similarity, with a modular architecture enabling most of its workflow to be performed by plugins: feature extraction, clustering, segmentation, summarization, intra-media and inter-segment visualization. MediaCycle focuses on user experience with user interfaces that can be tailored to specific use cases.}
}

@INPROCEEDINGS{MediaCyclingTEI2013,
  author = {Christian Frisson},
  title = {Designing tangible/free-form apps for navigating in audio/visual collections (by content-based similarity)},
  booktitle = {7th Conference on Tangible, Embedded \& Embodied Interaction},
  series = {TEI'13},
  year = {2013},
  address = {Barcelona, Spain},
  month = {February 10-13},
  publisher = {ACM}, 
  doi = {10.1145/2460625.2460686},
  keywords = {Interface design, multimedia content organization, free-form interfaces, tangible user interfaces},
  abstract = {This paper focuses on one aspect of doctoral studies, within the last year of completion, consisting in designing applications for the navigation (by content-based similarity) in audio or video collections: the choice of tangible or free-form interfaces depending on use cases. One goal of this work is to determine which type of gestural interface suits best each chosen use case making use of navigation into media collections composed of audio or video elements, among: classifying sounds for electroacoustic music composition, derushing video, improvising instant music through an installation organizing and synchronizing audio loops. Prototype applications have been developed using the modular MediaCycle framework for organization of media content by similarity. We conclude preliminarily that tangible interfaces are better-suited for focused expert tasks and free-form interfaces for multiple-user exploratory tasks, while a combination of both can create emergent practices.} 
}


@INPROCEEDINGS{LoopJamNIME2012,
  author = {Christian Frisson and Stéphane Dupont and Julien Leroy and Alexis Moinet and Thierry Ravet and Xavier Siebert and Thierry Dutoit},
  title = {LoopJam: turning the dance floor into a collaborative instrumental map},
  booktitle = {12th Conference on New Interfaces for Musical Expression},
  series = {NIME'12},
  year = {2012},
  editor = {Essl, G. and Gillespie, B. and Gurevich, M. and O'Modhrain, S.},
  address = {Ann Arbor, Michigan},
  month = {May 21-23},
  keywords = {Interactive music systems and retrieval, user interaction and interfaces, audio similarity, depth sensors},
  abstract = {This paper presents the LoopJam installation which allows participants to interact with a sound map using a 3D computer vision tracking system. The sound map results from similarity-based clustering of sounds. The playback of these sounds is controlled by the positions or gestures of participants tracked with a Kinect depth-sensing camera. The beat-inclined bodily movements of participants in the installation are mapped to the tempo of played sounds, while the playback speed is synchronized by default among all sounds. We presented and tested an early version of the installation to three exhibitions in Belgium, Italy and France. The reactions among participants ranged between curiosity and amusement.}
}

@INPROCEEDINGS{LoopJamJIM2012,
  author = {Christian Frisson and Stéphane Dupont and Alexis Moinet and Julien Leroy and Thierry Ravet and Xavier Siebert and Thierry Dutoit},
  title = {LoopJam: une carte musicale collaborative sur la piste de danse},
  booktitle = {19èmes Journées d'Informatique Musicale},
  series = {JIM'12},
  year = {2012},
  pages = {101-105},
  address = {Mons, Belgique},
  month = {9-11 mai},
  keywords = {Interactive music systems and retrieval, user interaction and interfaces, audio similarity, depth sensors},
  abstract = {Ce papier présente l'installation LoopJam qui permet aux visiteurs d'interagir avec une carte musicale par le biais d'un système de suivi gestuel par vision informatique. La carte sonore résulte d'un partitionnement des sons en groupes par similarité basée sur leur signal. Le rendu sonore est contrôlé par les positions ou gestes des participants captés par une caméra Kinect détectant la profondeur de la scène 3D. Les mouvements des participants exprimant une mesure ou un tempo sont corrélés à la vitesse de lecture commune à tous les échantillons synchronisés par le moteur audio. Nous avons présenté et testé une première version de cette installation lors de trois expositions en Belgique, Italie et France. Les réactions parmi les participants ont varié entre la curiosité et l'amusement.}
}

@INPROCEEDINGS{MediaCycleISEA2011,
  author = {Christian Frisson and Stéphane Dupont and Xavier Siebert and Thierry Dutoit},
  title = {Similarity in media content: digital art perspectives},
  booktitle = {17th Symposium on Electronic Art},
  series = {\href{http://isea2011.sabanciuniv.edu}{ISEA'11}},
  year = {2011},
  address = {Istanbul, Turkey},
  month = {September 14-21},
  keywords = {Media browsers, content-based similarity, similarity, digital art, new media arts, media content},
  abstract = {This essay examines how media content navigation by similarity can foster new practices in digital arts, blurring the boundaries between composing/performing, curating/authoring, creating/interpreting. With MediaCycle, a framework for browsing media databases by similarity, we created several prototypes: a website for browsing dancers' identities through video recordings, a collaborative dancefloor for music creation.},
  url = {https://isea2011.sabanciuniv.edu/paper/similarity-media-content-digital-art-perspectives}
}

@INPROCEEDINGS{AudioCycleAES2010,
  author = {Stéphane Dupont and Christian Frisson and Xavier Siebert and Damien Tardieu},
  title = {Browsing Sound and Music Libraries by Similarity},
  booktitle = {128th Audio Engineering Society Convention},
  series = {Conv.'10},
  year = {2010},
  address = {London, UK},
  month = {May 22-25},
  publisher = {AES},
  keywords = {Multimedia information retrieval, music information retrieval, content-based similarity, information visualization},
  abstract = {This paper presents a prototype tool for browsing through multimedia libraries using content-based multimedia information retrieval techniques. It is composed of several groups of components for multimedia analysis, data mining, interactive visualization, as well as connection with external hardware controllers. The musical application of this tool, uses descriptors of timbre, harmony, as well as rhythm and two different approaches for exploring/browsing content. First, a dynamic data mining allows the user to group sounds into clusters according to those different criteria, whose importance can be weighted interactively. In a second mode, sounds that are similar to a query are returned to the user, and can be used to further proceed with the search. The browsing steps are then stored and visualized as a tree. This approach also borrows from multi-criteria optimization concept to return a relevant list of similar sounds.}
}

@INPROCEEDINGS{DotConIHM2010,
  author = {Christian Frisson},
  title = {Conception centrée utilisateur de prototypes interactifs pour
	la gestion de contenu multimedia par similarité},
  booktitle = {22ème Conférence Francophone sur l'Interaction Homme-Machine},
  series = {IHM'10},
  year = {2010},
  address = {Luxembourg},
  month = {September 20-23},
  publisher = {ACM},
  url = {http://ihm2010.afihm.org/programme/rencontres-doctorales.html},
  keywords = {Prototypage rapide, visualisation d’information, interaction gestuelle, navigation hypermedia},
  abstract = {Cet article présente les travaux en cours d’une recherche doctorale visant à proposer une méthodologie de conception centrée utilisateur et de prototypage rapide afin de concevoir des applications interactives destinées à la navigation par similarité dans des bases de données multimedia, adaptées à des cas d’utilisation divers et profils d’utilisateurs variés. Les modalités d’interaction sont volontairement restreintes à la visualisation d’information et l’interaction manuelle. Une méthode de développement rapide, réutilisable et durable est proposée, exemplifiée par quelques prototypes à évaluer par des tests utilisateur.}
}

@INPROCEEDINGS{AudioGardenAM2010,
  author = {Cécile Picard and Christian Frisson and Damien Tardieu and Benoit Macq and Jean Vanderdonckt and Thierry Dutoit},
  title = {Towards User-Friendly Audio Creation},
  booktitle = {5th AudioMostly Conference on Interaction with Sound},
  year = {2010},
  series = {AM'10},
  address = {Pite\r{a}, Sweden},
  month = {September 15-18},
  publisher = {ACM},
  doi = {10.1145/1859799.1859820},
  keywords = {Interactive Sound Composing, Audio Analysis and Synthesis, Content-based Audio Similarity, Multi-fidelity Prototyping},
  abstract = {This paper presents a new approach to sound composition for soundtrack composers and sound designers. We propose a tool for usable sound manipulation and composition that targets sound variety and expressive rendering of the composition. We first automatically segment audio recordings into atomic grains which are displayed on our navigation tool according to signal properties. To perform the synthesis, the user selects one recording as model for rhythmic pattern and timbre evolution, and a set of audio grains. Our synthesis system then processes the chosen sound material to create new sound sequences based on onset detection on the recording model and similarity measurements between the model and the selected grains. With our method, we can create a large variety of sound events such as those encountered in virtual environments or other training simulations, but also sound sequences that can be integrated in a music composition. We present a usability-minded interface that allows to manipulate and tune sound sequences in an appropriate way for sound design.}
}

@INPROCEEDINGS{DeviceCycleNIME2010,
  author = {Christian Frisson and Stéphane Dupont and Xavier Siebert and Damien Tardieu and Thierry Dutoit and Benoit Macq},
  title = {Device{C}ycle: rapid and reusable prototyping of gestural interfaces,
	applied to audio browsing by similarity},
  booktitle = {10th Conference on New Interfaces for Musical Expression},
  series = {NIME'10},
  year = {2010},
  address = {Sydney, Australia},
  month = {June 15-18},
  isbn = {978-0-646-53482-4},
  keywords = {Human-computer interaction, gestural interfaces, rapid prototyping, browsing by similarity, audio database},
  abstract = {This paper presents the development of rapid and reusable gestural interface prototypes for navigation by similarity in an audio database and for sound manipulation, using the AudioCycle application. For this purpose, we propose and follow guidelines for rapid prototyping that we apply using the PureData visual programming environment. We have mainly developed three prototypes of manual control: one combining a 3D mouse and a jog wheel, a second featuring a force-feedback 3D mouse, and a third taking advantage of the multitouch trackpad. We discuss benefits and shortcomings we experienced while prototyping using this approach.}
}

@INPROCEEDINGS{MultimodalGuitarNIME2010,
  author = {Loïc Reboursière and Christian Frisson and Otso Lähdeoja and Mills III, John Anderson and Cécile Picard and Todor Todoroff},
  title = {Multimodal{G}uitar: a Toolbox for Augmented Guitar Performances},
  booktitle = {10th Conference on New Interfaces for Musical Expression},
  series = {NIME'10},
  year = {2010},
  address = {Sydney, Australia},
  month = {June 15-18},
  isbn = {978-0-646-53482-4},
  url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo%20O1-O20/P415_Reboursiere.pdf},
  keywords = {Augmented guitar, audio synthesis, digital audio effects, multimodal interaction, gestural sensing, polyphonic transcription, hexaphonic guitar},
  abstract = {This project aims at studying how recent interactive and interactions technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. Our contributions target three main axes: audio analysis, gestural control and audio synthesis. For this purpose, we designed and developed a freely-available toolbox for multimodal guitar performances, compliant with the PureData and Max/MSP modular environments for a more widespread use, gathering tools for: polyphonic pitch estimation, fretboard visualization and grouping, pressure sensing, modal synthesis, infinite sustain, rearranged looping and “smart” harmonizing.}
}
 
@article{ModalJASP2010,
	author = {Cécile Picard and Christian Frisson and François Faure and George Drettakis and Paul~G. Kry},
	doi = {10.1155/2010/392782},
	journal = {EURASIP Journal on Advances in Signal Processing},
	pages = {12},
	publisher = {Springer},
	series = {JASP'10 2010:392782},
	title = {Advances in Modal Analysis Using a Robust and Multiscale Method},
	volume = {2010},
	year = {2010},
    keywords = {Modal analysis, modal synthesis, audio synthesis, finite element modeling},
    abstract = {This paper presents a new approach to modal synthesis for rendering sounds of virtual objects. We propose a generic method that preserves sound variety across the surface of an object at different scales of resolution and for a variety of complex geometries. The technique performs automatic voxelization of a surface model and automatic tuning of the parameters of hexahedral finite elements, based on the distribution of material in each cell. The voxelization is performed using a sparse regular grid embedding of the object, which permits the construction of plausible lower resolution approximations of the modal model. We can compute the audible impulse response of a variety of objects. Our solution is robust and can handle nonmanifold geometries that include both volumetric and surface parts. We present a system which allows us to manipulate and tune sounding objects in an appropriate way for games, training simulations, and other interactive virtual environments.}
}

@INPROCEEDINGS{AudioCycleCBMI2009,
  author = {Stéphane Dupont and Thomas Dubuisson and Jér\^{o}me Urbain and Rapha\"{e}l Sebbe and Nicolas d'Alessandro and Christian Frisson},
  title = {Audio{C}ycle: Browsing Musical Loop Libraries},
  booktitle = {7th Workshop on Content-Based Multimedia Indexing},
  series = {CBMI'09},
  year = {2009},
  pages = {73-80},
  address = {Chania, Crete},
  month = {June 3-5},
  doi = {10.1109/CBMI.2009.19},
  publisher = {IEEE},
  keywords = {Media content, multimedia information retrieval, audio feature extraction},
  abstract = {This paper presents AudioCycle, a prototype application for browsing through music loop libraries. AudioCycle provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation, and listen to individual audio extracts, searching for those of interest. AudioCycle draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Concepts developped here will be of interest to DJs, remixers, musicians, soundtrack composers, but also sound designers and foley artists. Possible extension to multimedia libraries are also suggested.}
}

@INPROCEEDINGS{AudioCycleICME2009,
  author = {Jér\^{o}me Urbain and Thomas Dubuisson and Stéphane Dupont and Christian Frisson and Rapha\"{e}l Sebbe and Nicolas d'Alessandro},
  title = {Audio{C}ycle: A similarity-based visualization of musical libraries},
  booktitle = {10th Conference on Multimedia and Expo},
  series = {ICME'09},
  year = {2009},
  pages = {1847-1848},
  address = {New York City, USA},
  month = {June 28 - July 3},
  doi = {10.1109/ICME.2009.5202887},
  publisher = {IEEE},
  keywords = {Browsing, music similarities, visualization, 3D rendering},
  abstract = {This paper presents AudioCycle, a prototype application for browsing through music loop libraries. AudioCycle provides the user with a graphical view where the audio extracts are visualized and organized according to their similarity in terms of musical properties, such as timbre, harmony, and rhythm. The user is able to navigate in this visual representation and listen to individual audio extracts. AudioCycle draws from a range of technologies, including audio analysis from music information retrieval research, 3D visualization, spatial auditory rendering, audio time-scaling and pitch modification. The proposed approach extends on previously described music and audio browsers. Possible extension to multimedia libraries are also suggested.}
}

@INPROCEEDINGS{CoMediAnnotateeNTERFACE2010,
  author = {Christian Frisson and Sema Alaçam and Emirhan Co\c{s}kun and Dominik Ertl and Ceren Kayalar and Lionel Lawson and Florian Lingenfelser and Johannes Wagner},
  title = {CoMediAnnotate: towards more usable multimedia content annotation
	by adapting the user interface},
  booktitle = {6th Summer School on Multimodal Interfaces},
  series = {\href{http://www.enterface.net}{eNTERFACE}'10},
  year = {2010},
  address = {Amsterdam, Netherlands},
  month = {July 12 - August 6},
  keywords = {Multimodal annotation, rapid prototyping, information visualization, gestural interaction},
  abstract = {This project aims at improving the user experience regarding multimedia content annotation. We evaluated and compared current timeline-based annotation tools, so as to elicit user requirements. We address two issues: 1) adapting the user interface, by supporting more input modalities through a rapid prototyping tool and by offering alternative visualization techniques of temporal signals; and 2) covering more steps of the annotation workflow besides the task of annotation itself: notably recording multimodal signals.
We developed input devices components for the OpenInterface (OI) platform for rapid prototyping of multimodal interfaces: multitouch screen, jog wheels and pen-based solutions. We modified an annotation tool created with the Smart Sensor Integration (SSI) toolkit and componentized it in OI so as to bind its controls to different input devices. We produced mockups sketches towards a new design of an improved user interface for multimedia content annotation, and started developing a rough prototype using the Processing Development Environment.
Our solution allows to produce several prototypes by varying the interaction pipeline: changing input modalities and using either the initial GUI of the annotation tool, or the newly-designed one. We target usability testing to validate our solution and determine which input modalities combination best suits given use cases.}
}

@INPROCEEDINGS{MultimodalGuitareNTERFACE2009,
  author = {Christian Frisson and Loïc Reboursière and Wen-Yang Chu and Otso Lähdeoja and Mills III, John Anderson and Cécile Picard and Ao Shen and Todor Todoroff},
  title = {Multimodal Guitar: Performance Toolbox and Study Workbench},
  booktitle = {5th Summer School on Multimodal Interfaces},
  series = {\href{http://www.enterface.net}{eNTERFACE}'09},
  year = {2009},
  address = {Genova, Italy},
  month = {July 13 - August 8},
  keywords = {Audio- and polyphonic multi-pitch transcription, audio synthesis, digital audio effects, multimodal interaction and gestural sensing, finger tracking, particle filtering, multimodal fusion, guitar score following},
  abstract = {This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. We investigate two axes, 1) “A gestural/polyphonic sensing/processing toolbox to augment guitar performances”, and 2) “An interactive guitar score following environment for adaptive learning”. These approaches share quite similar technological challenges (sensing, analysis, processing, synthesis and interaction methods) and dissemination intentions (community-based, low-cost, open-source whenever possible), while leading to different applications (respectively artistic and educational), still targeted towards experienced players and beginners.}
}

@misc{MultimodalGuitarJEIGE2009,
  author = {Christian Frisson and Loïc Reboursière},
  title = {Objectifs du projet Multimodal Guitar},
  booktitle = {Journées d'étude Identités de la Guitare électrique},
  series = {\href{http://www.guitarelectrique.fr}{JEIGE}'09},
  year = {2009},
  address = {Maison des Sciences de l'Homme Paris-Nord, Saint Denis, France},
  month = {May 18-19},
  keywords = {Audio- and polyphonic multi-pitch transcription, audio synthesis, digital audio effects, multimodal interaction and gestural sensing, finger tracking, particle filtering, multimodal fusion, guitar score following},
  abstract = {This project aims at studying how recent interactive and interaction technologies would help extend how we play the guitar, thus defining the “multimodal guitar”. We investigate two axes, 1) “A gestural/polyphonic sensing/processing toolbox to augment guitar performances”, and 2) “An interactive guitar score following environment for adaptive learning”. These approaches share quite similar technological challenges (sensing, analysis, processing, synthesis and interaction methods) and dissemination intentions (community-based, low-cost, open-source whenever possible), while leading to different applications (respectively artistic and educational), still targeted towards experienced players and beginners.}
}

@INPROCEEDINGS{PhysioContenteNTERFACE2007,
  author = {Mitchel Benovoy and Andrew Brouse and Thomas Greg Corcoran and Hannah Drayson and Cumhur Erkut and Jean-Julien Filatriau and Christian Frisson and Umut Gundogdu and Ben Knapp and Rémy Lehembre and Christian Mühl and Miguel Angel Ortiz Pérez and Alaattin Sayin and Mohammad Soleymani and Koray Tahiro\u{g}lu},
  title = {{A}udiovisual {C}ontent {G}eneration {C}ontrolled by {P}hysiological
	{S}ignals for {C}linical and {A}rtistic {A}pplications},
  booktitle = {3rd Summer School on Multimodal Interfaces},
  series = {\href{http://www.enterface.net}{eNTERFACE}'07},
  year = {2007},
  pages = {103-116},
  address = {Istanbul, Turkey},
  month = {July 16 - August 10},
  url = {http://www.cmpe.boun.edu.tr/enterface07/outputs/final/p8report.pdf},
  keywords = {Multimodal interfaces, Biosignals, Brain-computer interfaces, Sonification, Auditory display, Interactive arts, Biologically-augmented performances},
  abstract = {While an extensive palette of sound and visual generation techniques have been developed during the era of digital signal processing, the design of innovative virtual instruments has come to dramatic fruition over the last decade. The use of measured biological signals to drive these instruments proposes some new and powerful tools for clinical, scientific and artistic applications. Over the period of one month - during the eNTERFACE’07 summer workshop in  ̇Istanbul, Turkey - researchers from the fields of human-computer interfaces, sound synthesis and new media art worked together towards this common goal.
A framework for auditory display and bio-musical applications was established upon which were based different experimental prototypes. Diverse methods for the analysis of measured physiological signals and of mapping the extracted parameters to sound and visual synthesis processes were explored. Biologically-driven musical instruments and data displays for clinical and medical purposes were built. From this have emerged some worthwhile perspectives on future research. This report summarises the results of that project.}
}

@misc{HelmholtzJFIS2005,
  author = {François Gautier and Jean-Loïc Le~Carrou and Erwan Collin and Jérémie Dufaud and Christian Frisson},
  title = {Helmoltz: un outil de caractérisation des deux premiers modes de la guitare},
  booktitle = {Journées Facture Instrumentale \& Sciences: ``Mettre en Commun''},
  series = {\href{http://www.itemm.fr/jfis/}{JFIS}'05},
  year = {2005},
  address = {Le Mans, France},
  month = {July},
  keywords = {Vibroacoustics, luthier, guitar, modal analysis},
  abstract = {Ce projet porte sur l’étude vibro-acoustique de la guitare. L’objectif est de fournir au luthier une chaîne de mesure simplifiée permettant d’extraire les paramètres d’un modèle à deux degrés de liberté de la guitare. Cet appui graphique lui permettra d’associer les caractéristiques physiques d’une guitare (qualité du bois, masse volumique des composants, raideur de la table d’harmonie) avec une courbe de réponse en fréquence.
Nous devons ainsi mettre à disposition au luthier un banc de mesure simple, rapide à l’utilisation et peu coûteux lui permettant de mesurer la réponse en fréquence de caisses de résonance et d'obtenir des paramètres objectifs.}
}
